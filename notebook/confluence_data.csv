Content,Metadata
"DevOps Künyesi Tüm işlerimizde her zaman göz önünde bulundurmamız gerekenler; İç müşterilerimizin memnuniyetini artırmalıyız . Yaptıklarımızla fark yaratmalıyız . Takım oyununu ve birlikteliği hep canlı tutmalıyız. Yaptığımız işlerdeki kaliteyi artırmalıyız . Sistematik çalışarak işlerimizi daha iyi yönetmeliyiz. Verimliliğimizi artırmalı ve kendi gelişimlerimizi de ihmal etmemeliyiz. The team Sinan Çayır About us Softtech DevOps Bilgi Paylaşım Ortamı Blog stream 3 excerpts Recently updated page, comment, blogpost, spacedesc 10 true concise","{'title': 'Softtech DevOps Araçları & Platform Yönetimi', 'id': '1409027', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=1409027'}"
"2c5d93d9-dfeb-46e0-ba4f-f7fcc4722201 com.atlassian.confluence.plugins.confluence-business-blueprints:file-list-blueprint Create file list 2c5d93d9-dfeb-46e0-ba4f-f7fcc4722201 com.atlassian.confluence.plugins.confluence-business-blueprints:file-list-blueprint file-list Upload, preview and share files with your team. File lists Create file list file-list","{'title': 'File lists', 'id': '3211318', 'source': 'https://wiki.softtech.com.tr/display/SDO/File+lists'}"
Automation-with-Ansible-do407-a20-en-1-20160804.pdfEffective_DevOps.pdf,"{'title': 'Softtech DevOps Files', 'id': '3211319', 'source': 'https://wiki.softtech.com.tr/display/SDO/Softtech+DevOps+Files'}"
Ip Adresi Sunucu Adı Domain 1 10.192.96.183 Genom İsbank 2 10.80.36.89 DevOps.Softtech Softtech 3 10.80.36.77 CheckMarx İsbank 4 10.80.36.82 CheckMarx İsbank 5 10.80.20.141 ScoreTFS İsbank 6 213.52.186.137 SuccessFactory DışNetWork 7 10.50.107.150 ClearQuest İsbank 8 10.80.36.92 SofttechTFS Softtech 9 10.80.36.102 Kozmos Softtech 10 10.192.99.89 ScoreNexus İsbank 11 10.80.36.96 Java Agent Softtech 12 10.80.36.97 DotNet Agent Softtech 13 10.80.36.134 Java Agent Softtech 14 10.80.36.135 DotNet Agent Softtech 15 10.222.8.72 NexusSofttech Softtech 16 10.80.36.49 Sonar-PROD Sofftech 17 10.222.8.84 GitLab Sofftech 18 10.222.8.85 ELK(Eski) Sofftech 19 10.222.8.86 Jenkins Sofftech 20 10.222.8.87 ELK Softtech 21 10.222.8.88 Nginx Load Balancer (Aktif) Softtech 22 10.222.8.89 Nginx Load Balancer (Backup) Softtech 23 10.222.8.90 Nginx Load Balancer Virtual IP Softtech 24 10.80.36.49 - - 25 10.80.36.39 SonarQube Test Sunucusu (Bir Önceki Yılın Bilgileri Saklanır) Softtech 26 10.80.36.118 10.80.36.39 Adresinde yer alan sunucunun DB'si. Softtech 27 10.80.36.47 SonarQube Prod DB Softtech 28 10.80.36.107 empty- for Yeni / Test TFS DB Softtech 29 10.80.36.108 empty- for Yeni / Test TFS Live Softtech 30 10.80.36.109 empty- for PythonScalaAgent S ofttech 31 10.80.36.125 empty- for uatdevops.softtech Softtech 32 10.80.36.10 Test Otomasyonu - Dosya Paylaşımı Softtech 33 10.80.36.12 empty Softtechimage2019-9-5_11-54-39.pngimage2019-9-5_11-54-32.pngimage2019-9-5_11-54-19.png,"{'title': 'Sunucular ve IpAdressleri', 'id': '3213477', 'source': 'https://wiki.softtech.com.tr/display/SDO/Sunucular+ve+IpAdressleri'}"
"Kaynak IP Hedef IP Port Açıklama 1 10.80.36.89 10.192.96.183 80,443 Genom 2 10.80.36.89 213.52.186.137 80,443 SuccessFactory 3 10.80.36.89 10.50.107.150 80,443 ClearQuest 4 10.80.36.89 10.80.36.92 80,443 SofttechTFS 5 10.80.36.89 10.80.36.102 80,443 Kozmos 6 10.80.36.89 10.80.36.77 80,443 CheckMarx 7 10.80.36.89 10.80.36.82 80,443 CheckMarx 8 10.80.36.89 10.80.36.49 80,443 Sonar 9 10.80.36.77 10.80.20.141 80,443 ScoreTFS 10 10.80.36.77 10.80.36.102 80,443 Kozmos 11 10.80.36.77 10.80.36.92 80,443 SofttechTFS 12 10.80.36.82 10.80.20.141 80,443 ScoreTFS 13 10.80.36.82 10.80.36.102 80,443 Kozmos 14 10.80.36.82 10.80.36.92 80,443 SofttechTFS 15 10.80.36.96 10.80.36.92 80,443 SofttechTFS 16 10.80.36.96 10.80.20.141 80,443 ScoreTFS 17 10.80.36.96 10.192.99.89 80,443 ScoreNexus 18 10.80.36.96 10.80.36.102 80,443 Kozmos 19 10.80.36.96 10.222.8.72 80,443,8081 NexusSofttech 20 10.80.36.97 10.80.36.92 80,443 SofttechTFS 21 10.80.36.97 10.80.20.141 80,443 ScoreTFS 22 10.80.36.97 10.192.99.89 80,443 ScoreNexus 23 10.80.36.97 10.80.36.102 80,443 Kozmos 24 10.80.36.97 10.222.8.72 80,443,8081 NexusSofttech 25 10.80.36.134 10.80.36.92 80,443 SofttechTFS 26 10.80.36.134 10.80.20.141 80,443 ScoreTFS 27 10.80.36.134 10.192.99.89 80,443 ScoreNexus 28 10.80.36.134 10.80.36.102 80,443 Kozmos 29 10.80.36.134 10.222.8.72 80,443,8081 NexusSofttech 30 10.80.36.135 10.80.36.92 80,443 SofttechTFS 31 10.80.36.135 10.80.20.141 80,443 ScoreTFS 32 10.80.36.135 10.192.99.89 80,443 ScoreNexus 33 10.80.36.135 10.80.36.102 80,443 Kozmos 34 10.80.36.135 10.222.8.72 80,443,8081 NexusSofttech 35 10.222.8.84 10.80.36.49 80,443 36 10.222.8.85 10.80.36.49 80,443 37 10.222.8.86 10.80.36.49 80,443image2019-9-5_12-2-31.png","{'title': 'Sunucu Network Tanımları', 'id': '3213482', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213482'}"
"SonarQube, Softtech bünyesinde geliştirilen bütün projelerin kod kalitelerini ölçmek üzere kullanılan bir araçtır. ( https://sonar.softtech / https://sonar.rally.softtech/ )image2019-9-9_8-5-43.png","{'title': 'SonarQube', 'id': '3213497', 'source': 'https://wiki.softtech.com.tr/display/SDO/SonarQube'}"
"SonarQube aracına CI sürecinin sonunda eklenen paketler varsayılan olarak ""Public"" izinli şekilde ekleniyor. Yeni eklenen paketlerin takibini; https://sonar.softtech/api/projects/search?visibility=public API üzerinden takip edebiliyoruz. Web üzerinden de yanıt dönebilen servisin detaylarını inceleyip bulunan ACI kodu ile bir ekibe adreslenmesi gerekiyorsa ilgili işlemin yapılması, karne kısıtı sebebiyle görünmemesi gerekiyorsa https://sonar.softtech/project_roles?id=ACIXXXXX adresine gidip ""Private"" şeklinde adreslenmesi gerekiyor. (API (Post) : http://sonar.softtech/api/projects/update_visibility?project=ACIXXXXX&visibility=private )","{'title': 'Check Public', 'id': '3213544', 'source': 'https://wiki.softtech.com.tr/display/SDO/Check+Public'}"
"SonarQube emekli işlemleri. Custom Measures olarak adlandırılan gruplama ve hesap etiketlerinde paketin ilgili metriği Pasif olarak işaretlenmeli. (İlgili adres: https://sonar.softtech/custom_measures?id=ACIXXXXX) Bu işlem ön yüzden yapılabileceği gibi aşağıdaki adımları izleyerek API'ler aracılığı ile de yapılabilir. Mevcut Custom Measures metriclerini silmek için → Post api/custom_measures/delete Paketin emekli olduğuna dair ilgili metriği eklemek için → Post api/custom_measures/create Emekli edilen paketin karışıklık oluşturmaması adına artık görünmesini istemeyiz. https://sonar.softtech/project_roles?id=ACIXXXXX adresinden pakete emekli olan paketler için önceden hazırlanmış olan sadece uygulama yöneticilerinin görmesini sağlayan ""İzin Temaları"" uygulanır. POST Api; api/permissions/apply_template Daha düzenli yönetmek adına paketlerin isimlerine ""ön ek"" eklenmek istenirse DB için aşağıdaki script'i çalıştırmak gerekecek. declare @scoreName nvarchar(2000); set @scoreName = 'RETIRED_PackageName'; UPDATE [SONAR].[dbo].[projects] SET [name] = @scoreName, [long_name] = @scoreName WHERE [kee] ='ACIXXXXX'image2019-9-16_11-40-36.pngimage2019-9-16_11-40-25.pngimage2019-9-16_11-32-42.png","{'title': 'Get Retired', 'id': '3213546', 'source': 'https://wiki.softtech.com.tr/display/SDO/Get+Retired'}"
"İçerisinde auto-generated kodları yer alan paketleri tamamiyle analizlerden çıkarılmamaktadır. Dili .java paketleri için pom.xml içerisine <module> tagı altında yer alan modullerin hemen yanına yazılım ekibi tarafından-.xml formatında comment olacak şekilde- <!--auto-->  yazılması gerekmektedir. Bu sayede analizlere dahil olmayacaktır. Gerekli değişiklikleri yaptıktan sonra bir sonraki analizde ilgili module analizlere dahil edilmemiş olacaktır. Not: Bu işlem sonrası hangi modüllerin kim tarafından çıkartıldığı DB'mize otomatik olarak kaydedilip, raporlanmaktadır. Dolayısıyla, bu işlemi yaparken doğru modülün belirtildiğine yazılım ekiplerince dikkat edilmesi gerekmektedir.","{'title': 'Auto Generated', 'id': '3213580', 'source': 'https://wiki.softtech.com.tr/display/SDO/Auto+Generated'}"
Yukarıdaki adımlar takip edilerek Portfolios sayfasına erişilebilir. Burada ana portfolyolar belirlenip alt kırılımları ile oluşturulabilir. ( Post API: http://sonar.softtech/api/views/create?name=DirektörlükAdık&key=DirektörlükiçinKeyimage2019-9-20_12-55-17.png,"{'title': 'Portfolios', 'id': '3213582', 'source': 'https://wiki.softtech.com.tr/display/SDO/Portfolios'}"
"Yukarıda belirtilmiş yol takip edilirse, projeler, o yıl içerisinde hangi şekilde gruplanacaksa onlar belirlenip, oluşturulabilir. Örnek aşağıdaki gibidir;image2019-9-20_13-9-1.pngimage2019-9-20_13-4-59.pngimage2019-9-20_13-4-8.png","{'title': 'Custom Metrics', 'id': '3213586', 'source': 'https://wiki.softtech.com.tr/display/SDO/Custom+Metrics'}"
,"{'title': 'DevOps Dokümantasyonu', 'id': '3213632', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213632'}"
,"{'title': 'DevOps Belgeleri', 'id': '3213634', 'source': 'https://wiki.softtech.com.tr/display/SDO/DevOps+Belgeleri'}"
"250 Softtech içindeki sürüm yönetimi geliştirme yapılan ortamlar, ekiplerin ve müşterinin ihtiyaçlarına göre belirlenmektedir. Bu kapsamda, Softtech Projelerinde Sürüm Yönetimi Banka Projelerinde Sürüm Yönetimi Softtech Yazılım Projelerinde Sürüm Yönetimi Softtech  Yazılım Projelerinde Versiyonlama Softtech Yazılım Projelerinde Majör.Minör.Patch şeklinde üçlü Semantic versiyonlama sistemi kullanılmaktadır. Majör versiyon artışı: Geliştirilen ürün ya da bileşen tamamen yeni bir güncelleme içeriyorsa Majör versiyon artışı yapılabilir. Majör versiyon artışı ilgili ürünü geliştiren ekipteki Ürün Mimarı, Yazılım Mimarı ya da yetkili Yazılımcılar tarafından kararlaştırılıp yapılabilir. Minör versiyon artışı: Geliştirilen ürün ya da bileşen yeni bir fonksiyon içeriyor ama geriye dönük olarak destekleyecek şekilde geliştirilmişse Minör versiyon artışı yapılır. Minör versiyon artışı ilgili ürünü geliştiren ekipteki Ürün Mimarı, Yazılım Mimarı ya da diğer yetkili Yazılımcılar tarafından kararlaştırılıp yapılabilir. Patch versiyon artışı: Geliştirilen ürün ya da bileşen üzerinde ürünün işleyişini değiştirmeyen geriye dönük olarak uyumlu yapılan hata yamalarına ilişkin versiyon artışlarıdır. Bu versiyon artışlarının her çıkan yama için yazılım ekibi tarafından ya da ortam üzerinden otomatik olarak yapılabilir. Yeni çıkan yazılımların sürümleri her zaman öncekilerden büyük olur. İlk yazılım versiyonları 0.y.z şeklinde olur. Yazılım yeterince olgun olduğunda 1.0.0 versiyounan yükseltilebilir. Snaphot versiyonları: Bazı durumlarda ürünü geliştiren ekipler belirli bir Majör.Minör.Patch versiyonu için deneme sürümü çıkabilirler. Bu durumda Majör.Minör.Patch-SNAPSHOT (1.0.1-SNAPSHOT) ibaresi eklenerek ilgili sürümün deneysel olduğunu ifade eder. alpha, beta, ga versiyonları (opsiyonel): Sürüm'e hazırlanan yazılımların ya da bileşenlerin semantic versiyonlarının sonuna alpha, beta, ga gibi ifadeler eklenerek ilgili sürümlerin üretim ortamlarında kuruluma hazırlık aşamalarında olduklarına ilişkin ifadelere yer verilebilir. Burada alpha ilk sürüm denemesi, beta daha stabil olan sürüm denemesi, ga (genaral availability) ise üretim ortamlarında kullanılmaya hazır stabil sürümü ifade eder. Yazılım versiyon yönetiminde tespit edilen bir hata sonucu, ya da iş gereksinimleri gereği yeni versiyonlar ilgili kurallara dikkat edilerek her zaman çıkılabilir. Softtech Yazılım Projelerinde DevOps İlkeleri Softtech Yazılım projelerinde oluşturulan DevOps Pipeline tanımları aşağıdaki ilkeler gözetilerek hazırlanır; Softtech Projelerinde Deployment Adımları Softtech'te geliştirilen yazılım projelerinde aşağıdaki DevOps adımları takip edilmektedir, Softtech DevOps hatlarında (banka hariçi) geliştirilen projelerde sürüm yönetimi bulunulan ortama ve yazılım ekiplerinin ihtiyaçlarına göre şekillenmektedir. Softtech DevOps ürünlerinde çıkılan sürümlerde oluşan değişiklik listesi versiyon numarası ile beraber iligli Jira projesine eklenmektedir. <todo>Jira'da Sürüm Bilgisi yönetimine ilişkin bilgiler yer alacak</todo> Banka Projelerinde Sürüm Yönetimi Banka projelerinde Kırmızı ve Mavi hatta bulunan paketlerin Sürüm bilgisi XL Deploy aracı üzerinden belirlenmektedir. Banka tarafındaki sürece ilişkin belgeye linkten * eriebilirsiniz. UAB, KGF Projelerinde Sürüm Yönetimi UAB ve KGF Projelerinde sürüm yönetimi Release Management belgesine * göre yapılmaktadır. Burada sırasısıyla, https://gitlab.rally.softtech/rally/rubix/reference-documentation/-/tree/master/RALLY-Framework/latest Branch Yönetimi Güncel Sürüm (Versiyon) Yönetimi Gelenksel Sürüm Yönetimi bölümlerinden oluşmaktadır. * İlgili belgelerin 20 Eylül 2019 tarihinde alınan sürümleri eklenmiştir. 250 250UAB KGF Release Management.pdf","{'title': 'Softtech Yazılım Versiyonlama ve Sürüm Yönetimi', 'id': '3213636', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213636'}"
,"{'title': 'Kubernetes and Container Infrastructure', 'id': '3213657', 'source': 'https://wiki.softtech.com.tr/display/SDO/Kubernetes+and+Container+Infrastructure'}"
"GlusterFS Nedir? Basit olarak SDS (Software Defined Storage) pozisyonunda yapılandırılır. En az 3 adet olmak koşuluyla (önerilen) N tane linux server bir araya getirilerek kurulan, yüksek erişilebilirlikli (High Available), hata toleranslı (Fault Tolerant) ve yüksek performanslı bir storage sistemidir. Genel anlamda kümeye dahil sunucuların fiziksel cihazlar olması önem arz eder fakat zorunlu değildir. Heketi Nedir? GlusterFS kümelerini Rest API üzerinden yönetebilmek için ortaya çıkmış (daha çok konteyner orkestrasyon sistemleri için) ve go programlama diliyle yazılmış bir arayüz sistemidir. Yüksek erişilebilirlikli olarak çalıştırmak şuan için mümkün değildir. Bu nedenle iyi yapılandırılmasında ve iyi monitör edilmesinde fayda vardır. (Keepalived uygulaması kullanılarak HA özellik sağlanabilir.) GlusterFS/Heketi Kurulumu GlusterFS kurulumu için buradan , Heketi kurulumu için buradan ayrıntılı şekilde bilgi alabilirsiniz. Biz Ne İçin Kullanacağız? Kullandığımız bir NFS sistemi olmadığı için, Origin ortamında persistent volume olarak kullanabileceğimiz bir sisteme ihtiyaç vardı. Fiziksel olarak bir Datacenter alanımız da olmadığı için bu yönteme gittik. Ayrıca GlusterFS uygulamasını heketi ile birlikte origin ortamına tam entegre edebildik. Hem dynamic provisioning hem de persistent volume storage olarak bu sistemi kullanacağız. GlusterFS Cluster Yapısı GlusterFS_1 Hostname: SCGFSS01.softtech.local IP: 10.222.8.97 GlusterFS_2 Hostname: SCGFSS02.softtech.local IP: 10.222.8.98 GlusterFS_3 Hostname: SCGFSS03.softtech.local IP: 10.222.8.99 Heketi Yapısı Heketi_1 Hostname: SCGFSS01.softtech.local IP: 10.222.8.97 Public Hostname: heketi,uat.softtech Public IP: 10.222.8.90 Heketi Home: /etc/heketi Heketi Config: /etc/heketi/heketi.json","{'title': 'GlusterFS Kurulumu (Heketi İle Birlikte)', 'id': '3213659', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213659'}"
"Origin Nedir? Red Hat firmasına ait olan Openshift konteyner orkestrasyon aracının open source port edilen versiyonudur. Kubernetes base bir sistemdir. Softtech İçin Altyapı Yapımız 2 master, 2 infra ve 5 compute sunucusundan oluşmaktadır. Rollerin sebebi aşağıda açıklanmıştır. Master: Bu rol sunucularda sadece cluster iletişimi için gerekli podlar bulunur. İşleri, sadece cluster kontrolünü sağlamaktır. Yapımızda etcd sunucuları da master sunucular üzerinde konumlandırılmıştır. Bu bakımdan masterların sağlıklı olması son derece önemlidir. Infra: Bu rol sunucularda cluster çalışılabilirliğinden sorumlu podlar çalışır. Cluster için kurulan namespacelerin hepsi, bu rol sunucuların üzerinde koşar. Compute: Bu rol sunucularda kullanıcı projeleri çalışır. Bu konu, cluster ve user yüklerinin birbirini etkilememesi açısından çok önemlidir. Web Konsol Erişimi Varsayılan kurulumda load balancer olarak HAProxy yapılandırılmaktadır. Bizim bir load balancer clusterımız olduğu için (Nginx), erişimi oradan vermeye karar verdik. Erişim için aşağıdaki bilgileri kullanabilirsiniz. Public Hostname: platform.uat.softtech:8443 Kurulum Aşamaları Openshift veya origin kurulumu için, openshift resmi reposunda bulunan openshift-ansible projesi kullanılmaktadır. Proje içerisinde bulunan ansible playbookları ile kurulum veya kaldırma işlemi kolaylıkla yapılabilmektedir. İşin en önemli kısmı, inventory dosyasını doğru hazırlamaktır. Bu iş için özel olarak kurduğumuz bir ansible sunucumuz da vardır. (10.222.8.96) Kurulumlar bu sunucu üzerinden yönetilmektedir. Ansible sunucusu, kontrol edeceği sunuculara şifresiz olarak (ssh key) ile bağlanmalıdır. (zorunlu değil, fakat verimli) Örnek inventory dosyası, ansible sunucusu üzerinde /root/openshift-ansible/inventory altında softtech.stg.cluster isimli dosyada yer almaktadır. Kurulan komponentlerin listesi aşağıdadır. Monitoring Stack Logging Stack Registry Stack Ansible Operator Stack Kurulumu başlatmak için aşağıdaki komutları sırasıyla başlatmak yeterli olacaktır. ansible-playbook -i <inventory_file> playbooks/prerequisites.yml ansible-playbook -i <inventory_file> playbooks/deploy_cluster.yml Entegrasyonlar İlerleyen aşamalrda aşağıdaki entegrasyonlar gerçekleştirilecektir. Log Entegrasyonu: Cluster içerisinde gömülü olarak kurduğumuz monitoring projesi, üzerinde bulunan dataları, mevcut ELK yapımıza gönderecek. Bununla birlikte merkezi bir log yönetim yapısı ortaya koymuş olacağız. Monitoring Entegrasyonu: Cluster içerisinde gömülü olarak kurduğumuz bir Prometheus stack bulunmakta. Bu projenin görevi, cluster içerisindeki core servisler ve kullanıcı servislerini (ek ayar ister) izlemektir. Bu yapıyı, mevcutta bulunan Zabbix yapımızla birleştirerek, merkezi bir monitoring yapısı ortaya koyacağız. Hibrit Repository: Yapımızda bulunan Nexus ile birlikte, oluşturulan konteyner imajlarını nexus üzerinde saklamak ve kolayca bu yapıya deploy etmek düşüncesindeyiz. Mevcutta oluşturulmuş bir konteyner registry vardı. Buraya doğrudan erişim için bir port (8083) verdim. Bu sayede imaj repomuz açılmış oldu. Node Etiketleme Genel anlamda tüm orkestrasyon araçlarında olan bir özellik, etiketlemedir. (labeling) Bu sayede ilgili proje, tam konumlandırılmak istenen yerde devreye alınabilir. Aşağıda örnek olarak verilen etiketleme, hibrit konteyner yönetimi için önemli bir adımdır. Bu sayede uygulamalarımız, dünyada istenen herhangi bir bölgede devreye girebilir. Etiketleme için örnekler, node ekleme kısmında anlatılacaktır. node_type web_server app_server db_server in_memory_cache_server disk_cache_server country tr fr de city istanbul frankfurt paris region east west provider self aws gcp Örnek label komutu Node Label: oc label node <node-name> node_type=""app_server"" country=""tr"" city=""istanbul"" region=""west-3"" provider=""self"" Master ve Infra Label: oc label node <node-name> country=""tr"" city=""istanbul"" provider=""self""","{'title': 'Red Hat Origin Kurulumu', 'id': '3213662', 'source': 'https://wiki.softtech.com.tr/display/SDO/Red+Hat+Origin+Kurulumu'}"
Yazılımcıların dikkat etmesi gereken konular.,"{'title': 'Dev', 'id': '3213706', 'source': 'https://wiki.softtech.com.tr/display/SDO/Dev'}"
"'ın notları aşağıdaki gibidir: Java Maven Projeleri için Dikkat edilecek Hususlar Birim test sonuçlarının ve code coverage'ların sonar'a doğru şekilde yansıması için kodların maven ile hatasız build olması ve tüm birim testlerin hatasız çalışması gerekmektedir. Bir birim test bile hata alırsa build başarısız sayılmakta ve analiz yapılamamaktadır. Graddle ile build yapılan projelerde ya da maven ile build edip jacoco ayarlarında değişikliği giden ekiplerin birimtest ve coverage analiz sonuçlarının nerede oluştuğunu ekibimize iletmesi gerekmektedir. Aksi takdirde analiz sonuçları yazdığımız scriptler tarafından bulunamayacak ve sıfır kabul edilecektir. Birim testler için kullanılan default değerler aşağıdaki gibidir. Bunların dışında bir path kullanıluyorsa bunun devops ekibine iletilmesi yazılım ekibinin sorumluluğundadır. Local ortamda build sonrasında aşağıda belirtilen adreslerde raporların oluştuğunun teyit edilmesi çok faydalı olur. Bir çok problem daha local ortamda çözülebilinir. -Dsonar.exclusions=**/*.xml -Dsonar.java.binaries=target -Dsonar.tests=src/test -Dsonar.binaries=target -Dsonar.java.coveragePlugin=jacoco -Dsonar.junit.reportPaths=target/surefire-reports -Dsonar.jacoco.reportPaths=target/jacoco.exec Yazılan birim testlerin property dosyaları dahil hiçbir dış sisteme gitmemesi gerekmektedir. Dış sisteme giden birimtestler build aşamasında hata alacağı için birim test ve RCI analizi yapılamayacaktır. Analiz sonuçları Sonar'a ( http://sonar.softtech/projects ) paket adı + ACI kodu şeklinde yansıtılmaktadır. Analizlerde hata ya da eksiklik gözlemleyen ekiplerin öncelikli olarak https://tfs.softtech/tfs adresine giderek build sonuçlarına bakması gerekiyor. Projenin sorunsuz build olduğu teyit edildikten sonra devops ekibi ile iletişime geçilmesi gerekmektedir. tfs.softtech java paketleri için: https://tfs.softtech/tfs/IsbankCollection/Java-Maven tfs.softtech .net paketleri için: credentials: ANT vb. legacy yöntemlerle build olan projelerin maven'a geçmesi gerekmektedir. Maven'a geçmeyen ve kapsam dışı kriterlerine uymayan paketlerin analizleri yapılamayacağız için sıfır olarak değerlendirileceklerdir. Birimtest sonuçları her modülün altındaki /target/surefire-reports dizininde oluşuyor. Projeyi derledikten sonra (mvn clean install) bu klasörün içinin dolu olduğunu teyit ediniz. Sonar sadece birim test coverage'ını hesaplıyor, Yazılan entegrasyon testleri coverage hesabına katılmıyor. Örnek olarak aşağıdaki iki test sınıfını ele alalım. Sonar ConverterTest sınıfının birim test sınıfı olduğunu anlıyor ve coverage sonuçlarına ekliyor. Ancak, PaymentPlanCalculatorApiImplIIT sınıfının bir birim test sınıfı olduğunu anlayamıyor. Bu sınıf içindeki testleri entegrasyon testi olarak yorumluyor. Bu nedenle birim test coverage sonuçlarına bu sınıf içindekiler yansımıyor. Bu durum, uygulama paketiniz altındaki test coverate'ının olduğundan düşük görünmesine neden olabilir. Test sınıf isimlerine dikkat ederek ve birim test yazma standartlarına uyarak bu durumuun oluşmasına engelleyebilirsiniz. Birim test dışında entegrasyon testide yazıyor olabilirsiniz, ancak bu yazdığınız testlerin birimtest kapsamında değerlendirilmeyeceğini hatırlatırız. Entegrasyon testlerinin sonar analizlerine yansıması için sonar'a entegrasyon testleride gösterilebilmektedir. Ancak bu durumda da birim testler kapsam dışında kalmaktadır. Bir olumsuzluk yaşamamak için bu konuya dikkat edilmesi gerekmektedir. Dsonar.jacoco.reportPaths=target/jacoco.exec -Dsonar.jacoco.reportPaths=target/jacoco-int.exec Mock kullanımı; Mocklama yaparken test ettiğiniz method içinden çağırılan diğer method'ları mocklayabilirsiniz. Ancak, test ettiğiniz methodun kendisini mock'larsanız, bu durumda test coverage'ına yansımayabilir. Bu durumun oluşmaması için mocklama yaparken test ettiğiniz method'un kendisini mocklamamaya özen gösterebilirsiniz. Yazdığınız birim testler, production kodları ile aynı paket hiyeraşisinde olmasına özen gösterin. Farklı tests ve production kodu farklı paketler altında olduğu durumlarda birim test coverage'ları sonar tarafından algılanmayabiliyor. Yukardaki örnekte, birim test farklı bir klasör altına yazılmış durumda. ConverterTest sınıfının'da convert isimli bir alt klasöre taşınması daha doğru olur. Production kodları ve birim testler aşağıdaki folder yapısında oluşturulmalıdır. Production kodları için src/main/java klasörleri altında paketler oluşturulmalıdır. Birim test kodları için src/test/java klasörleri altında paketler oluşturulmalıdır.","{'title': 'Yazılımcıların Dikkat Etmesi Gereken Hususlar', 'id': '3213709', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213709'}"
Operasyoncuların dikkat etmesi gereken konular.,"{'title': 'Ops', 'id': '3213711', 'source': 'https://wiki.softtech.com.tr/display/SDO/Ops'}"
,"{'title': 'devops.softtech', 'id': '3213734', 'source': 'https://wiki.softtech.com.tr/display/SDO/devops.softtech'}"
"Epik Senaryo Detaylar Öncelik Çeyrek Büyüklük Komponent Notlar Editörler 1 Checkmarx entegrasyonu bulguların kümile edilerek devops softtech üzerinde gösterilmesi .... Yüksek 2019/4 21 Backend, Frontend .... 2","{'title': 'backlog', 'id': '3213736', 'source': 'https://wiki.softtech.com.tr/display/SDO/backlog'}"
Tfs üzerinde üretilen Personal Access Tokenları en fazla 1 yıl süreyle üretilebiliyor. Bu süre dolduktan sonra agent offline durumuna geçtiği için sıraya yeni iş alamıyor. Bu sebepten dolayı tokenların yıllık olarak güncellenmesi gerekmektedir. Öncelikle token generate etmek için aşağıdaki adımlar uygulanır. https://tfs.softtech/tfs/IsbankCollection/_details/security/tokens linkine gidilir. Add butonuna tıkladıktan sonra gerekli alanlar doldurulur ve Create Token butonuna tıklanır. Üretilen token kopyalanır. (Token üretilen hesabın yetkili bir hesap olması önem taşımakta.) Agent'in dosya yolunda bulunan _work dosyası conflict olmaması için yedeklenip silinir. Command Prompt açılır ve Agentın dosya yoluna gidilir. config.cmd remove komutu ile agent servisi kaldırılır. .\config.cmd --sslskipcertvalidation --url {TFS URL} --auth PAT --token {Token} --pool {Pool}--runasservice --work _work --agent {Agent} komutu ile agent config yapılandırması başlatılır. Windows Service kullanıcısı için softtech\ytfsbuild gibi sunucu yöneticisi olan bir kullanıcı tercih edilmelidir. İşlem tamamlandıktan sonra agent çalışmaya hazır duruma gelir. Agent'ı çalıştırmak için .\run.cmd komutu çalıştırılır. Detaylı bilgi için: https://www.c-sharpcorner.com/article/how-to-install-configure-and-unconfigure-build-agent-in-tfs-2015/token2.PNGadd_token.PNG2.png1.png,"{'title': 'TFS Agentlarının Token Güncellemesi', 'id': '3213743', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213743'}"
https://tfs.softtech özelindeki belgeleri içerir.,"{'title': 'TFS Softtech', 'id': '3213761', 'source': 'https://wiki.softtech.com.tr/display/SDO/TFS+Softtech'}"
"Nerelerde Gerekli? Derlemeler esnasında alınan hatalardan fark edilebilecek durumlar. Sertifikaların süresinin dolması durumunda. Örneğin; TFS ajanlarından https://scorenexus.isbank adresine erişilmediği; derlemenin kayıtlarından farkedilir. Önce; güncel sertifika indirilir veya talep edilir. Güncel Sertifika Firefox üzerinden şu şekilde indirilir; İlgili adrese gidilir. Menü→ Seçenekler → Gelişmiş → Sertifikalar → Sertifikaları Göster → İçe Aktar Sonrasında Keytool kullanmak için aşağıdaki dizine gidilir; ve ilgili bütün sertifikalar aşağıdaki komutlar ile girilmelidir; Komut keytool -importcert -file {{path of cer}} -alias {{alias this cer}} -storepass changeit -keystore {{path of cacerts}} Örneğin; {{path of cert}} → C:/Certs/scorenexusisbank.cer {{named this cert}} → scorenexusisbank (isimlendirme scorenexus.isbank için -kolay anlaşılması adına böyle olmalı) {{path of cacerts}} → ""%JAVA_HOME%\jre\lib\security\cacerts"" (JRE içerisinde yer alan cacerts'in yolu verilmeli) Böylelikle sertifikalar güncellenmiş olacaktır.image2019-10-2_19-49-10.png","{'title': 'TFS Softtech için Sertifika Tanımlarının Güncellenmesi', 'id': '3213765', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213765'}"
TFS Softtech Build Agent'larının NuGet Restore aşamasında https://scorenexus.isbank adresine giderken yaşadığı ssl sorunun çözümü için agent'ın kurulu olduğu sunucuda Trusted Root Certification Authorities altında TURKIYE IS BANKASI A.S. Kok Sertifika Saglayicisi sertifikasının bulunması gerekmektedir. Aşağıda sertifika dosyası eklidir. 250 Sertifikayı yüklemek için üzerine çift tıklanıp açılan pencereden Install Certificate... seçilmelidir. Açılan Import Wizard aracılığıyla Store Location Local Machine seçilerek ilerlenir. Sertifikanın depolanacağı yer Trusted Root Certification Authorities seçilerek devam edilir. Özet ekranında son kontroller yapıldıktan sonar işlem tamamlanır. Kontrol için Internet Explorer aracılığı ile https://scorenexus.isbank adresine gidilip herhangi bir sertifika sorunu olup olmadığına bakılır.summary.PNGstore.PNGwizard.PNGcertificate.PNG,"{'title': ""TFS Softtech Build Agent'larının scorenexus ile İletişimindeki Sertifika Sorunu Çözümü"", 'id': '3213821', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213821'}"
Sunucularımızda C:\Program Files\Isbank\IsHostNET PATH'i içerisinde bulunması gereken dosya ektedir. İlgili dosyanın olması gereken PATH'de olmaması veya silinmesi durumunda tekrar eklenmelidir ve periyodik olarak sunucularda ilgili .config dosyasının kontrolü sağlanmalıdır. 250,"{'title': 'IsHostNET Config Dosyası', 'id': '3213822', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213822'}"
"Ansbile Nedir? Ansible; uygulama dağıtımı, bulut sağlama, iç servis orkestrasyonu vb. işleri otomatize edebilen açık kaynaklı bir otomasyon aracıdır. Kullanımı basittir ve herhangi bir agent yapısı gerektirmez. Başlatılması ve çalıştırılması için herhangi bir ek servis, veritabanı vb. gerektirmez. Otomatize edilecek işleri tanımlamak için playbook yapısını kullanır. Playbook yapıları da insanların çok basit bir şekilde okuyup anlayabileceği YAML(.yml-.yaml) dilinde yazılırlar. Ansible Nasıl Çalışır? Ansible, makineleri(node) birbirlerine bağlar ve ""Ansible Modules"" denilen ufak programları yollar. Bu modüller de varsayılan bir şekilde SSH üzerinde çalıştırılırlar. Modüllerin işi bitince silinirler. 2 çeşit makine mevcuttur: Kontrol Makinesi(Management Node-Controlling Node)→Diğer makinelerin yönetiminin yapıldığı makinelerdir. Bütün playbook çalıştırılması işlemlerini kontrol ederler. Ansible kurulumu da bu makineye yapılır. Ayrıca içerisinde ansible modüllerinin çalıştırılması gerektiği ve ssh bağlantısı kurulması gereken hostların bir listesini tutan inventory dosyası mevcuttur. Uzak Makine(Remote Machine)→Kontrol makinesi tarafından kullanılan/kontrol edilen dosyalardır. Ansible Kurulumu Kuruluma başlamadan önce ""sudo apt-get update -y"" ve ""sudo apt-get upgrade -y"" komutlarıyla gerekli olan güncellemeler çekilir ve kurulur. Bu komutlarla beraber Ansible'ın çalışması için gereken Python da kurulmuş olur. Daha sonra ""sudo apt install software-properties-common"" komutuyla software-properties-common"" paketi kurulur. Bu paket repository yönetimini kolaylaştırmaktadır. ""sudo apt-add-repository --yes --update ppa:ansible/ansible"" komutuyla ile Ansible PPA kurulumu yapılarak, ""sudo apt-get update -y"" komutuyla tekrardan güncellemeler görülür. En son adımda ""sudo apt install ansible"" komutuyla kurulum tamamlanır. Kurulumun başarılı olup olmadığı ""ansible --version"" komutu kullanılarak kontrol edilebilir. Bu komut kurulan Ansible'ın bilgilerini verecektir. Ansible'ın dosyaları /etc/ansible konumunda olacaktır. roles klasöründe oluşturulan role yapıları, ansible.cfg dosyasında Ansible ayarları, hosts dosyasında uzaktan bağlanılacak makinelerin bilgileri tutulur. YAML Dosyası(.yml) Aşağıda kullanıcı oluşturma işlemi için yazılan playbook(createUser.yml) dosyası görülmektedir: name etiketinin altında Ansible playbook ismi tanımlanmıştır. Playbook'un yapacağı göreve göre mantıksal bir isim verilebilir. hosts ve connection etiketlerinde playbook'un hangi ip adreslerinde ve hangi makinelerde çalışacağı tanımlanmıştır. Burada bu playbook, localde çalışacak şekilde ayarlanmıştır. Localde çalışılması için bu 2 etiketin verilmesi zorunludur. tasks etiketi altında çalıştırılacak görevler listelenir. Bu etiketin altında kullanıcıyı oluşturacak bir görev, ismi verilerek tanımlanmıştır. Bu iş için user modülü kullanılmıştır. name etiketinde oluşturulacak kullanıcının kullanıcı ismi verilir. password etiketinde kullanıcı parolası tanımlanır. Ancak parola şifrelenmiş bir şekilde verilmelidir. Şifreleme işi için 2 yol uygulanmıştır. İlk yol mkpasswd hizmetini kullanmaktır. ""mkpasswd --method=sha-512"" komutu çalıştırılır, istenilen parola girilerek şifrelenmiş hali üretilir ve kopyalanıp YAML dosyasındaki password etiketine verilir. Eğer hizmet, kurulu değilse ""sudo apt-get install whois"" komutuyla kurulabilir. Hizmet, whois paketi içindedir. 2. yol olarak Python'a ait şifre hashleme kütüphanesi olan Passlib kütüphanesi kullanılabilir. ""pip install passlib"" komutuyla kütüphane kurulur ve ' python -c ""from passlib.hash import sha512_crypt; import getpass; print(sha512_crypt.using(rounds=5000).hash(getpass.getpass()))"" 'komutu çalıştırılarak aynı şekilde verilecek parola girilir. Daha sonra parolanın şifrelenmiş hali üretilir, aynı şekilde password etiketine yapıştırılır. Üretilen parolanın şifrelenmiş halinin doğru bir şekilde birebir kopyalanıp yapıştırıldığından emin olunmalıdır. Aksi takdirde kullanıcı oluşturulur ancak verilen parolayla giriş yapılamaz. Parolanın yanlış olduğu uyarısı verilir. state etiketinde hesabın varolup olmaması gerektiği belirtilir. shell etiketinde kullanıcının shelli belirtilir. system etiketi oluşturulan kullanıcının sistem kullanıcısı olup olmayacağını belirtir. createhome etiketi kullanıcı için bir home klasörü oluşturulup oluşturulmayacağını belirtir. home etiketi home klasörünün yerini ayarlar. Daha ayrıntılı bilgiler için "" https://docs.ansible.com/ansible/latest/reference_appendices/faq.html#how-do-i-generate-encrypted-passwords-for-the-user-module "" ve "" https://docs.ansible.com/ansible/latest/modules/user_module.html "" linklerine bakılabilir. Playbook Çalıştırma ve Kullanıcı Oluşturma YAML dosyasına bütün gerekli parametreler girildikten sonra dosyanın olduğu konumda bir terminal çalıştırılır. YAML dosyası ansible-playbook komutu ile çalıştırılır. Açılan terminalde ""sudo ansible-playbook createUser.yml"" komutu çalıştırılır ve kullanıcı oluşturma işlemi tamamlanır. Komutun başına sudo konularak root kullanıcı olarak çalıştırılması sağlanmıştır çünkü oluşturulan kullanıcı için /etc/passwd dosyasının güncellenmesi gerekmektedir. Aksi takdirde erişim yetkisi sorunundan dolayı kullanıcı oluşturulmayacaktır. Komut çalıştırıldıktan sonra Ansible, parolanın hashli olmaması vb. beklenmedik durumları uyarı olarak verecektir.ansible_etc.JPGansible_userlist.pngansible_home.JPGansible_user.JPGansible_create.pngansible_password.JPGansible_playbook.pngansible_yaml.JPGansible_version.JPGansible_works.jpgansible_logo.png","{'title': 'Ansible Kullanılarak YAML Scriptiyle Linux Ubuntu Üzerinde Yeni Bir Kullanıcı Oluşturulması', 'id': '3213935', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213935'}"
,"{'title': 'DevOps-Sprint Bültenleri', 'id': '3213941', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213941'}"
"Sprint Künyesi Sprintin Numarası 6 Sprintin Tarihi 14.10.2019 - 21.10.2019 Sprintin Amacı devops.softtech ürün gelişimi görevleri ve Karne hedeflerinde yer alan görevlerin takibi. Epic Bazında Dağılım false false Jira project%20%3D%20DEV%20AND%20%22Epic%20Link%22%20in%20(DEV-157%2C%20DEV-136%2C%20DEV-138%2C%20DEV-1)%20AND%20Sprint%20%3D%2034 customfield_10101 pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Görev Kapatanlar false false Jira project%20%3D%20DEV%20AND%20status%20%3D%20Done%20AND%20Sprint%20%3D%2034 assignees pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Tamamlanan Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Done AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 İşlemi Devam Eden Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = ""In Progress"" AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Açık Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Open AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41","{'title': 'Sprint 6 Bülteni', 'id': '3213943', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3213943'}"
"Problem find: ‘/var/lib/jenkins/workspace/SofttechDevops/Common/packs’: No such file or directory Solution Yerel bilgisayarda .Net paketleri olsa daha derleniyor, ancak jenkins (linux) üzerinde sadece dotnetcore paketleri ile derlenebiliyor. Eğer .Net Framework bağımlılığı varsa derlenmiyor ve nuget paketi oluşmuyor. Bu nedenle nexus'a yükleme sırasında hata alınıyor. Çözüm olarak .netcore'da olmayan paketlerin bağımlılıklardan çıkarılması gerekiyor. Related articles Related articles appear here based on the labels you select. Click to edit the macro and add or change labels. false 5 GUV false modified true page label in (""build"",""dotnet"",""dotnetcore"") and type = ""page"" and space = ""GUV"" dotnet dotnetcore build true Related issues","{'title': 'dotnetcore package upload to jenkins', 'id': '3214025', 'source': 'https://wiki.softtech.com.tr/display/SDO/dotnetcore+package+upload+to+jenkins'}"
,"{'title': 'Eğitimler', 'id': '3214061', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214061'}"
"Modern Authentication eğitimi sunularına erişebilirsiniz, 250 250 250 250 250 250 250 250 2509.Securing API with APIM.pdf8.OWIN.pdf7.ADAL.pdf6.Developing Applications.pdf5.AAD - B2C.pdf4.Azure AD.pdf3.ADFS.pdf2.OAuth2 and OpenID Connect.pdf1.Introduction.pdf","{'title': 'Modern Authentication Eğitim', 'id': '3214063', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214063'}"
"Sprint Künyesi Sprintin Numarası 7 Sprintin Tarihi 21.10.2019 - 28.10.2019 Sprintin Amacı devops.softtech ürün gelişimi görevleri ve Karne hedeflerinde yer alan görevlerin takibi. Epic Bazında Dağılım false false Jira project%20%3D%20DEV%20AND%20%22Epic%20Link%22%20in%20(DEV-157%2C%20DEV-136%2C%20DEV-138%2C%20DEV-1)%20AND%20Sprint%20%3D%2051 customfield_10101 pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Görev Kapatanlar false false Jira project%20%3D%20DEV%20AND%20status%20%3D%20Done%20AND%20Sprint%20%3D%2051 assignees pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Tamamlanan Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Done AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 İşlemi Devam Eden Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = ""In Progress"" AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Açık Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Open AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41","{'title': 'Sprint 7 Bülteni', 'id': '3214074', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214074'}"
3043cf8f-fa2d-4d20-8f4b-4b0ec5bde227 com.atlassian.confluence.plugins.confluence-knowledge-base:kb-troubleshooting-article-blueprint Add troubleshooting article 3043cf8f-fa2d-4d20-8f4b-4b0ec5bde227 com.atlassian.confluence.plugins.confluence-knowledge-base:kb-troubleshooting-article-blueprint kb-troubleshooting-article Provide solutions for commonly encountered problems. Troubleshooting article Add troubleshooting article kb-troubleshooting-article,"{'title': 'Troubleshooting articles', 'id': '3214080', 'source': 'https://wiki.softtech.com.tr/display/SDO/Troubleshooting+articles'}"
"SonarLint, kurulumu çok basit bir 'Derleyici' eklentisidir. SonarQube yöneticisi tarafından yazılımcı ile paylaşılan 'token' sayesinde şirket içinde kabul edilen SonarQube kuralları ile henüz geliştirme ortamında RCI endeksine uyum sağlanabilmektedir. SonarLint piyasada sıkça kullanılan derleyicilerin bir çoğu tarafından desteklenmektedir. Kurulumu için; https://www.sonarlint.org/ adresine gidilir ve uygun derleyiciye uygun eklenti indirilip, kurulur. Derleyici yeniden başlatılır. VS derleyicileri için üst menüden 'Analyze' içine girilir ve 'Manage SonarQube Connection' seçilir. SonarQube yöneticisi tarafından paylaşılan token bilgisi veya LDAP kullanıcı adı-şifresi (ad.soyad) ile mevcut SonarQube sunucusunun bilgileri ( https://sonar.softtech) girilir. Yetkisine sahip olduğunuz paketleri 'Team Explorer' içerisinde 'Connections' altında SonarQube sekmelerini takip ederek ulaşabilirsiniz.","{'title': 'SonarLint', 'id': '3214089', 'source': 'https://wiki.softtech.com.tr/display/SDO/SonarLint'}"
"Sprint Künyesi Sprintin Numarası 8 Sprintin Tarihi 28.10.2019 - 04.11.2019 Sprintin Amacı devops.softtech ürün gelişimi görevleri ve Karne hedeflerinde yer alan görevlerin takibi. Epic Bazında Dağılım false false Jira project%20%3D%20DEV%20AND%20%22Epic%20Link%22%20in%20(DEV-197%2C%20DEV-192%2C%20DEV-198%2C%20DEV-213%2C%20DEV-136%2C%20DEV-157)%20AND%20Sprint%20%3D%2052 customfield_10101 pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Görev Kapatanlar false false Jira project%20%3D%20DEV%20AND%20status%20%3D%20Done%20AND%20Sprint%20%3D%2052 assignees pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Tamamlanan Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Done AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 İşlemi Devam Eden Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = ""In Progress"" AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Açık Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Open AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41","{'title': 'Sprint 8 Bülteni', 'id': '3214112', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214112'}"
"Container Orkestrasyon aracı olarak Kubernetes ve bu ekosistem içinde yer alan bileşenler konusuda kurumsal farkındalık geliştirme, yetkinlik kazanma ve kazandırma, kurum içi farkındalık yaratma, gelişim ve uygulama ortamları yaratma konusunda çalışmalar yürütülecektir. Bu çalışma grubunun kapsamında, sınırlı olmamakla beraber aşağıdaki konular yer almaktadır, Container Orkestrasyon Araçları ile Entegre CI/CD Pipelineları geliştirme Container Orkestrasyon Araçları Container Orkestrasyon Aracı olarak Kubernetes Container Orkestrasyon Platformu olarak OKD (Origin Kubernetes Distribution) OKD Plotformunda Güvenlik Hybrit ve Federe OKD Uygulamaları OKD Üzerinde Uygulama Kurma ve Ölçekleme Pratikleri Container Orkestrasyon Araçları Üzerinde Service Mesh Uygulamaları (Istio vb.) OKD Üzerinde Metriklerin İzlenmesi OKD Sistem Metriklerinin İzlenmesi OKD üzerinde Container Performanslarının İzlenmesi Service Metriclerinin İzlenmesi Ekosistemde yer alan diğer bileşenler Hashi Corp Vault Entegrasyonu Istio Entegrasyonu 250 Çalışma Grubunun Çıktıları Çalışma grubunun hedef çıktıları arasında, Gerçek ortamda kurulmuş, çalışan, opere edilen, monitör edilen, güvenli ve ölçeklenebilen Kuberntes Altyapıları Kuberntes Altyapısında Sağlıklı bir şekilde çalışan service oriented mimari ile geliştirilmiş uygulamalar Kubernetes Altyapısını ve ilgili Uygulamaları kapsayan CI/CD Pipelineları Her türden Kurulum scripti, playbookları ve YAML konfigürasyon dosyaları (PATH: git repo for the documents) Derlenen bilgilere ait dokümantasyonların oluşturulması Toplantı 1 Notları Tarih: 11 Kasım 2019 Katılımcılar: Sinan, Yiğit, Hikmet, Taylan, Atacan, Ahmet İlk Aşama Çalışma Konuları Uygulamaların kurulması – githubtan örnek uygulamalar kurmak Okd security Hybrit, federe okd clusters Metrikler ve hpalar Hashi Corp Vault integrations Service Mesh (Istio) Toplantı 2 Notları Tarih: 18 Kasım 2019 Katılımcılar: Sinan, Yiğit, Melih, Alpay, Hikmet, Taylan Yapılan scripleri ve çalışmaları yönetmek için GitLab üzerinde proje açıldı. https://gitlab.softtech/softtechiac Görevlerin Dağılımı Hashi Corp Vault - Service Mesh (Istio, Linkerd) - , Metrikler ve HPAlar (Scaling) - Uygulama Kurulumları (CI/CD) - ManageIQ Kurulumu - Infra as Code - OpenShift - İkinci aşamada ele alınacak konular, Hibrit ve Federe Clusterlar OKD Security","{'title': 'Kubernetes ve Docker Altyapı Çalışma Grubu', 'id': '3214187', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214187'}"
"Sprint Künyesi Sprintin Numarası 9 Sprintin Tarihi 04.11.2019 - 11.11.2019 Sprintin Amacı devops.softtech ürün gelişimi görevleri ve Karne hedeflerinde yer alan görevlerin takibi. Epic Bazında Dağılım false false Jira project%20%3D%20DEV%20AND%20status%20%3D%20Done%20AND%20%22Epic%20Link%22%20in%20(DEV-213%2C%20DEV-198%2C%20DEV-192%2C%20DEV-157%2C%20DEV-197%2C%20DEV-136%2C%20DEV-212%2C%20DEV-138%2C%20DEV-1%2C%20DEV-220) customfield_10101 pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Görev Kapatanlar false false Jira project%20%3D%20DEV%20AND%20status%20%3D%20Done%20AND%20Sprint%20%3D%2053 assignees pie 450 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Tamamlanan Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Done AND Sprint = 53 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 İşlemi Devam Eden Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = ""In Progress"" AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 Açık Görevler Jira key,summary,created,assignee,status 30 project = DEV AND status = Open AND Sprint = 34 2f3948c0-ae52-36e0-9a8c-3c79fe243c41","{'title': 'Sprint 9 Bülteni', 'id': '3214302', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214302'}"
,"{'title': 'Bültenler', 'id': '3214433', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214433'}"
"Mevcut Yeni Ön/Arka Yayın Planı 1 - Paket sahipliği sorgulama ekranı geldi. Ön/Arka Kasım 2 - Arama konteynerı Arama konteynırı daha kompakt hale getirildi ve görsel iyileştirmeler sağlandı. Ön Kasım 3 - TFS anahtarları https://tfs.softtech anahtarları 1 yıllık güncellendi. Arka Kasım 4 - Yeni özellikler için bayraklar Yeni bayrakları kaldırıldı. Ön Kasım 5 - CX Trend hata mesajı İlgili hata mesajı düzeltildi index'te daha üst bir katmana alındı. Ön Kasım 6 - Raporlar DevOps, CX ve SQ verileri güncellendi. Ön Kasım 7- CX branch bilgisi CX branch bilgisinin yanında hangi repodan tarandığını gösteren bilgi eklendi. Ön/Arka 8- Paket sahipliği bilgisi Ürün dönüşümünden kaynaklı paketlerini göremeyen ekipler için bilgi ekranı eklendi. Ön Kasım 9- Paket envanteri bilgi ekranı Ürün dönüşümünden kaynaklı paketlerini göremeyen ekipler için bilgi ekranı eklendi. Ön Kasım 1- Paket sahipliği sorgulama ekranı Yazılım ekiplerini Genom'un iç içe yapısından alıp, daha kullanıcı dostu, sade ve işlevsel bir paket sorgu ekranı tasarlandı. 2-  Arama konteynerı Daha kompakt bir arama katmanı oluşturuldu. 4- Bayraklar Bir önceki versiyonda gelen yeni özellikleri belirtmek için eklenen 'Yeni' bayrakları kaldırıldı. 5- Hata mesajı CX tarafında kullanıcılara gösterilen hata mesajının katmanı yukarı taşındı ve 'tooltip' şeklinde güncellendi. 7- Raporlar Rapor ekranları (DevOps, CX ve SQ) tam ekran yapılarak ekran görüntüsü alma işlemi kolaylaştırıldı ve yeni veriler ile güncellendi. 8- Repository için gösterilen bilgi CX sayfasında branch isminin üstünde beklendiğinde hangi repository'den tarandığını ifade eden bir uyarı eklendi ve iyileştirmeler yapıldı. 9- Dönüşen ekiplerin paket sahiplik bilgisinin henüz genom ile eşleşmemesinden kaynaklı paketlerini göremeyen kullanıcılarımız için alınacak aksiyonların bulunduğu bir bilgi ekranı eklendi.icons8-new-50.pngimage2019-12-3_11-30-44.pngimage2019-11-25_14-9-58.pngimage2019-11-25_13-50-2.pngimage2019-11-25_13-49-14.pngimage2019-11-25_13-47-37.pngimage2019-11-25_13-46-45.pngimage2019-11-25_13-45-25.pngimage2019-11-25_13-42-20.pngimage2019-11-25_13-41-43.pngimage2019-11-25_13-41-12.png2-searchbar.png","{'title': 'Kasım Release Bülteni', 'id': '3214436', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214436'}"
"Vault modern bilgi işlemde kullanılan token'ları, şifreleri, sertifikaları, API key'leri ve diğer anahtarları depolar, güvence altına alır ve bunlara erişimi sıkı bir şekilde kontrol eder. https://learn.hashicorp.com/vault Bu kılavuzda Vault ile key value şeklinde saklanan değerlerin jenkins pipeline'ı üzerinde nasıl kullanıldığı anlatılmaktadır. Jenkins ile vault entegrasyonu için aşağıdaki adımlar izlenecektir: Versiyon 2 vault key value engine ayarlanması Policy oluşturulması Approle otorizasyonunun aktif edilmesi Jenkins'e özel policy ile approle oluşturulması Jenkins role id ve secret id'sinin oluşturulması Jenkins'e vault plugin'inin kurulması ve ayarları Pipeline'dan vault secret'larına erişim 1. Versiyon 2 Vault Key Value Secret Engine Ayarlanması Vault üzerinde çeşitli tiplerde secret engine oluşturmak mümkündür. Bu kılavuzda jenkins pipeline'nında kullanmak üzere key value engine (kv) oluşturulacaktır. KV engine V1 ve V2 olmak üzere iki versiyona sahiptir, bu kılavuzda Vault 0.10 ile tanıtılan daha gelişmiş özelliklere sahip V2 kv engine oluşturulup kullanılacaktır. Mevcut secret engine'lar aşağıdaki komut ile görüntülenebilir: vault secrets list Örnek çıktı: Path Type Accessor Description cubbyhole/ cubbyhole cubbyhole_c60dbdda per-token private secret storage identity/ identity identity_372628a7 identity store secret/ kv kv_b6474f8a key/value secret storage sys/ system system_9bc14db3 system endpoints used for control, policy and debugging Eğer bir kv engine varsa onu kullanarak devam edebilir veya yenisini oluşturabilirsiniz. Yeni bir v2 kv engine şu komutla oluşturulur: vault secrets enable -path=<path> kv-v2 Eğer V1 kv engine'a sahipseniz ve bunu kullanmak istiyorsanız V2'ye şu şekilde yükseltilebilir: vault kv enable-versioning <path-of-the-kv>/ Bu kılavuzda yeni bir secret engine yerine vault'un ön tanımlı gelen ve secret/ path'ine sahip kv engine'ı ile devam edilecektir. 2. Policy Oluşturulması Vault'taki her şey path temellidir. Politikalar, Vault'daki belirli path'lere ve işlemlere erişim izni vermek veya yasaklamak için kullanılır. Jenkins için bir policy oluşturmak için jenkins-policy-file.hcl şeklinde bir dosya oluşturup içine aşağıdaki örnek yazılabilir. # Read-only permission on 'secret/data/jenkins/\*' path

path ""secret/data/jenkins/\*"" {
capabilities = [ ""read"" ]
} Bu örnekteki politika, secret/jenkins altındaki alan için yalnızca okuma izni sağlamaktadır. ""secret/ data /jenkins"" V2 kv engine'de ilk path'den sonra araya data gelmelidir. KV secret engine'de V1'den V2'ye yükseltme yapıldığında mevcut politikalarınız varsa bu şekilde düzenlemeniz gerekir. Oluşturduğunuz dosyayı kullanarak jenkins adında bir policy oluşturmak için şu kod kullanılır: vault policy write jenkins jenkins-policy-file.hcl 3. Approle Otorizasyonunun Aktif Edilmesi Vault içerisinde aynı secret engine'lardaki gibi farklı otorizasyon metodları bulunmaktadır. Bu metodlar içerisinde uygulamalar arası haberleşmek için AppRole kullanılır. Rol oluşturmadan önce AppRole metodunun şu kod ile aktif hale getirilmesi gerekir. vault auth enable approle 4. Jenkins'e Özel Policy ile AppRole Oluşturulması Jenkins adında, jenkins policy'si kullanılarak rol oluşturmak için şu kod kullanılır. vault write auth/approle/role/jenkins policies=""jenkins"" Oluşturduğunuz rolü konfirme etmek için şu kod ile detaylarına bakılır. vault read auth/approle/role/jenkins 5. Jenkins Role Id ve Secret Id'sinin Oluşturulması Oluşturulan jenkins rolüne ait Role Id ve Secret Id jenkins'e vault erişiminde gereklidir. Bunlar aşağıdaki kod ile oluşturulup not edilmelidir. Role Id için: vault read auth/approle/role/jenkins/role-id Secret Id için: vault write -f auth/approle/role/jenkins/secret-id 6. Jenkins'e Vault Plugin'inin Kurulması ve Ayarları Vault'a erişim ayarları ve pipeline'da vault kullanımı için Jenkins'e şu plugin kurulur: HashiCorp Vault Plugin Kind'ı Vault App Role Credential olacak şekilde daha önce not edilen Role Id ve Secret Id girilerek yeni bir jenkins credential yaratılır. Jenkins sistem konfigürasyonu altında bulunan Vault Plugin altındaki Vault URL girilir ve oluşturulan credential seçilir. 7. Pipeline'dan Vault Secret'larına Erişim Pipeline üzerinden vault kv secretlarına environment variable olarak erişim sağlanabilir. Bunun için örnek pipeline kodu aşağıdaki gibidir. node {
    // define the secrets and the env variables
    // engine version can be defined on secret, job, folder or global.
    // the default is engine version 2 unless otherwise specified globally.
    def secrets = [
        [path: 'secret/jenkins/<path>',
                engineVersion: 2, 
                secretValues: [
                    [envVar: 'user', vaultKey: 'user'],
                    [envVar: 'pass', vaultKey: 'pass']
                ]
        ]
    ]

    // inside this block your credentials will be available as env variables
    withVault([vaultSecrets: secrets]) {
        sh 'echo $user'
        sh 'echo $pass'
    }

}vault-approle-workflow.pngvault_credentials.PNGvault_config.PNGauth.PNGsecret_engines.PNG","{'title': 'Vault & Jenkins Entegrasyonu', 'id': '3214583', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214583'}"
Varsa; datamodel'in dokümante edilerek wiki'de paylaşılması Varsa; Component mimarisinin hazırlanması ve wiki'de yayınlanması varsa; Entegrasyon mimarisinin hazırlanması ve wiki'de yayınlanması Varsa; Fiziksel/Mantıksal network ve sunucu mimarisinin hazırlanması ve wiki'de yayınlanması Analiz dökümanlarının wiki'de yayınlanması Sistemin inputları kullanıcıları üreteceği çıktılar Etkileştiği diğer sistemler Ekran görüntüleri Test senaryoları Logların doğru şekilde atıldığının teyit edilmesi Batch işlerde hatalı/başarılı işlerin sonunda bilgilendirme mail'i atılması Yapılan değişikliklerin sonar/checkmarx'a yansıdığının teyit edilmesi birim test coverage'ının artması Major bulgu oluşmaması Test senaryolarının işletilerek sonuçların yansıtılması CI/CD ve kalite işleri,"{'title': 'Kabul Kriterleri', 'id': '3214591', 'source': 'https://wiki.softtech.com.tr/display/SDO/Kabul+Kriterleri'}"
,"{'title': 'Ürün - Envanter Eşleşmesi', 'id': '3214604', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214604'}"
"About This product includes some services provided by the consolidation with PYS and Genom to show package ownerships. Identifying a unique ID (Softtech Component Code -it will also call like SCC) for each repositories opened and thus providing a management will ensure sustainable to administrate of application architectures. Notice: The requests to open new repositories must be managed on this screens to ensure architecture system. Inputs Genom and PYS Outputs packageInfo Response Services for Inputs getGenomInfo The ‘Post Request’ with this API https://genomapi.apps.generic.kube.isbank/api/v2/package with body like: { ""Code"":""ACICode"" } getPYSInfo The services are given by PYS administrators. Services for Outputs packageInfo The service needs to some information about like these: reposityoryInfo (TFS, gogs, gitlab etc.) language (C#, java etc.) hostInfo –optional- (Linux, Windows) deployable (true or false) packageStatus (active, retired) tecknichalOwner (like someone, some leadship) UK ? Services for architecture Create repositories Diagrams Will be added Timeline To create getGenomInfo needs 3 work days and to write tests need 2 work days. To create getPYSInfo needs to understanding necessaries. So It doesn’t calculate any scheduling time. (Estimated time: 5 work days with its tests) To create packageInfo needs min 6 work days and to write tests need 3 work days. To create automated new repositories need 3 work days and to write tests need 2 work days. To change ownerships need 3 work days and to write tests need 2 work days. 15/12 16/12 17/12 18/12 19/12 Discussing and last changes Get services from getPYSInfo getGenomInfo Front-End developments getGenomInfo Front-End developments getGenomInfo getGenomInfo Test getGenomInfo Test 22/12 23/12 24/12 25/12 26/12 getPYSInfo Front-End developments getPYSInfo Front-End developments getPYSInfo Front-End developments getPYSInfo getPYSInfo Test getPYSInfo getPYSInfo Test 29/12 30/12 31/12 1/1 2/1 packageInfo Front-End developments packageInfo Front-End developments packageInfo Front-End developments Holiday packageInfo Test 5/1 6/1 7/1 8/1 9/1 packageInfo packageInfo Test packageInfo packageInfo Test createRepository Front-End developments createRepository Front-End developments createRepository Front-End developments 12/1 13/1 14/1 15/1 16/1 createRepository Test createRepository Test changeOwnership Front-End developments changeOwnership Front-End developments changeOwnership Front-End developments 19/1 20/1 21/1 22/1 23/1 changeOwnership Test changeOwnership Test UAT Test Improvements Test Improvements 26/1 27/1 28/1 29/1 30/1 Test Improvements Test Improvements Test Improvements Test Improvements Prod","{'title': 'Documents of architacture', 'id': '3214606', 'source': 'https://wiki.softtech.com.tr/display/SDO/Documents+of+architacture'}"
"Softtech bünyesindeki paketlerin devops.softtech içerisinde görünür kılınmasına yönelik yürüttüğümüz çalışma kapsamında ürün dönüşümü gerçekleşen ekiplerin alt kırılımları aşağıdaki gibi olmuştur. Geçtiğimiz hafta PYS Ekibi ile devops.softtech entegrasyonu için biraraya geldik ve ürün grubundan takım kırılımına kadar güncel sahiplik bilgisini portföy ekibinden temin edebileceğiz. Sonrasında ihityacımız olan paket sahipliği detayına bu bilgiler ışığında erişeceğiz. Bu çalışmaya yönelik ortaya koyduğumuz veri modeli, aşağıda olduğu gibi son halini almıştır. PostgreSQL ve MSSQL ortamında veri tabanlarıı oluşturularak örnek teşkil etmesi açısından devops.softtech ürününe ait paketler ile . Her iki db için backup dosyaları aşağıdadır. 250 250dbo.pngCapture.JPG","{'title': 'Güncel Ürün Dönüşümü Veri Modeli', 'id': '3214637', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3214637'}"
image2019-12-12_8-53-16.pngimage2019-12-12_8-43-48.png,"{'title': 'Minimal Solution', 'id': '3214653', 'source': 'https://wiki.softtech.com.tr/display/SDO/Minimal+Solution'}"
"SAC means Softtech Application Code (UK) SCC means Softtech Compoment Code (ACI) Necceseries to Data Collection Must a screen coordinated by system admins and must get this given parameters; --Package Name --Package Code --Application Code --Ownership --External Git Url --Used Technology When create a project in https://devops.softtech , a package needs this veriables on Package, PackageInfo, UKInformation. Table Package PackageId → will be generate automatically Name → Required UKCode → Required Owner → Must be SOFTTECH ACICode → Required Version → Optional but must be an integer DirectorshipCode → SF Table PackageInfo PackageInfiId → will be generated automatically ACICode → Required ExternalTFSUrl → Required TfsBuildId → Optional Technology → Required LineCondition → Required LogicalNodes → Optional Branch → Required IsScoreTFS → Required Table UKInformation UKCode must be map a DirectorshipMapCode After this changing in tables, the packages will be show on https://devops.softtech Notice : To show SonarQube and Checkmarx results, the package codes will be same all tools and https://devops.softtech DBsimage2019-12-12_9-16-52.pngimage2019-12-12_8-58-39.pngimage2019-12-12_8-57-53.pngimage2019-12-12_8-53-16.pngimage2019-12-12_8-43-48.png","{'title': 'Analysis', 'id': '3214658', 'source': 'https://wiki.softtech.com.tr/display/SDO/Analysis'}"
"When the user enter 'Urun Yonetimi' screen in administration page, the screen will meet. Softtech Component Code → SCC must be unique (ACI) Softtech Application Code → SAC must be unique (UK) Paket Adı → SCC Name Git Adresi → Repository Teknoloji → Java, .Net Müdürlük → DirectorshipCode (SF code)image2019-12-16_10-20-20.pngimage2019-12-16_10-19-11.pngimage2019-12-16_10-17-11.png","{'title': 'Screen', 'id': '3214710', 'source': 'https://wiki.softtech.com.tr/display/SDO/Screen'}"
image2019-12-20_11-13-30.pngimage2019-12-20_11-11-24.pngimage2019-12-20_11-8-26.png,"{'title': 'Flow Chart', 'id': '3214715', 'source': 'https://wiki.softtech.com.tr/display/SDO/Flow+Chart'}"
250,"{'title': 'BatchJobs', 'id': '3214782', 'source': 'https://wiki.softtech.com.tr/display/SDO/BatchJobs'}"
image2019-12-23_14-1-5.pngimage2019-12-23_12-32-56.pngimage2019-12-23_12-32-17.pngimage2019-12-23_12-30-20.png,"{'title': 'Architecture', 'id': '3214787', 'source': 'https://wiki.softtech.com.tr/display/SDO/Architecture'}"
"GraphQL, RESTful API call'larını optimize etme amacıyla Facebook tarafından geliştirilen açık kaynak kodlu bir veri sorgulama dilidir. Kullanılma amacı kompleks hale gelen ve response süresi uzayan RESTful API requestlerinin daha hızlı response dönmesini ve kompleksliğinin azaltılmasını sağlamaktır. Neden GraphQL? GraphQL kullanılarak (RESTful calların aksine) istenen veri query syntaxı kullanılarak kısıtlanabilir. Bu sayede internet üzerinden gelen veri miktarında büyük bir azalma sağlanabilir. Ayrıca birden fazla RESTful call ile çekilebilecek veri bir query kullanılarak çekilebilir. Bu da işlemin kompleksliğinin azalmasını sağlar. GraphQL DB independent bir yapıya sahiptir. NoSQL ve Relational DB'ler ile kullanılabilir. GraphQL Server veriyi DB'lerden alabileceği gibi RESTful API ya da Microservicelerle entegre olarak da alabilir. Server ve schema yapısı hybrid bir şekilde de kurulabilir. Aynı zamanda hem birden fazla DB'den hem de microservicelerden ve/veya RESTful API'lardan veri alınabilir. Figure 1: GraphQL Hybrid Approach Güçlü bir sorgu syntaxine sahiptir. Bu sebeple veriyi gezme(traversing), modifiye etme ve veriyi elde etmeyi kolaylaştırır. Server-Side Components 1) Schema: GraphQL schema GraphQL server yapısının ana birimidir. Serverın cilent tarafından kullanılacak olan fonksiyonelliğini tanımlar. Örnek: type Query { greeting:String students:[Student] } type Student { id:ID! firstName:String lastName:String password:String collegeId:String } 2) Query: Client tarafından gelen spesifik bir formata sahip, server tarafından parselanabilen basit stringlerdir. Client application, server tarafındaki schemaya uygun bir şekildeki queryleri kullanarak veriyi elde etmek için servera istek gönderir. query myQuery{
   greeting
} 3) Resolver: Client tarafından gelen querylerin veya mutationların schema yapısına uygun bir şekilde çözümlenmesini ve gerekli işlemin(traversing, modifying, retrieving data etc.) yapılmasını sağlar. greeting:() => {    return ""hello from  Server!"" } Client-Side Components 1) GraphiQL: Browser tabanlı bir önyüzdür. Developerların GraphQL queryleri ve mutationları test edip, düzenlemesine yardımcı olur. 2) Apollo Client: GraphQL client appliacationları arasıdna en çok kullanılan araçtır. Client-Server Architecture Figure 2: Client-Server Architecture Type System Schemayı tanımlarken kullanılan veri tipleridir. 1) Scalar: tek bir değer depolar.(Int, Float, String, Boolean, ID) greeting: String 2) Object: bir grup objeyi tutmaya yarar. Her obje bir veri tipine maplenir. type Student {    stud_id:ID    firstname: String    age: Int    score:Float } 3) Query Type: Queryler serverdan veriyi çekmek için kullanılan operasyonlardır. Queryleri tanımlamak için Schema Definition Language(SDL) kullanılır. type Query {    field1: data_type    field2:data_type    field2(param1:data_type,param2:data_type,...paramN:data_type):data_type } type Query  {
   greeting: String
} 4) Mutation Type: Mutationlar Servera veriyi oluşturmak, silmek ya da güncellemek için gönderilen operasyonlardır. type Mutation {    field1: data_type    field2(param1:data_type,param2:data_type,...paramN:data_type):data_type  } type Mutation {
   addStudent(firstName: String, lastName: String): Student
} 5) Enum Type: Önceden saptanmış ve değişiklik göstermeyen listeler için kullanılır. Haftanın günleri gibi. type Days_of_Week{    SUNDAY    MONDAY    TUESDAY    WEDNESDAY    THURSDAY    FRIDAY    SATURDAY } 6) List Type: Spesifik bir veri tipi olan bir dizeyi tutmak için kullanılır. type Query {    todos: [String] } 7)Non-Nullable Type: Herhangi bir veri tipini Non-Nullable yapmak için sonuna '!' işaretini koymak yeterli. field:data_type! type Student {
   stud_id:ID!
   firstName:String
   lastName:String
   fullName:String
   college:College
} Not: Client appliacation veriyi elde etmek istiyorsa servera query gönderir ve veri dönmesini bekler. Veriyi manipüle etmek(create, update, delete, patch etc.) istiyorsa schemaya uygun bir şekilde servera mutation gönderir. Figure 3: GraphiQL kullanarak queryilerin test edilmesi Reference: Tutorialspoint Gitlab Repos: graphql-examplesgraphql_hybrid_approach.jpgExample_pic.PNGClient-Server architecture Example.PNG","{'title': 'GraphQL', 'id': '3214823', 'source': 'https://wiki.softtech.com.tr/display/SDO/GraphQL'}"
"Uygulamaların yetkisiz erişim, veri sızıntıları vb. güvenlik açıklarının statik ve dinamik code analizleriyle tespit edilmesini ve bu açıkların raporlanması hizmetini verir. Scanning Tools 1) Static Application Security Testing (SAST) 2) Dynamic Application Security Testing (DAST) 3) Container Scanning 4) GitLab Security Dashboard 5) Dependency Scanning 6) Dependency List 7) License Compliance 1. Static Application Security Testing (SAST) Kaynak kodu bilinen güvenlik zafiyetlerinin olup olmadığını anlamak için analizinin yapılmasıdır. Merge işlemi sırasında kaynak ve hedef branchler arasındaki zaafiyet farklarını gösterir. Figure 1: Merge Example SAST SAST sonuçları zaafiyet açıklarına göre aşağıdaki gibi sıralanabilir: 1. Critical
    2. High
    3. Medium
    4. Low
    5. Unknown
    6. Everything else gitlab-ci.yml dosyasına SAST.gitlab-ci.yml template'i include edilerek aktifleştirilir. Aşağıdaki örnekte olduğu gibi environment variablelar kullanılarak yapılandırılabilir. include:
  template: SAST.gitlab-ci.yml

sast:
  variables:
    CI_DEBUG_TRACE: ""true"" Desteklenen programlama dilleri ve frameworkler aşağıdaki gibidir. Figure 2: Desteklenen Programlama dilleri 2. Dynamic Application Security Testing (DAST) Uygulama deploy edildikten sonra çalışan sistemin gelebilecek ataklara(cross-site scripting or broken authentication flaws etc.) karşı güvenlik açığı olup olmadığının tespiti için kullanılır. Çalışan uygulamaların analizini yaparken OWASP ZAProxy toolunu kullanır. Pasif analiz(ZAP Baseline Scan) ve aktif analiz yaparak rapor verir. Pasif analiz default olarak gelir. Aktif analiz aşağıdaki gibi konfigürasyonlarla açılabilir. Aktif analiz daha geniş bir rapor vermek için uygulamaya belirli periyodlarla(haftalık) saldırı yapılarak açıklarının tespit edilmesini sağlar. Figure 3: Merge Example DAST include:
  template: DAST.gitlab-ci.yml

variables:
  DAST_FULL_SCAN_ENABLED: ""true"" 3. Container Scanning Containerlar için açık tespit eden Clair ve klar araçlarını kullanarak, kullanılan Docker imagelarının güvenlik açıklarını tespir eder. include: template: Container-Scanning.gitlab-ci.yml 4. GitLab Security Dashboard Gruplar, projeler ve pipeline'lar özelindeki açıkları genel bir ekranda gösterir. Dashboard'dan faydalanabilmek için öncelikle konfigurasyonunun yapılması gerekmektedir. Desteklenen raporlar aşağıdaki gibidir: Container Scanning Dynamic Application Security Testing Dependency Scanning Static Application Security Testing Güvenlik açıkları dismiss edilebilir ya da açığın çözülmesi için issue oluşturulup çözülmesi için assign edilebilir. Figure 4: Group Security Dashboard Figure 5: Project Security Dashboard Figure 6: Pipeline Security Dashboard 5. Dependency Scanning Uygulamanın kullandığı external library'lerin açıklarının tespit edilmesini sağlar. .gitlab-ci.yml dosyasına Dependency-Scanning.gitlab-ci.yml template'i include edilerek aktifleştirilir. include:
  template: Dependency-Scanning.gitlab-ci.yml

dependency_scanning:
  variables:
    CI_DEBUG_TRACE: ""true"" Figure 7: Merge Dependency Scanning 6. Dependency List Proje dependencylerinin ve önemli detaylarının açıklarıyla birlikte gösterilmesini sağlar. Gereklilikleri: Dependency Scanning jobı ci pipelinenında tanımlanmş olmalı Projelerin Gemnasium tarafından desteklenen en az bir dili veya package managerı kullanması gerekmektedir. Figure 8: Merge Dependency List 7. License Compliance Projenin kullandığı lisansların uyumluluğunu kontrol eder. Aktifleştirilmesi için .gitlab-ci.yml dosyasına License-Management.gitlab-ci.yml template'i include edilmelidir. include:
  template: License-Management.gitlab-ci.yml

license_management:
  variables:
    LM_PYTHON_VERSION: 2 References GitLab Secure SAST DAST Container Scannig Security Dashboard Dependency Scanning Dependency List License Compliance Gemnasiumdependency_list_v12_4.pngdependency_scanning.pngpipeline_security_dashboard_v12_6.pngproject_security_dashboard_v12_3.pnggroup_security_dashboard_v12_6.pngdesteklenen.PNGdast.pngsast.png","{'title': 'GitLab Secure', 'id': '3214840', 'source': 'https://wiki.softtech.com.tr/display/SDO/GitLab+Secure'}"
"Mevcut Yeni Ön/Arka Yayın Planı 1 - Guide modülü 'Yardım', 'İndir' ve 'Yeni Neler Var?' butonları eklendi. Ön Aralık 2 - Hata mesajı (Gelmeyen veri için) Hata mesajları, görünümleri de standart hale getirilerek düzenlendi. Ön Aralık 3 - Hata mesajı (Yanlış veri) Yanlış bir formatta gönderilen veri için uyarı mesajı eklendi. Ön/Arka Aralık 4 - Bilgi butonları Bilgi butonları güncellendi ve standart haline getirildi. Ön Aralık 5 - Softtech içi yazılan ürünler için kayıt ekranı Ön/Arka Aralık 6- Softtech içi envanter sorgulama ekranı. (Sadece Genom verileri için) Ön/Arka Aralık 7- Raporlar sayfası Araçlardan gelen güncel veriler eklendi ve bazı görsel iyileştirmeler sağlandı Ön Aralık 8- SonarQube Tablosu Tarih, branch ve durum bilgileri eklendi. Ön/Arka Aralık 9- Checkmarx Queue Tablosu Kuyruktaki taramalar ve aktif taramalar şeklinde iki tablo yapıldı. Ön/Arka Aralık 1) Her tablo sayfası için sağ üst kısımda 'tool-content' şeklinde güncellendi. 2) Hata mesajları standart haline getirildi ve 'error-content' şeklinde güncellendi. 3) Kullanıcının yanlış veri girmesiyle gelen boş ekran düzeltildi ve hata mesajı basıldı. 4) Daha anlaşılır ikonlar. 5) Softtech içi yazılan paketlerin envantere kaydedilebilmesi için; bu sayfada istenen verilerin girilmesi gerekmektedir. Böylece paketlerin sistem üzerinde kaydı sağlanıyor ve https://devops.softtech platformunda paketin gösterimi sağlanabiliyor. 6) Paket Sorgulama ekranı ile kullanıcının ACI veya UK kodları ile yapacağı sorgularda, sorgu için detay bilgi gösterebilen bir sayfa hazırlandı. Böylece paket sahiplik sorunları gibi durumlarda ekipleri Genom önyüzünün karmaşıklığından kurtarıp sade ve ulaşılmak istenen bilgiler ile kullanıcı bilgilendirilebiliyor. Şimdilik sadece Genom'dan gelen veriler gösteriliyor. 7) Karne ile aynı oranları yansıtabilmek adına veriler güncenlendi. Ayrıca, grafik içerisinde yer alan verilerin daha rahat okunabilmesi ve göze çarpması için bazı iyileştirmeler yapıldı. 8) SonarQube tablosuna analiz edilen branch, en son build tarihi ve en son build durumunu gösteren alanlar eklendi. 9) Tarama tabloları ayrıldı. Aktif taramalar ve Kuyruktaki taramalar şeklinde iki tablo oluşturuldu. Böylece ekipler daha rahat bir şekilde paketlerinin durumlarını görebiliyor olacaklar.icons8-new-50.pngimage2020-1-14_11-1-57.pngimage2020-1-9_11-16-41.pngimage2020-1-9_11-12-38.pngimage2020-1-9_11-10-44.pngimage2020-1-9_11-7-10.pngimage2020-1-2_9-55-29.pngimage2020-1-2_9-49-48.pngimage2020-1-2_9-49-4.pngimage2020-1-2_9-45-53.pngimage2020-1-2_9-43-50.pngimage2020-1-2_9-40-29.pngimage2020-1-2_9-34-44.png","{'title': 'Aralık Release Bülteni', 'id': '3215771', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=3215771'}"
"Etcd CoreOS takımı tarafından oluşturulmuş açık kaynaklı ve dağıtık çalışan key-value saklamaya yarayan bir yapıdır. İsmi unix'teki global konfigürasyon ayar dosyalarının yer aldığı /etc klasörüne gönderme yapar. Pek çok dağıtık sistem için güvenilir bir şekilde veri saklaması sayesinde omurga görevi görür. Linux, bsd ve OS X gibi farklı işletim sistemlerinde çalışabilir. Etcd Özellikleri Yinelenik: Tüm kayıtlar cluster'daki her node'da bulunur. Yüksek müsaitlik: Etcd donanım ve network'lerden bir kaçı sorun yaşasa bile bundan etkilenmeyecek şekilde tasarlanmıştır. Tutarlılık: Okunan her değer her zaman en güncel değeri döner Basitlik: İyi tanımlanmış ve kullanıcılara yönelik bir API içerir Güvenlik: İsteğe bağlı olarak istemci sertifikası kimlik doğrulaması ile otomatik TLS uygular. Hız: Saniyede 10.000 yazma işlemi gerçekleştirebilir. Güvenilirlik: Kayıtlar, Raft algoritması kullanılarak düzgün bir şekilde dağıtılır. Etcd Nasıl Çalışır Etcd'nin nasıl çalıştığını anlamak için üç temel kavramı tanımlamak önemlidir: - liderler - seçimler - dönemler Raft tabanlı bir sistemde, cluster bir lider seçmek için seçim yapar Gelen isteklerden yazma gibi veri tutarlılığı gerektiren işlemler liderler tarafından işlenir. Fakat okuma gibi işlemler cluster'daki her node tarafından karşılanabilir. Liderler yeni değişiklikleri kabul etmekten, bilgileri takipçi node'lara kopyalamaktan ve ardından node'lardan teyit aldıktan sonra değişiklikleri yapmaktan sorumludur. Her cluster'ın aynı anda yalnızca bir lideri olabilir. Bir lider ölürse veya artık yanıt vermiyorsa, node'ların geri kalanı yeni bir lider seçmek için önceden belirlenmiş bir zaman aşımından sonra yeni bir seçime başlayacaktır. Her node, yeni bir seçim başlatmadan ve kendini aday olarak seçmeden önce bekleyeceği süreyi temsil eden rastgele bir seçim zamanlayıcısı tutar. Eğer bir zaman aşımı gerçekleşmeden önce node liderden yanıt alamazsa yeni bir döneme başlayarak, kendini aday olarak işaretleyerek ve diğer node'lardan oy isteyerek yeni bir seçime başlar. Her node, oylamasını isteyen ilk aday için oy kullanır. Bir aday cluster'daki node'ların çoğundan oy alırsa, yeni lider olur. Seçim zaman aşımı her node'da farklılık gösterdiğinden, ilk aday genellikle yeni lider olur. Ancak, birden fazla aday varsa ve aynı sayıda oy alırsa, mevcut seçim süresi lider olmadan sona erecek ve yeni bir dönem, yeni rastgele seçim zamanlayıcılarıyla başlayacaktır. Yukarıda belirtildiği gibi, herhangi bir değişiklik lider node'a yönlendirilmelidir. Etcd, değişikliği hemen kabul etmek ve yerine getirmek yerine, node'ların çoğunun değişiklik üzerinde anlaşmasını sağlamak için Raft algoritmasını kullanır. Lider, önerilen yeni değeri cluster'daki her node'a gönderir. Node'lar daha sonra yeni değerin alındığını onaylayan bir mesaj gönderir. Node'ların çoğu teyit mesajını gönderirse, lider yeni değeri taahhüt eder ve her node'a değerin günlüğe kaydedildiğini bildirir. Bu, her değişikliğin yapılabilmesi için cluster node'larından yeterli çoğunluk gerektirdiği anlamına gelir. Etcd ve Kubernetes Etcd’nin Kubernetes'teki işi, dağıtılmış sistemler için kritik verileri güvenli bir şekilde saklamaktır. Etcd; kubernetes'in yapılandırma verilerini, durumunu ve meta verilerini depolamak için kullanılan en iyi ve kubernetes'in birincil veri deposu olarak bilinir. Kubernetes genellikle birkaç cluster ile çalıştığından, Etcd gibi dağıtılmış bir veri deposu gerektiren bir sistemdir. Etcd, verileri bir cluster'da depolamayı ve değişiklikleri izlemeyi kolaylaştırır, böylece Kubernetes cluster'ındaki herhangi bir node'un veri okumasına ve yazmasına izin verir. Etcd’nin izleme (watch) işlevselliği Kubernetes tarafından sistemin gerçek veya istenen durumundaki değişikliklerini izlemek için kullanılır. Eğer farklılık varsa, Kubernetes iki durumu uzlaştırmak için değişiklikler yapar. Kubectl komutunun her okuması Etcd'de saklanan verilerden alınır, yapılan herhangi bir değişiklik (kubectl apply) Etcd'de kayıt oluşturur veya günceller. Her crash etcd'deki değer değişikliği tetikler. Etcd ile etkileşim Etcd ile etkileşime girmenin iki ana yolu vardır: - etcdctl komutunu kullanarak - doğrudan RESTful API aracılığıyla etcdctl , Etcd sunucusuyla etkileşim kurmak için kullanılan bir komut satırı arabirimidir. Key'leri ayarlama, güncelleme veya kaldırma, cluster sağlığını doğrulama, Etcd node'ları ekleme veya kaldırma ve veritabanı snapshot'ları oluşturma gibi çeşitli eylemleri gerçekleştirmek için kullanılabilir. Varsayılan olarak etcdctl , geriye dönük uyumluluk için Etcd sunucusuyla v2 API'sini kullanır. etcdctl komutunun v3 API'sını kullanarak Etcd ile konuşmasını istiyorsanız, ETCDCTL_API ortam değişkeni üzerinden sürümü “3” olarak ayarlamanız (export ETCDCTL_API=3) gerekir. API ise, bir Etcd sunucusuna gönderilen her istek bir gRPC çağrısıdır. Bu gRPC gateway, HTTP / JSON isteklerini gRPC iletilerine çeviren bir RESTful proxy sunar. 1. etcdctl etcdctl versiyon öğrenme: $ etcdctl version
etcdctl version: 3.1.0-alpha.0+git
API version: 3.1 etcdctl key yazma okuma silme izleme Yazma $ etcdctl put foo bar
OK Ayrıca, lease eklenerek belirli bir süre için bir anahtar ayarlanabilir. İşte foo1 anahtarının değerini 10 saniye için bar1 olarak ayarlama komutu: $ etcdctl lease grant 10
lease 32695410dcc0ca06 granted with TTL(10s)
$ etcdctl put --lease=32695410dcc0ca06 foo1 bar1
OK Okuma Tek değer $ etcdctl get foo
foo
bar Yalnızca değer $ etcdctl get foo --print-value-only
bar Aralık foo = bar
foo1 = bar1
foo2 = bar2
foo3 = bar3 değerlerinin kayıtlı olduğunu varsayarsak foo ... foo3 aralığını okuma: $ etcdctl get foo foo3
foo
bar
foo1
bar1
foo2
bar2 Prefix kullanarak okuma $ etcdctl get --prefix foo
foo
bar
foo1
bar1
foo2
bar2
foo3
bar3 Prefix kullanarak gelen sonucu limitleme $ etcdctl get --prefix --limit=2 foo
foo
bar
foo1
bar1 Eski versiyonlar foo = bar         # revision = 2
foo1 = bar1       # revision = 3
foo = bar_new     # revision = 4
foo1 = bar1_new   # revision = 5 değerlerinin kayıtlı olduğunu varsayarsak $ etcdctl get --prefix foo # en güncel versiyona ulaşma
foo
bar_new
foo1
bar1_new

$ etcdctl get --prefix --rev=4 foo # 4. revizyona ulaşma
foo
bar_new
foo1
bar1

$ etcdctl get --prefix --rev=3 foo # 3. revizyona ulaşma
foo
bar
foo1
bar1

$ etcdctl get --prefix --rev=2 foo # 2. revizyona ulaşma
foo
bar

$ etcdctl get --prefix --rev=1 foo # ilk revizyona ulaşma Silme Uygulamalar etcd cluster'ından bir anahtarı veya bir dizi anahtarı silebilir. keys:
foo = bar
foo1 = bar1
foo3 = bar3
zoo = val
zoo1 = val1
zoo2 = val2
a = 123
b = 456
z = 789 değerlerinin kayıtlı olduğunu varsayarsak Tek değer $ etcdctl del foo
1 # one key is deleted Aralık foo foo9 aralığını silme $ etcdctl del foo foo9
2 # two keys are deleted Prefix kullanarak silme $ etcdctl del --prefix zoo
2 # two keys are deleted
Here is the command to delete keys which are greater than or equal to the byte value of key b :
$ etcdctl del --from-key b
2 # two keys are deleted Silinen anahtar / değer çiftini döndürme komutu $ etcdctl del --prev-kv zoo
1   # one key is deleted
zoo # deleted key
val # the value of the deleted key İzleme Anahtar değer üzerindeki değişiklikler aşağıdaki komutlar ile izlenebilir. Tek değer $ etcdctl watch foo
# in another terminal: etcdctl put foo bar
PUT
foo
bar Aralık $ etcdctl watch foo foo9
# in another terminal: etcdctl put foo bar
PUT
foo
bar
# in another terminal: etcdctl put foo1 bar1
PUT
foo1
bar1 Prefix ile $ etcdctl watch --prefix foo
# in another terminal: etcdctl put foo bar
PUT
foo
bar
# in another terminal: etcdctl put fooz1 barz1
PUT
fooz1
barz1 etcdctl lease oluşturma Uygulamalar, bir etcd cluster'ında anahtarlar için lease verebilir. Bir lease'e bir anahtar eklendiğinde, kullanım ömrü lease süresinin ömrüne bağlıdır ve bu da time-to-live (TTL) ile yönetilir. Her bir lease, uygulama tarafından belirlenen minimum yaşam süresi (TTL) değerine sahiptir. Bir lease'in TTL süresi sona erdiğinde bağlı tüm anahtarlar silinir. Lease Oluşturma # grant a lease with 10 second TTL
$ etcdctl lease grant 10
lease 32695410dcc0ca06 granted with TTL(10s)

# attach key foo to lease 32695410dcc0ca06
$ etcdctl put --lease=32695410dcc0ca06 foo bar
OK Lease revoke etme $ etcdctl lease revoke 32695410dcc0ca06
lease 32695410dcc0ca06 revoked

$ etcdctl get foo
# empty response since foo is deleted due to lease revocation Keep-Alive Uygulamalar, süresinin dolmaması için TTL'sini yenileyerek lease'i canlı tutabilir. $ etcdctl lease grant 10
lease 32695410dcc0ca06 granted with TTL(10s) Bu şekilde oluşturulmuş bir lease olduğunu varsayarsak aşağıdaki komutla ölmemesi sağlanabilir. $ etcdctl lease keep-alive 32695410dcc0ca06
lease 32695410dcc0ca06 keepalived with TTL(10)
lease 32695410dcc0ca06 keepalived with TTL(10)
lease 32695410dcc0ca06 keepalived with TTL(10)
... Lease hakkında bilgi alma # grant a lease with 500 second TTL
$ etcdctl lease grant 500
lease 694d5765fc71500b granted with TTL(500s)

# attach key zoo1 to lease 694d5765fc71500b
$ etcdctl put zoo1 val1 --lease=694d5765fc71500b
OK

# attach key zoo2 to lease 694d5765fc71500b
$ etcdctl put zoo2 val2 --lease=694d5765fc71500b
OK Bu şekilde oluşturulmuş bir lease olduğunu varsayarsak $ etcdctl lease timetolive 694d5765fc71500b
lease 694d5765fc71500b granted with TTL(500s), remaining(258s) Yukarıdaki komutla lease hakkında bilgi alınabilir, ya da aşağıdaki komutla lease'e bağlı anahtar değer çifti görüntülenebilir. $ etcdctl lease timetolive --keys 694d5765fc71500b
lease 694d5765fc71500b granted with TTL(500s), remaining(132s), attached keys([zoo2 zoo1])

# if the lease has expired or does not exist it will give the below response:
Error:  etcdserver: requested lease not found 2- etcd API gRPC gateway Key yazma ve silme <<COMMENT
https://www.base64encode.org/
foo is 'Zm9v' in Base64
bar is 'YmFy'
COMMENT

curl -L http://localhost:2379/v3/kv/put \
  -X POST -d '{""key"": ""Zm9v"", ""value"": ""YmFy""}'
# {""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""2"",""raft_term"":""3""}}

curl -L http://localhost:2379/v3/kv/range \
  -X POST -d '{""key"": ""Zm9v""}'
# {""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""2"",""raft_term"":""3""},""kvs"":[{""key"":""Zm9v"",""create_revision"":""2"",""mod_revision"":""2"",""version"":""1"",""value"":""YmFy""}],""count"":""1""}

# get all keys prefixed with ""foo""
curl -L http://localhost:2379/v3/kv/range \
  -X POST -d '{""key"": ""Zm9v"", ""range_end"": ""Zm9w""}'
# {""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""2"",""raft_term"":""3""},""kvs"":[{""key"":""Zm9v"",""create_revision"":""2"",""mod_revision"":""2"",""version"":""1"",""value"":""YmFy""}],""count"":""1""} Key İzleme curl -N http://localhost:2379/v3/watch \
  -X POST -d '{""create_request"": {""key"":""Zm9v""} }' &
# {""result"":{""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""1"",""raft_term"":""2""},""created"":true}}

curl -L http://localhost:2379/v3/kv/put \
  -X POST -d '{""key"": ""Zm9v"", ""value"": ""YmFy""}' >/dev/null 2>&1
# {""result"":{""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""2"",""raft_term"":""2""},""events"":[{""kv"":{""key"":""Zm9v"",""create_revision"":""2"",""mod_revision"":""2"",""version"":""1"",""value"":""YmFy""}}]}} Transactions # target CREATE
curl -L http://localhost:2379/v3/kv/txn \
  -X POST \
  -d '{""compare"":[{""target"":""CREATE"",""key"":""Zm9v"",""createRevision"":""2""}],""success"":[{""requestPut"":{""key"":""Zm9v"",""value"":""YmFy""}}]}'
# {""header"":{""cluster_id"":""12585971608760269493"",""member_id"":""13847567121247652255"",""revision"":""3"",""raft_term"":""2""},""succeeded"":true,""responses"":[{""response_put"":{""header"":{""revision"":""3""}}}]} # target VERSION
curl -L http://localhost:2379/v3/kv/txn \
  -X POST \
  -d '{""compare"":[{""version"":""4"",""result"":""EQUAL"",""target"":""VERSION"",""key"":""Zm9v""}],""success"":[{""requestRange"":{""key"":""Zm9v""}}]}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""6"",""raft_term"":""3""},""succeeded"":true,""responses"":[{""response_range"":{""header"":{""revision"":""6""},""kvs"":[{""key"":""Zm9v"",""create_revision"":""2"",""mod_revision"":""6"",""version"":""4"",""value"":""YmF6""}],""count"":""1""}}]} Kimlik Doğrulama # create root user
curl -L http://localhost:2379/v3/auth/user/add \
  -X POST -d '{""name"": ""root"", ""password"": ""pass""}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""1"",""raft_term"":""2""}}

# create root role
curl -L http://localhost:2379/v3/auth/role/add \
  -X POST -d '{""name"": ""root""}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""1"",""raft_term"":""2""}}

# grant root role
curl -L http://localhost:2379/v3/auth/user/grant \
  -X POST -d '{""user"": ""root"", ""role"": ""root""}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""1"",""raft_term"":""2""}}

# enable auth
curl -L http://localhost:2379/v3/auth/enable -X POST -d '{}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""1"",""raft_term"":""2""}} Token ile kimlik doğrulama # get the auth token for the root user
curl -L http://localhost:2379/v3/auth/authenticate \
  -X POST -d '{""name"": ""root"", ""password"": ""pass""}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""1"",""raft_term"":""2""},""token"":""sssvIpwfnLAcWAQH.9""} Otorizasyon token'ını header'da kullanma curl -L http://localhost:2379/v3/kv/put \
  -H 'Authorization : sssvIpwfnLAcWAQH.9' \
  -X POST -d '{""key"": ""Zm9v"", ""value"": ""YmFy""}'
# {""header"":{""cluster_id"":""14841639068965178418"",""member_id"":""10276657743932975437"",""revision"":""2"",""raft_term"":""2""}}","{'title': 'Etcd', 'id': '3215785', 'source': 'https://wiki.softtech.com.tr/display/SDO/Etcd'}"
"Mevcut Yeni Çalışma Yayın Planı 1- Karışık ve farklı argümanlar içeren menüler. Solda yer alan menüler sadeleştirildi ve araçlar için anlaşılır ve standart bir hale getirildi. Ön Şubat 2- Tablodaki paketler için işlev butonları. Butonlar minimize edildi ve ekranda daha çok veri gösterilmesi sağlandı. Ön Şubat 3- Baz değerleri. 2019 baz değerleri kaldırıldı ve 2020 çalışmaları tamamlanana kadar ilgili uyarı ekranı tasarlandı. Ayrıca; indirme butonları da bu doğrultuda uyarı verecek şekilde güncellendi. Ön Şubat 4- Checkmarx 'Kuyruktaki Taramalar' tabloları. İki tablo içeren bu sayfa 'tab' görünümüne alındı ve daha kompakt görünmesi sağlandı. Ayrıca; ilgili tablolara filtreleme özelliği eklendi. Ön Şubat 5- Checkmarx 'Güncel Değeler' tabloları. Tablodaki yazıların kayma sorunu giderildi. Ön Şubat 6- Checkmarx 'Trend Artışı Olan Paketler' tablosu. Görsel iyileştirmeler ve filtreleme özelliği eklendi. Ön Şubat 7- Güvenli Kod Geliştirme Ligi için nihai görünüm sağlandı. Ön Şubat 8- 'Raporlar' sayfası. Raporlar sayfası içerisinde SQ, CX ve DevOps oranları yıl sonu değerleri ile güncellendi ve karne notları yanlarına yansıtıldı. Ön Şubat 9- Checkmarx Yönetim Sayfası genel hatlarıyla tamamlandı. Ön Şubat 1) Menüler Menüler standart bir hale getirildi. 2) Araçların 'Gösterim' ekranları. Araçların güncel değerlerinin gösterildiği ekranlarda butonlar küçültüldü ve böylece gelen veriler için daha geniş bir alan oluşması sağlandı. 3) Baz Liste ekranları. (Hem SonarQube hem Checkmarx) 2020 'Baz Çalışmalar' tamamlanana kadar ekipleri karşılayacak bir uyarı ekranı eklendi. 4) Checkmarx Kuyruktaki Taramalar ekranı. Tablolar iki tab şeklinde ayrıldı ve 'Paket Adı' kısmına filtreleme özelliği getirildi. 5) Checkmarx 'Güncel Değerler' ekranı. Tablo başlıklarında yer alan harf kayma sorunu giderildi. 6) Checkmarx 'Trend Artışı Olan Paketler' ekranı. Tablo daha kompakt bir hale getirildi ve 'Paket Adı' kısmına altındaki değerlerde 'Hızlı Arama' sağlayacak bir filtreleme mekanizması getirildi. 7) 'Güvenli Kod Geliştirme Ligi' ekranı. 'Güvenli Kod Geliştirme Ligi' stabil ve nihai görünümüne kavuştu. 8) 'Raporlar' ekranı. Yıl sonu güncel verileri ve karne notu eklendi 9) Checkmarx 'Araç Yönetim Sayfası' ekranı. Checkmarx servisleri ile ön-yüz güçlü tablo özellikleri ile entegre edildi. Checkmarx için rapor alma servisleri ile entegrasyon sağlandı ve UK ile birlikte bir e-posta adresi ile beraber raporların iletilmesi sağlandı. High Trend değerinde artış olan paketlere bilgi gönderebilmek üzere buton eklendi. Güncellenecek servisler ile beraber 'Tarama Başlat' ve 'Rapor Al' butonları da aktif olacaktır.image2020-2-10_19-43-23.pngicons8-new-50.pngimage2020-2-10_19-39-6.pngimage2020-2-10_19-38-9.pngimage2020-2-10_19-37-18.pngimage2020-2-10_19-37-8.pngimage2020-2-10_19-36-14.pngimage2020-2-10_19-35-28.pngimage2020-2-10_19-34-13.pngimage2020-2-10_19-33-0.pngimage2020-2-10_19-29-30.png","{'title': 'Şubat Release Bülteni', 'id': '10356102', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=10356102'}"
,"{'title': 'Test Otomasyonu', 'id': '10356241', 'source': 'https://wiki.softtech.com.tr/display/SDO/Test+Otomasyonu'}"
"Kaynak kodun test edildiği testler ""White-Box Testler"" olarak isimlendirilir. Uygulamanın davranışının test edildiği testler ise ""Black-Box Testler"" olarak isimlendirilir. ""White-Box Testler"" 'de çok değerli olmakla birlikte, Test otomasyonu kapsamında sadece ""Black-Box Testler"" bulunmaktadır! Test Otomasyonları ürünün bir parçasıdır ve geliştirilebilmeleri için domain uzmanlığı gerekmektedir. Bu nedenle test otomasyonları ürün takımı tarafından geliştirilir ve bakımı yapılır. Ürün Takımları test otomasyon'da kullanacak senaryoları İş Analisti liderliğinde hazırlar. Bu senaryoların kapsamı Müşterinin kabul test ekibi tarafından kontrol edilir. Ürün takımları ihtiyaç ve beklentilerini test yatayına katılarak bildirip sürecin sürekli iyileşmesine katkıda bulunurlar. belirtilen ağırlıklar göz önüne alınarak hangi test tipinde ne kadar ağırlık verileceğine karar verilir. Test otomasyonunun çalışma sıklığı ve çalışacağı ortam göz önüne alınarak testlerin maximum çalışma süreleri belirlenmelidir. Bu konuda detayları matrisinde bulabilirsiniz. Test otomasyonu için var olan test ortamları kullanılacaktır. Bu nedenle manuel ve otomatik testler aynı test ortamında çalışacaklar. Bu nedenle test otomasyonunda kullanılacak datalar test öncesinde oluşturulur ve testlerin tamamlanmasından sonra temizlenir. Ürün Takımı; Kendi ürünü için test datası üretme ve test datası temizleme servislerini açarak kendi kullanımına ve diğer ekiplerin kullanımına sunar. Test ortamlarının şirket genelinde pratiklerinin ortaklaşması, devops hattına entegre edilmesi ve süreç içerisinden otomatik ya da manuel olarak çalıştırılması, test sonuçlarının raporlanması, otomatik kurulum için gerekli test kabul kriterlerinin belirlenmesi Test Otomasyon ekibi tarafından Yapılır. Tüm  bu çalışmalarda test yatayı üzerinden ürün takımlarının sağladığı geri bildirimler dikkate alınır ve sürekli iyileştirilir. Aşağıdaki işlerin sorumluluğu Test otomasyon ekibi tarafından üstlenilir. Test otomasyonu için kullanılan araçların yönetimi DevOps entegrasyonları Test otomasyon sonuç raporlarının oluşturulması, sunulması ve saklanması Test otomasyon KPI'larının ölçümlenmesi Ekipler arası koordinasyonun sağlanması ve ortak pratiklerin oluşturulması Ürün takımları kendileri için değerli olan test senaryolarını tespit etmek için aşağıdaki kriterlere göz önüne almalıdır. İş Katma Değeri; Test otomasyonuna yapılan yatırıma yüksek geri dönüşü(ROI) sağlaması Risk; Olası bir hatanın yaratacağı etkinin yüksek olduğu uygulamalar/componentler Test Otomasyon Eforu; Otomatize edilmesi planlana akışlar için harcanması gereken efor Test Otomasyon için Gereken kaynaklar; Test otomasyonunun çalışması için gereken kaynaklar Uygulama Sürüm sıklığı; Uygulamaya ne sıklıkla yeni özellikler eklendiği, yeni geliştirilen bir ürünmü yoksa bakımı yapılan bir ürün mü olduğu Uygulamanın Ömrü; Uygulamanın ne kadar uzun bir süre hizmet vereceği Test Otomasyonuna Uygunluk; Uygulamanın geliştirildiği teknolojiler, kurgulandığı mimari, diğer uygulamalara bağımlılıklar gibi konular. Regresyon Test Piramidi piramit Test pramidinin altında yer alan testler mümkün olduğunda geliştirme ortamına yakın çalıştırılırlar. Geliştirilmesi görece kolay, çalışma hızı yüksek ve kaynak tüketimi düşük olan testler piramidin altındadır. Piramidin altında yer alan testlere ağırlık verilmelidir, üste doğru çıktıkça testlerin maliyeti arttığı için bu tip testler oran olarak daha az geliştirilmelidir. Regresyon Test Seviyesi - Test Ortam Matrisi matrisimage2020-2-13_16-57-58.pngimage2020-2-13_16-56-47.png","{'title': 'Test Otomasyonu Prensipleri', 'id': '10356243', 'source': 'https://wiki.softtech.com.tr/display/SDO/Test+Otomasyonu+Prensipleri'}"
Planlanan versiyon: v1.4.0 Mevcut Yeni Geliştirme Yayın Planı 1 Ligler menüsü. Front-end / Back -end Mart 2 Hakkında menüsü. Front-end Mart 3 Avatar ve isim kısmı. İlgili alan küçültüldü ve daha kompakt bir hale getirildi. Front-end Mart 4 Softtech DevOps menüsü Anasayfa olarak değiştirildi Front-end Mart 5 Açılış mesajı. Hem devops ekibinin gördüğü hem de iç müşterinin muhatap olduğu ana ekran mesajı duyurular olarak güncellendi. Front-end Mart 6 Hakkında menüsü eklendi ve devops.softtech işlevleri hakkında detay bilgiler verildi. Front-end Mart 7 Her sayfanın en alt kısmına versiyonlama bilgileri eklendi. Front-end Mart 8 Ligler ayrı bir menüye taşındı ve 'Yeni' ikonu eklendi. Front-end Mart 9 'Güvenli Kod Geliştirme' liginde eksik gelen UAT verileri. 'Güvenli Kod Geliştirme' liginde UAT verileri düzeltildi. Back-end Mart 10 'Güvenli Kod Geliştirme' ligi içerisinde Süper Ligte yer alan hatalı Eğitim Tamamlama oranı bilgisi. Düzeltildi. Front-end Mart 11 'Güvenli Kod Geliştirme' liginde yer alan verilerin son güncelleme tarihi. Front-end / Back-end Mart 12 Ligler içerisinde nasıl bir sıralama yapıldığına dair bilgi gösterimi sağlandı ve sıralamalar düzeltildi. Front-end / Back-end Mart 13 Kalite liginde sıralama eklendi. Front-end Mart 14 Tabloda görünmeyen veriler. Tablo uzunluğuna responsive değerler verilerek sorun giderildi. Front-end Mart 1-) Ligler ayrı bir menüye alındı. 2-) Hakkında menüsü eklendi. 3-) İsim ve avatar ortak tasarım diline alındı. 4-) DevOps Softtech yazısı Anasayfa olarak değişti. 5-) Karşılama ekranına 'duyurular' eklendi. 6-) Hakkında menüsü içerisine detaylı bilgi verildi. 7-) Her sayfaya versiyonlama eklendi. 8-) Ligler menüsünün ikonları 'Yeni' şeklinde güncellendi. 12-) Güvenli Kod Geliştirme liginde tablo sıralamalarının nasıl yapıldığına dair bilgi verildi. 13-) Kalite ligine Sıralama eklendi.image2020-3-24_0-6-8.pngimage2020-3-24_0-5-41.pngimage2020-3-24_0-4-40.pngimage2020-3-24_0-4-15.pngimage2020-3-24_0-3-33.pngimage2020-3-24_0-3-6.pngimage2020-3-24_0-2-4.pngimage2020-3-24_0-1-41.pngimage2020-3-24_0-0-44.pngimage2020-3-24_0-0-20.pngimage2020-3-23_23-59-46.pngicons8-new-50.png,"{'title': 'Mart Release Bülteni', 'id': '10357166', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=10357166'}"
Test Otomasyon Entegrasyonu,"{'title': 'Test Otomasyon', 'id': '13959603', 'source': 'https://wiki.softtech.com.tr/display/SDO/Test+Otomasyon'}"
"1) DB Side 2) Service Side To Set New Value updateReportValues [post] [authorize] Scenario Get information about product_groups , scenario_type, and value(for history table) from front-end Create a row in ""history"" for values above with date_value. And set given value as current_value in ""report"" table To Get Current Value getCurrentValues [get] [authorize] Scenario The response must have scope, report and historical values ​​corresponding to a product group. History must be month by month. (It can be used last values for all months.) The response can be like that; tableData: [{ product_group_name: 'Kart İhraç', healthCheck: 0, smoke: 0, regression: 0, healthCheck_automated: 0, smoke_automated: 0, regression_automated: 0, history: [""ocak: 0"", ""şubat: 0""], graph: [0,0], scope: 'Çalışmalar Devam Ediyor' }, { product_group_name: 'Kart Kabul', healthCheck: 18, smoke: 89, regression: 0, healthCheck_automated: 0, smoke_automated: 0, regression_automated: 0, history: [""ocak: 0"", ""şubat: 0""], graph: [0,0] scope: 'Çalışmalar Tamamlandı' } ]image2020-4-15_19-46-6.pngimage2020-4-15_19-25-14.pngimage2020-4-15_18-48-52.pngimage2020-4-14_19-12-54.pngimage2020-4-14_17-8-14.pngimage2020-4-14_15-53-13.png","{'title': 'DB Relations and Service Requirements', 'id': '13959605', 'source': 'https://wiki.softtech.com.tr/display/SDO/DB+Relations+and+Service+Requirements'}"
,"{'title': 'Scope Page', 'id': '13959642', 'source': 'https://wiki.softtech.com.tr/display/SDO/Scope+Page'}"
,"{'title': 'Statistical Page', 'id': '13959644', 'source': 'https://wiki.softtech.com.tr/display/SDO/Statistical+Page'}"
"devops.softtech v1.5.0 ve devops.softtech v1.6.1 ID Mevcut Yeni Geliştirme Tarafı Yayın Planı 1- Duyuru ekranının güncellenmesi. Front-end Haziran 2- SonarQube menüsü SonarQube menüsüne 'Genel Bakış' sekmesinin eklenmesi. Back-end/Front-end Haziran 3- Checkmarx menüsü Checkmarx menüsüne 2020 yılı için 'Baz' listenin dahil edilmesi. Back-end/Front-end Haziran 4- Checkmarx menüsü Checkmarx menüsüne 'Genel Bakış' sekmesinin eklenmesi. Back-end/Front-end Haziran 5- Test otomasyonu menüsünün eklenmesi. (v1.6.1) Back-end/Front-end Haziran 6- Raporlar menüsü Test otomasyon sekmesinin eklenmesi (v1.6.1) Back-end/Front-end Haziran 1- Duyurular Duyurular menüsü 'card' tasarım diline kavuşturuldu ve hepsine ilgili sayfalara yönlendirebilmesi için 'router' eklendi. Sayfanın en altında devops.softtech'in versiyonlanmasına gidildi. İlgili versiyon numarasının üstüne fare imlecinin gelmesi ile değişikliklerin bir özetinin gösterimi sağlandı. 2- SonarQube | Genel Bakış SonarQube menüsünün altına Ürün Grubu kırılımından başlamak üzere Ürün Takımı ve sahip olduğu paketlere kadar kırılım sağlanarak ilgili araç üzerinde takip süreci kolaylaştırıldı. SonarQube üzerinde 'Portfolio' mantığına eş değer bir çalışma tamamlanmış oldu. Kırılımların sonunda paket adına tıklanması ile SonarQube'da ilgili paketin detaylarına erişim için bir yönlendirme eklendi. Böylece SonarQube tarafı için gereksinimlere özel bir çalışma yapıldı. 3- Checkmarx | Baz Değerler Checkmarx menüsü altında konumlandırılan 'Baz Değerler' sekmesi; 2020 yılı başında gerçekleştirilen baz çalışmalarının gösterimi sağlandı. Arama ve indirme fonksiyonu eklendi. 4- Checkmarx | Genel Bakış Checkmarx menüsü altına yerleştirilen 'Genel Bakış' sekmesi ile Checkmarx menüsü altında araca özel, şirket hedefleri ile paralel bir şekilde veri gösterimi sağlandı. Böylece verilerin sıralanması ile direktörlüklerinde bu şekilde sıralanması ile Güvenli Kod Geliştirme ligi'ne çağrışım gerçekleşti. Burada Ürün Grubu ve Ürün Takımı kırılımı sağlanmış oldu. 5- Test Otomasyonu Menüsü (v1.6.1) Test Otomasyonu metriklerinin gösterimi için çeşitli kırılımlarda sekmeler eklendi. a) Kapsam sekmesinin görünümü b) Genel Bakış sekmesinin görünümü c) Kazanımlar sekmesinin görünümü d) Banka Dışı / Test Otomasyon çalışmalarının gösterimi 6) Raporlar | Test Otomasyonu (v1.6.1) İlgili sekme altında, aylara göre Test Otomasyonu sayıları ve ilgili sayıların değişimin takibi için bir sayfa tasarlandı.image2020-5-28_16-13-47.pngimage2020-5-28_16-12-0.pngimage2020-5-28_16-10-59.pngimage2020-5-28_16-10-8.pngimage2020-5-28_16-9-34.pngimage2020-5-28_16-5-14.pngimage2020-5-28_16-2-1.pngimage2020-5-28_15-56-46.pngimage2020-5-28_15-53-52.pngimage2020-5-28_15-50-54.pngicons8-new-50.png","{'title': 'Haziran Release Bülteni', 'id': '16646179', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=16646179'}"
"Gitlab Rally entegrasyonu gitlab.rally.softtech'te bulunan ve ölçülmesi gereken bazı repoları gitlab.softtech üzerindeki ilgili repoya kopyalayan scripttir. Başlık Bilgi Açıklama Sunucu 10.222.8.86 Jenkins sunucusunda (10.222.8.86) çalışıyor Dizin /data/remote-project-sync Kritik Dosyalar remote_project_sync.py repolardan verileri çeken script Kritik Dosyalar config.yaml bağlantı kurulacak gitlab sunucusu ve kopyalanacak repoları barındıran script Crontab jenkins kullanıcısı altında 12.08.2020 tarihinde ilgili crontab ayarı: Her Pazar günü saat 9'da çalışacak şekilde, 0 9 * * 7 timeout 30m python3 /data/remote-project-sync/remote_project_sync.py Crontab dosyası /var/spool/cron/crontabs/jenkins Jenkins kullanıcısı ile açılan crontab dosyası bu dosya ve dizin altında saklanmaktadır Operasyonel güncelleme # jenkins kullanıcısına geçiş su - jenkins # crontab edit etmek için crontab -e Kaynaklar https://help.ubuntu.com/community/CronHowto","{'title': 'Gitlab Rally Replikasyon İşi', 'id': '18908172', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18908172'}"
,"{'title': 'Middleware Bileşenler', 'id': '18908174', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18908174'}"
"Anthos Test ortamının Softtech sunucu ortamlarına kurulumu ele alınmaktadır. Test ortamının kurulum adımları, GCP Creating Project & Service Accounts Anthos Whitelisting & Trial Application VMware Configuration Preparing VMWare stack for Anthos installation Preparing Networking stack for Anthos installation Anthos Deploying Anthos GKE On-Premise Creating user clusters Logging Stackdriver Integration Deploying Elasticsearch as a Internal Logging 3rd Party Application (Market Place üzerinden kurulmuştur) Service Mesh Anthos Service Mesh Demonstration Configuration Management Anthos Config Management Demonstration (configürasyon reposu olarak github.com ’daki kurumsal hesabımızda proje açılmıştır) Monitoring Deploying Prometheus for application metric tracking (kurulum başarıyla tamamlandı, grafana üzerinden metriklerin bir kısmı eksik görüntüleniyor. Bu metriklere ilişkin ek çalışma yapılacak.) CI/CD Jenkins Pipeline Integration HELM Configuration Application Deployment 2PM Uygulama Kurulumu Softtech Application Deployment (Plato) - Edefter Uygulaması POB Uygulaması","{'title': 'Anthos Test Ortamı', 'id': '18908662', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18908662'}"
3 Network Planlaması Anthos 2 Network Yapısı Netork Adı Network Aralığı Vlan Açıklama System Network 192.168.104.0/24 1213 DNS ve Host sunucuları Anthos2 Management 10.224.0.0/24 1265 Beta ortamları için Jumpboxlar Anthos2 Admin Cluster 10.224.1.0/24 1266 Beta ortamları için admin cluster Anthos2 Beta Cluster 10.224.3.0/24 1268 Beta ortamları clusterı Genel Network Yapısı Alan IP Aralığı Açıklama Network Adı Prod Ortamı 192.168.0.0/20 Prod Ortamı kurulumları 1214 Prod Network Beta Ortamı 192.168.96.0/21 Beta Ortamı kurulumları 1215 Beta Network Management Network 192.168.104.0/24 Management networkü 1213 VM Network Prod Extranet 192.168.105.0/24 Prod ortamı için tanımlanan ilk extranet 1216 Prod Extranet Servis Networkü 192.168.112.0/20 User Clusterlarda servisler için ayrılan IP bloğu Pod Networkü 192.168.128.0/17 User Clusterlarda Podlar için ayrılan IP bloğu Alan IP Aralığı Açıklama Network Adı Data Plateau 172.22.1.0/24 Data Plateau Kule 3 Networkü Data Pkateau - Kule 3 Anthos Prod Ortamı Domain adı: prod.st vCenter ResourcePool: prod Alan IP Aralığı Açıklama Vlan tanımı 192.168.0.0/20 Prod ortamında yer alacak kaynakların bulunduran subnet. Port Group 192.168.0.0/21 Prod ortamında host sunucularını barındıracak subnet. VIP Port Group 192.168.8.0/21 Prod ortamında VIP Iplerini barındıracak subnet. Anthos Beta Test Ortamı Anthos Beta Test Ortamında temel olarak Anthos ile ilgili testler yürütülecektir. Domain: antbeta.st ResourcePool: antbeta Alan IP Aralığı Açıklama Vlan tanımı 192.168.96.0/21 Beta ortamında yer alacak kaynakların bulunduran subnet. Port Group 192.168.96.0/22 Beta ortamında host sunucularını barındıracak subnet. VIP Port Group 192.168.100.0/22 Beta ortamında VIP Iplerini barındıracak subnet. Servis ve Pod Networkleri Tanımlanacak olan her bir user cluster altında tanımlanacak Servisler ve Podlar aşağıdaki ağlarda bulunan IP aralıklarını kullanacaklardır. Alan IP Aralığı Açıklama Servis Networkü 192.168.112.0/20 Anthos User Clusterlar altında tanımlanan bir servisin IP kullanacağı blok aralığıdır. Sistem tarafından dinamik olarak atanır. Pod Networkü 192.168.128.0/17 Anthos User Clusterların altında tanımlanan Podların IP kullanacakları blok aralığıdır. Otomatik olarak tanımlanır.,"{'title': 'Anthos Network Alt Yapısı', 'id': '18909542', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18909542'}"
,"{'title': 'Gitlab', 'id': '18909612', 'source': 'https://wiki.softtech.com.tr/display/SDO/Gitlab'}"
"Gitlab sertifika yönetimi Gitlab'ın kendi configürasyon dosyaları üzerinden yönetilmektedir. Sertifika yenileme işlemi için aşağıdaki adımlar takip edilmelidir. Adımlar Lokasyon Açıklama Sertifikaları ekleme /etc/certs/gitlab cer, key dosyaları bu dizin altına kopyalanır. Aynı zamanda key dosyasına erişmek üzere kullanılacak anahtarı barındıran dosyada bu dizine yer alır. Konfigürasyon dosyası güncelleme /etc/gitlab burada gitlab.rb dosyası bir editör ile edit edilir. Aşağıdaki satırlar uygun şekilde güncellenir. ... nginx['ssl_certificate'] = ""/etc/certs/gitlab/gitlab.softtech_20200922_1045_pem.cer"" nginx['ssl_certificate_key'] = ""/etc/certs/gitlab/gitlab.softtech_20200922_1045_pem.key"" nginx['custom_gitlab_server_config'] = 'ssl_password_file ""/etc/certs/gitlab/ssl_key_2020"";' ... Gitlabı yeniden başlatma Aşağıdaki komut çalıştırılır, gitlab-ctl reconfigure başarısız olunması durumunda, gitlab-ctl restart denenir. Sertifika tarafında sağlanan şifrenin çalışma durumu aşağıdaki komut kullanılarak kontroller yapılabilir, openssl rsa -in /etc/certs/gitlab/gitlab.softtech_20200922_1045_pem.key gelen şifre ekranında erişim şifresi girilir. Sonuçta düzgün bir şekilde alanlar görülmeli.image2020-9-22_15-34-13.png","{'title': 'Gitlab Sertifika Güncelleme', 'id': '18909614', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18909614'}"
"Anthos üzerinde host edilen projelerin bilgileri bu alanda derlenmiştir. Proje İletişim Eposta Prod Beta Poc Açıklama Eren Erdoğan Eren.Erdogan@softtech.com.tr Devrede Kapatıldı (30.11.2020) Edefter Serkan Uman Serkan.Uman@softtech.com.tr Planlama Devrede AHE Erhan Koçak UAT Tedarikçi Finansmanı Planlama ERP Finance Planlama P2P Planlama Temel Bankacılık Orkun Uğur, Ferhan Ceyhun Planlama ERP HR Serkan Uman Serkan.Uman@softtech.com.tr Planlama Tayfun Aydın Tayfun.Aydin@softtech.com.tr Devrede Devrede Planlama API Platformu Umut Tozlu, Mehmet Güneş, Serkan Sezer Planlama Welder, Direct API Bordro Servisi Şükran Atalay, Şule, Serkan Sezer Planlama Dışarıdan alınan uygulama üzerinden farklı bordro hesaplama hizmeti sağlanacak. Data Plateau Çetin Yalçın Güleç, Shivam Aurora Planlama Planlama Data analiz ürünü, birkaç bileşenden oluşan bir Data Analiz platformu Data ML Çetin Yalçın Güleç, Hakan Olgun Planlama Planlama Data ML için kullanılan GPU makinelerin taşınması söz konusu. İşNet'ten böyle bir çözüm de istenebilir.","{'title': ""Anthos'ta Host Edilen Projeler"", 'id': '18911918', 'source': 'https://wiki.softtech.com.tr/display/SDO/Anthos%27ta+Host+Edilen+Projeler'}"
"3 PoB (Point of Business) projesi Plateau ortamı üzerinde geliştirilmiştir. Mevcut durumda sistem üzerinde çalışan servisler, PoB Projesinde Kullanılan Sunucular PoB projesi kapsamında Anthos Kubernetes Ortamına ek olarak aşağıdaki sunucular kullanılmaktadır, No Sunucu Adı Ortam IP İşlemci Bellek (GB) Disk (GB) Açıklama Durum 1 PMYSQLPOB11 Prod 192.168.7.129 4 32 100 + 300 Aktif - Pasif MySQL kurulumu, Master sunucu Aktif 2 PMYSQLPOB12 Prod 192.168.7.130 4 32 100 + 300 Aktif - Pasif MySQL kurulumu, Pasif sunucu Aktif 3 PMNGLOG01 Prod 192.168.104.50 4 16 50 + 2000 Log sunucusu, Mongo DB Aktif 4 TMYSQLPOB02 Beta 192.168.99.139 2 16 300 MySQL Enterprise Test Sunucusu, Master Aktif,, Eylül 2021 5 TMYSQLPOB03 Beta 192.168.99.140 2 16 300 MySQL Enterprise Test Sunucusu, Slave Aktif, Eylül 2021 6 MYSQLPOB01 Beta 192.168.99.128 4 8 100 Beta ortamı MySQL sunucusu Kapatıldı 30.11.2020 Pob Projesi Sentinel Mikro Servisleri PoB projesinde kullanılan altyapıda ye alan mikro servisler. Sentinel uygulamasından alınmıştır. Ek Bilgiler S2SVPN tünel içerisinden 192.168.45.248 source IP’si ile 192.168.45.251  ( servicegatewayvpn.isbank.com.tr ) 7517 port  ile erişim sağlar. Pob uygulamasının external IP'si : 90.159.29.29 / 90.159.29.30image2020-11-29_17-35-51.png","{'title': 'PoB Projesi', 'id': '18911920', 'source': 'https://wiki.softtech.com.tr/display/SDO/PoB+Projesi'}"
"Confluence kurulumuna ilişkin bilgiler, Sunucu IP Portlar Açıklama sbpjrdb01 10.222.18.12 Out: 443, 25 Confluence web sunucusu. SBUCFPDB01 10.222.8.82 Confluence DB sunucusu. Tanımlanan Network Kuralları Kaynak IP(ler) Hedef IP(ler) Portlar 10.222.18.12 10.222.8.33, 10.222.8.34 LDAPS (636) 10.222.18.12 10.222.8.36 SMTP (25) 10.222.18.12 10.222.8.82 Postgres (5432) 10.222.18.12 10.222.8.90 443, 80 10.222.8.76, 10.222.8.80, 10.222.8.86 10.222.18.12 SSH (22) 10.222.8.0/24 10.222.18.12 443, 80, 8090, 8091 10.222.8.96 destination: 10.222.18.11 port:tcp/22 erişimi tanımlıdır.","{'title': 'Wiki Softtech - Confluence', 'id': '18912161', 'source': 'https://wiki.softtech.com.tr/display/SDO/Wiki+Softtech+-+Confluence'}"
Arşive ayrılmış dokümanlardır.,"{'title': 'Anthos Arşiv', 'id': '18912163', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18912163'}"
Anthos ortam ve bu ortamda host edilen proje bilgileri bilgileri bu alanda paylaşılacaktır.,"{'title': 'Anthos', 'id': '18912165', 'source': 'https://wiki.softtech.com.tr/display/SDO/Anthos'}"
,"{'title': 'Anthos Altyapısı', 'id': '18912170', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=18912170'}"
"Anthos ortamında bulunan sunucular burada listelenmiştir, 3 Prod Ortam Sunucuları No Sunucu Adı IP vCPU RAM (GB) Disk (GB) Açıklama 1 PMYSQLPOB11 192.168.7.129 4 32 100 + 300 PoB Projesi MySQL sunucuları 2 PMYSQLPOB12 192.168.7.130 4 32 100 + 300 PoB Projesi MySQL sunucuları 3 PMSSQL011 192.168.7.131 2 16 50 + 200 Meetin, 2pm MSSQL sunucusu 4 PNFS01 192.168.7.200 2 8 50 + 2000 Prod ortamı NFS sunucusu 5 PUMINIO02 192.168.7.134 2 16 50 + 300 Minio Sunucusu Beta Ortam Sunucuları No Sunucu Adı IP vCPU RAM (GB) Disk (GB) Açıklama 1 MYSQLPOB01 192.168.99.128 4 8 100 PoB Projesi MySQL sunucusu 2 TMYSQLEDF01 192.168.99.129 1 16 100 Edefter Projesi MySQL sunucusu 3 TMYSQLDO011 192.168.99.130 2 4 100 Onboardng Projesi MySQL sunucusu 4 TUBJITSI011 192.168.99.131 2 2 50 Onboarding Jitsi sunucusu 5 TUBJIBRI011 192.168.99.132 2 2 50 Onboarding Jibri sunucusu 6 BNFS01 192.168.99.200 2 8 50 + 2000 Beta ortamı NFS sunucusu 7 TUBSEC01 192.168.99.201 4 16 100 Beta ortamları Security uygulaması, Ubuntu 20,04 LTS Management Ortam Sunucuları No Sunucu Adı IP vCPU RAM (GB) Disk (GB) Açıklama 1 SPPOB01 192.168.104.224 4 PoB Projesi MySQL sunucusu 2 STWD01 192.168.104.225 1 3 PUJNKNS01 192.168.104.226 4 8 200 Jenkins Server 4 5","{'title': 'Anthos Ortamındaki Sunucular', 'id': '24117302', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=24117302'}"
"3 Digital Onboarding Projesinde Kullanılan Sunucular Digital Onboaring projesi kapsamında Anthos Kubernetes Ortamına ek olarak aşağıdaki sunucular kullanılmaktadır, No Sunucu Ad Ortam IP vCPU RAM (GB) Disk  (GB) Açıklama 1 PUJITSI01 Prod Extranet 192.168.105.10 2 4 50 Jitsi Sunucusu 2 PUJIBRI01 Prod Extranet 192.168.105.11 3 2 32 Jibri sunucusu 3 PUJIBRI02 Prod Extranet 192.168.105.12 3 2 32 Jibri sunucusu 4 PUJIBRI03 Prod Extranet 192.168.105.13 3 2 32 Jibri sunucusu 5 PUJIBRI04 Prod Extranet 192.168.105.14 3 2 32 Jibri sunucusu 6 PUJIBRI05 Prod Extranet 192.168.105.15 3 2 32 Jibri sunucusu 7 PUJIBRI06 Prod Extranet 192.168.105.16 3 2 32 Jibri sunucusu 8 PUJIBRI07 Prod Extranet 192.168.105.17 3 2 32 Jibri sunucusu 9 PUJIBRI08 Prod Extranet 192.168.105.18 3 2 32 Jibri sunucusu 10 PUJIBRI09 Prod Extranet 192.168.105.19 3 2 32 Jibri sunucusu 11 PUJIBRI010 Prod Extranet 192.168.105.20 3 2 32 Jibri sunucusu 12 PUMINIO01 Prod Extranet 192.168.105.9 2 16 50 + 1800 Minio sunucusu, SATA storage. Ekipten gelen talep üzerine 6 Ocak 2021 tarihinde işlemci sayısı 4'ten 2'ye düşürülmüştür. 13 PUFRS01 Prod Extranet 192.168.105.21 2 4 50 Yüz Algılama uygulaması sunucusu Prod Extranet, 3 Kasım 2021 14 PMYSQLOB01 Prod 192.168.7.131 4 32 50 + 200 MySQL sunucusu, Prod ortamı 15 TMYSQLDO011 Beta 192.168.99.130 4 32 100 MySQL sunucusu, Beta ortamı 16 TUBJITSI011 Beta 192.168.99.131 2 4 50 Jitsi sunucusu, Beta ortamı 17 TUBJIBRI011 Beta 192.168.99.132 2 4 50 Jibri sunucusu, Beta ortamı 18 TUUJITSI01 Beta 192.168.99.133 2 2 32 Jitsi UAT sunucusu, UAT Ortamı, 5 Şubat 2021 19 TUUJIBRI01 Beta 192.168.99.134 2 2 32 Jibri UAT sunucusu, UAT Ortamı, 5 Şubat 2021 20 TUBFRS01 Beta 192.168.99.135 2 4 40 Yüz Algılama uygulaması sunucusu, 19 Nisan 2021 21 TUDTS01 Beta 192.168.99.136 8 32 100 DoB uyglamasına Docker Beta kurulumu 22 TUDTS02 Beta 192.168.99.137 8 24 100 DoB uygulaması MicroK8S üstüne kurulumu 23 Network Tanımları Firewall Kuralları Kayank IP Hedef IP Port 192.168.105.0/24 192.168.0.0/20 443, 80 192.168.105.0/24 İnternet 443, 80 192.168.0.0/20 192.168.105.0/24 443, 80 192.168.105.0/24 İlgili adreslere NTP 192.168.105.0/24 192.168.104.10, 192.168.104.11 DNS 192.168.104.0/24 192.168.105.0/24 SSH, HTTPS, HTTP VPN 192.168.105.0/24 İlgili portlar Dış Ip Tanımları Dış Ip İç IP Açılan Portlar Açıklama 212.98.16.6 192.168.105.10 TCP: 80, 443, 8443, 8000 UDP: 10000-20000 Jitsi sunucusuna servisler kapsamında yapılan erişim tanımlarıdır. 212.98.16. 192.168.99.131 İş Yatırım Site to Site VPN ve Erişilen Servisler https://tapi.isyatirim.com.tr/Onboarding/v1/tr/checkcustomer/tckn https://tapi.isyatirim.com.tr/Onboarding/v1/tr/sendsms https://tapi.isyatirim.com.tr/Auth/v1/tr/appLogin Site to Site VPN Üzerinden Erişim Yetkisi Verilen IPler Ortam Liste Prod 192.168.0.128 192.168.0.129 192.168.0.130 192.168.0.131 192.168.0.132 192.168.0.133 192.168.0.134 192.168.0.135 192.168.0.136 192.168.0.137 192.168.0.138 192.168.0.139 192.168.8.65 192.168.7.132 Beta 192.168.96.128 192.168.96.129 192.168.96.130 192.168.96.131 192.168.96.132 192.168.96.133 192.168.100.65 Management 192.168.104.41 192.168.0.5 Minio ve Portainer Kurulumu 192.168.105.9 sunucusu üzerine docker ile portainer ve minio kurulumu gerçekleşti minio 9000 portunu portainer 8000 ve 8001 (web arayüzü) portunu kullanıyor. Kurulum için kullanılan diskler: Minio: /data (/dev/sdb1 mounted) Portainer: /opt/portainer Kurulumda kullanılan scriptler: docker run -d -p 8000:8000 -p 8001:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /opt/portainer:/data portainer/portainer-ce docker run -d -p 9000:9000 --name minio --restart always -v /data:/data -e ""MINIO_ACCESS_KEY=***"" -e ""MINIO_SECRET_KEY=***"" minio/minio server /data NFS Server Kurulumu NFS servisinin çalışacağı Ubuntu sunucularda NFS kurulumu için aşağıdaki adımlar izlenir, apt update apt install nfs-kernel-server nfs-common portmap mkdir -p /srv/nfs/nfsdata chown nobody:nogroup /srv/nfs/nfsdata systemctl status nfs-server etc/export dosyası edit edilir. nano /etc/exports /srv/nfs/nfsdata master(rw,sync,no_root_squash,no_subtree_check) /srv/nfs/nfsdata worker1(rw,sync,no_root_squash,no_subtree_check) /srv/nfs/nfsdata worker2(rw,sync,no_root_squash,no_subtree_check) systemctl restart nfs-kernel-server NFS İstemci Ayarları NFS'e bağlanacak Worker node sunucularda aşğıdaki adımlar izlenir, apt install -y nfs-common mkdir -p /mnt/nfsdata mount -t nfs 10.10.10. 132:/srv/nfs/nfsdata /mnt/nfsdata Sunucu restart olduktan sonra da NFS bağlantısının çalışamsı için fstab dosyasına ekleme yapılır. nano /etc/fstab nfs:/srv/nfs/nfsdata /mnt/nfsdata nfs4 rw,sync 0 0 mount | grep nfs","{'title': 'Digital Onboarding Projesi', 'id': '24117478', 'source': 'https://wiki.softtech.com.tr/display/SDO/Digital+Onboarding+Projesi'}"
"Kubernetes Infrastructure LoadBalancer NFS Master1 Master2 Master3 Worker1 Worker2 Worker3 Worker Infra1 Worker Infra2 Bütün Makinaların Hostname Tanımlarının Yapılması Bütün makinaların birbirine hostname ile erişebilmesi içi tüm makinalarda hosts dosyasına hostaname tanımı yapılır. nano /etc/hosts LoadBalancer Kurulumu LoadBalancer uygulaması olan haproxy makinaya apt-get ile yüklenir. Master Node'ları Yönetmek için HaProxy Master IP'leri tanımlama Haproxy config dosyası /etc/haproxy/haproxy.cfg'dir. Burada Kubernetes Cluster'ımızda 3 master kullanacağımız için haproxy üzerinde 3 master node'umuzu tanımlıyoruz. yml Configler yapıldıktan sonra ; Configleri yaptıktan sonra haproxy configleri aktif olması için restart ediyoruz. Bütün Kubernetes Node'ların Yapılması Gerekenler (master1, master2, master3,worker1,worker2,worker3,workerinfra1,workerinfra2) Firewall Kapatma Kubernetes portlarının ve docker portlarının herhangi bir firewall ayarına takılmaması için ubuntu firewall'ını kapatıyoruz. Swap Kapatma Kubernetes swap kullanımına izin vermediği için swap'i kapatıyoruz.  Ayrı bir sebep olarak Swap'in açık olması memory'i gereksiz olarak yorar. Kubernetes Ağ İletişimi için sysctl Ayarlarının Güncellenmesi >/etc/sysctl.d/kubernetes.conf< Docker Kurulumu Kubernetes Kurulumu Kubernetes Apt Repository Ekleme /etc/apt/sources.list.d/kubernetes.list

}]]> Kubernetes Component Yükleme Master1  Node'unda  Çalıştırılacak Komutlar Kubectl Komutunu Kullanmak İçin Çalıştırılacak Komut; WeaveNet CNI Plugin Yükleme Master Node'ları Dahil Etmek için ; Worker Node'ları Dahil Etmek için; Kubernetes'in Düzgün Çalıştığını Kontrol Etmek İçin; NFS Kurulumu NFS Bound Etmek İçin Bütün Node'lar da Çalıştırılması Gereken Komutlar Kubernetes Ortamında  NFS Provisoner Kurulumu Aşağıda paylaşılan yaml'larda sadece Provisoner.yaml düzeltilecek ve nfs server ip'leri girilecek. Storage-class.yaml Service-account.yaml Provisoner.yaml Kubernetes Default Storage Class olarak Tanımlama Burada StorageClass'da verdiğimiz ismi kullanıyoruz( nfs-storageclass) Helm Kurulumu Helm ile KubeApps Kurulumu Service Account Oluşturmak İçinss.png","{'title': 'Kubernetes On-Premises Kurulumu', 'id': '24117822', 'source': 'https://wiki.softtech.com.tr/display/SDO/Kubernetes+On-Premises+Kurulumu'}"
"Flow Sunucuları Sunucu Adı İşletim Sistemi İşlemci Bellek Disk IP Açıklama STFLWS01 Ubuntu 18.04 LTE 16 64 200 GB + 2000 GB 10.222.8.122 Flow Uygulama Sunucusu STFLWDB01 Ubuntu 18.04 LTE 16 64 50 GB + 500 GB 10.222.8.123 Flow DB Sunucusu Dış IP adresleri: 90.158.30.4 Flow Sunucuları Firewall Tanımları Kaynak IP Hedef IP Portlar DNS Açıklama 10.222.8.122 10.80.36.92 HTTP, HTTPS tfs.softtech TFS Softtech erişimi 10.222.8.122 10.80.36.102 HTTP, HTTPS kozmos.softtech Kozmos TFS sunucu erişimi 10.222.8.122 10.80.20.141 HTTP, HTTPS scoretfs.isbank Score TFS sunucu erişimi Flow DB Kurulum Adımları Docker engine kuruldu 500 GB boyutunda /dev/sdb diski /data/postgre altına bağlandı Aşağıdaki docker komutlarıyla gerekli sunucu ve bileşenler kuruldu docker run --name postgre-flow -d --restart always -p 5432:5432 -v /data/ postgre:/var/lib/postgresql/data -e PGDATA=/var/lib/postgresql/data/pgdata -e POSTGRES_USER=flow -e POSTGRES_PASSWORD=*** postgres docker run -d --name adminer -p 8080:8080 --restart always adminer docker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker. sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce Flow Uygulama Kurulum Adımları","{'title': 'Flow Uygulama Kurulumu', 'id': '27332074', 'source': 'https://wiki.softtech.com.tr/display/SDO/Flow+Uygulama+Kurulumu'}"
,"{'title': 'Plateau Kurulumları', 'id': '27332208', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=27332208'}"
"Dijital Onboarding uygulamasının OnPrem kurulumları Kubernetes, MySQL, Object Storage, Jitsi ve Jibri kurulumlarını gerektirmektedir. Sistem Gereksinimleri 3 2 Kubernetes Dijital Onbaording Uygulaması Vanilla Kubernetes ortamında çalışmaktadır. Vanilla Kubernetes versiyonlarından 1.19.3 - 1.19.7 versiyonlardan uygun olan çalışma ortamında kurulu olmalıdır. Kubernetes ortamında Ingress Gateway kurulu ve kullanılabilir durumda olmalıdır. Kubernetes destekli kurulum yapılacak sunucu üzerinde en az 5 vCPU, 16 GB bellek ve 0.5 TB kadar disk alanı bulunmalıdır. Kubernetes dışındaki seçeneklerde (Linux sunucularda servis olarak ya da Windows sunucular üzerinde Apache Tomcat gibi) gereksinimlerin müşteri ile yeniden değerlendirilmesi uygun olacaktır. Elastic Search, Filebeat Kibana Kubernetes üzerindeki uygulama loglarının takibi için uyumlu Elastic Search, Kibana ve Filebeat bileşenleri kurulu olmalı ve Kubernetes'e entegre edilmelidir. Elastic Search ve Kibanaya ilişkin bilgilere sitesinden erişebilirsiniz. Grafana ve Prometheus Kubernetes platformunda çalışan podların ve genel olarak sistemin performans durumunu takip etmek için Grafana ve Prometheus kurulup Kubernetes'e entegre edilmelidir. MySQL Tercih ettiğiniz Linux işletim sistemi üzerinde MySQL 8.0.11 - MySQL 8.0.22 sürümlerinden birisi kurulu olmalıdır. Yüksek erişebilirlik ihtiyacına bağlı olarak Aktif - Pasif, Akitf - Aktif modda kurulu MySQL kurulumları da kullanılabilir. Kurulum yapılacak sunucular üzerinde en az 4 vCPU işlemci, 16 GB bellek ve 300 GB DB disk alanı bulunmalıdır. Ortamınıza uygun MySQL 8 kurulumunu sitesinde yer alan adımları takip ederek yapabilirsiniz. Object Storage - Minio Minio kurulumu için Ubuntu 18.04 LTS sürümü kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 4 vCPU, 16 GB bellek ve 2 TB kadar disk alanı bulunmalıdır. Ortamınıza uygun Minio sürümü kurulumunu sitesinden yapabilirsiniz. Yüz Tanıma Uygulaması Yüz tanıma servisi için Ubuntu 18.04 LTS üzeri kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 2 vCPU, 4 GB bellek ve 40GB kadar disk alanı bulunmalıdır. Not: Kurulum detayları ayrıca iletilecektir. Dijital Onboarding Uygulamasının Kurulumu Kubernetes ortamı üzerinde onboarding uygulamasının kurulacağı bir namespace oluşturulur. bash Oluşturulan namespace'in container registiry'den image çekebilmesi için gerekli olan secret namespace'e eklenir. bash Bu namespace içinde oluşturulan tüm deployment'ların öntanımlı image pull secretler'ının oluşturduğumuz secret olması için namespace'e bir patch yapılması gerekmektedir. bash Onboarding uygulamasının plateau helm chart'ı ile kurulması için ihtiyaç duyulan 3 dosya hazırlanmalıdır. Bunlar: credentials.yaml: Uygulama servislerinin git reposu ve database'lere erişiminde kullanacağı parolalar bulunur. database.yaml: Database adresi portu ve teknoloji bilgisi yer alır. product-values.yaml: Kullanılacak servislerin versiyonları ve ek ayarlarını içerir. Bu dosyaların yanı sıra bir git reposunda her servis için konfigürasyon dosyaları bulunmalıdır. Yukarıdaki hazırlıklar tamamlandığında uyguluma tek komut ile ayağa kalkacaktır. bash Uygulama tek komutla kaldırılabilir. bash Jitsi Sunucu Kurulumu Jitsi sunucusunda en az 4 vCPU işlemci, 16 GB bellek, 200 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Sistem Güncellemesi Kontrolü Sistemin güncel olduğu kontrol edilir bash Kullanılacak Alan Adı ve DNS Ayarları DNS alan adı Jitsi hizmet noktanızı tanımlar. bash Jitsi Repositorysinin Eklenmesi ve Jitsi Kurulumu Jitsi kurulumu için repolar sisteme tanıtılmalıdır. Sonrasında kurulum gerçekleştirilir. bash > /etc/apt/sources.list.d/jitsi-stable.list
wget -qO - https://download.jitsi.org/jitsi-key.gpg.key | sudo apt-key add –

apt update && apt install jitsi-meet -y ]]> Kurulum esnasında SSL/TLS sertifika seçenekleri sorulacaktır. Test ortamı kurulumları için Lets Encrypt sertifikaları tercih edilebilir. PROD ortamları için ise kurumsal sertifika sağlayıcınızdan gelen sertifikalar kullanılmalıdır. Encrypt seçeneği seçerek ilerlenir. Sertifikayı daha sonra değiştireceğiz. Yükleme bittikten sonra aşağıdaki script çalıştırılır bash Jitsi Üzerinde Performans Ayarları Prosody Port Manager Hatasının Düzeltilmesi Portmanager hataları için aşağıdaki ayarlar yapılır. bash Genel Performansı İyileştirmek Sistemin ses ve video ayarları yapılarak daha performanslı çalışması sağlanır. bash Jitsi Sunucusunun Jibriler için Hazırlanması Jibri sunucu kurulumlarına başlamadan önce mevcut ortamın Jibri sunucuları ve servisleri için hazırlanması gerekmektedir. Jitsi sunucusunda çalışan Java versiyon kontrol edilir. bash VMde IBM gibi farklı bir JDK varsa (GCP VM IBM jdk ile birlikte gelir) OpenJDK'yi değiştirmemiz gerekir. bash Jitsi Sunucusunun MUC Ayarları Aşağıdaki ayarları Jitsi ve Jibriler üzerinde MUC (Multi User Chat) bileşenin düzgün çalışması için yapılmalıdır. bash Jitsi Üzerinde Jibri ve Recorder Kullanıcılarını Oluşturma Jitsi sunucusu üzerinde jibri ve recorder kullanıcıları oluşturulur. Birden fazla Jibri sunucusu kullanılacak ise her biri için ayrı jibri ve recorder kullanıcıları ve gerekli ayarları eklenir. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= # Kayıt ayarlarının yapulması
vi /etc/jitsi/meet/meet.firmaadi.abc-config.js
fileRecordingsEnabled: true,
hiddenDomain: 'recorder.meet.firmaadi.abc', ]]> Yapılandırmadan sonra tüm jitsi servisleri yeniden başlatılır. bash Kurumsal Sertifikanın Yüklenmesi Sertifika değiştirilmesi için full chain sertifikasını ve anahtarı uygun yere kaydedilir, aşağdaki scriptle ilgili path verilir. bash Jitsi kurulumunda karşılaşılan sorunlar için GitHub ve Doküman sayfalarına başvurlabilir. Jitsi Sunucusunda NAT Yapılandırılması Jitsi sunucunuz bir NAT (Firewall)'un arkasında ise bu durumda sip-communicator.properties dosyasında aşağıdaki eklentiler yapılır. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= ]]> Jibri Sunucu Kurulumu Kuruluacak olan her bir Jibri sunucusunda en az 3 vCPU işlemci, 4 GB bellek, 50 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Diğer işletim sistemlerinde kurulum için Jibri sitesi takip edilmelidir. Sunucuda Güncelleme Yapılır bash Jibri Sunucusnda Host Kayıtları Jibri sunucusunda host kayıtları alan adı planalamanıza bağlı olarak güncellenir. bash Jibri Sunucularında Java Sürümünün Yüklenmesi Jibri sunucularında doğru java sürümünün yüklenmesi için aşağıdaki adımlar izlenir, bash > ~/.bash_profile
source ~/.bash_profile]]> ALSA Loopback Modülünün Yüklenmesi ALSA Loopback modülü sanal bir ses sürücüsüdür. bash > /etc/modules

# cihaza erişim test edilir
modprobe snd-aloop

# snd-allop modülünün yüklenip yüklenmediğini test etme
lsmod | grep snd-aloop]]> Jibri Sunucusunda Google Chrome Kurulumu Google Chrome Kurulumu bash /etc/apt/sources.list.d/google-chrome.list

# google chrome un yüklenmesi
apt update && apt install google-chrome-stable -y]]> Google Chrome Üzerinde Uyarıların Kapatılması bash >/etc/opt/chrome/policies/managed/managed_policies.json]]> Chrome Driver Kurulumu bash Jibrinin Kurulması Jibrinin kurulması için gereken anahtarlar ve kütüphane linkleri eklenir ve kurulum yapılır. bash /etc/apt/sources.list.d/jitsi-stable.list""

apt update && apt install jibri -y]]> Jibri Kullanıcısının Gerekli Gruplara Eklenmesi bash Kayıtların Saklanması Kayıtların saklanması için dizin tanımlanıp üzerinde izinler düzenlenir. bash Jibri Ayarlarının Yapılması Jibri sunucusuna ssh ile bağlandıktan sonra ilgil conf dosyasında Jitsi sununcusuna ilişkin baplantı ayarları ve diğer ortam ayarları yapılır. bash Config dosyasyonda sizin ortamınıza uygun ayarları yapılır.{ jfx Confluence Güncellemeler bittikten sonra jibri servisi yeniden başlatılır. bashimage2021-1-26_16-34-30.png","{'title': 'Softtech Dijital Onboarding Uygulaması OnPrem Kurulumu', 'id': '27332210', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=27332210'}"
"Jitsi conf: org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= ]]> jibri: vi /etc/hosts apt update && apt install wget gnupg software-properties-common -y wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add - add-apt-repository --yes [ https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/|https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/ ] apt update && apt install adoptopenjdk-8-hotspot -y java -version update-alternatives --config 'java' echo export JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java >> ~/.bash_profile source ~/.bash_profile apt update && apt install unzip ffmpeg curl alsa-utils icewm xdotool xserver-xorg-input-void xserver-xorg-video-dummy -y echo ""snd-aloop"" >> /etc/modules modprobe snd-aloop lsmod | grep snd_aloop curl -sS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add echo ""deb \[arch=amd64\] http://dl.google.com/linux/chrome/deb/ stable main"" > /etc/apt/sources.list.d/google-chrome.list apt update && apt install google-chrome-stable -y mkdir -p /etc/opt/chrome/policies/managed echo '\{ ""CommandLineFlagSecurityWarningsEnabled"": false \}' >>/etc/opt/chrome/policies/managed/managed_policies.json CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE ` wget -N http://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P ~/ unzip ~/chromedriver_linux64.zip -d ~/ rm ~/chromedriver_linux64.zip mv -f ~/chromedriver /usr/local/bin/chromedriver chown root:root /usr/local/bin/chromedriver chmod 0755 /usr/local/bin/chromedriver wget -qO - https://download.jitsi.org/jitsi-key.gpg.key | sudo apt-key add - sh -c ""echo 'deb https://download.jitsi.org stable/' > /etc/apt/sources.list.d/jitsi-stable.list"" apt update && apt install jibri -y usermod -aG adm,audio,video,plugdev jibri mkdir /srv/recordings chown jibri:jitsi /srv/recordings vi /etc/jitsi/jibri/config.json <span style=""color: #c814c9"">{</span> <span style=""color: #dfdfdf"">// NOTE: this is a <strong>SAMPLE</strong> config file, it will need to be configured with</span> <span style=""color: #dfdfdf"">// values from your environment</span> <span style=""color: #dfdfdf"">// Where recording files should be temporarily stored</span> ""<span style=""color: #c1651c"">recording_directory</span>"":""<span style=""color: #b42419"">/srv/recordings</span>"", <span style=""color: #dfdfdf"">// The path to the script which will be run on completed recordings</span> ""<span style=""color: #c1651c"">finalize_recording_script_path</span>"": ""<span style=""color: #b42419"">/path/to/finalize_recording.sh</span>"", ""<span style=""color: #c1651c"">xmpp_environments</span>"": <span style=""color: #c814c9"">[</span> <span style=""color: #c814c9"">{</span> <span style=""color: #dfdfdf"">// A friendly name for this environment which can be used</span> <span style=""color: #dfdfdf"">// for logging, stats, etc.</span> ""<span style=""color: #c1651c"">name</span>"": ""<span style=""color: #b42419"">prod environment</span>"", <span style=""color: #dfdfdf"">// The hosts of the XMPP servers to connect to as part of</span> <span style=""color: #dfdfdf"">// this environment</span> ""<span style=""color: #c1651c"">xmpp_server_hosts</span>"": <span style=""color: #c814c9"">[</span> ""<span style=""color: #b42419""> meet.onplateau.com </span>"" <span style=""color: #c814c9"">]</span>, <span style=""color: #dfdfdf"">// The xmpp domain we'll connect to on the XMPP server</span> ""<span style=""color: #c1651c"">xmpp_domain</span>"": ""<span style=""color: #b42419""> meet.onplateau.com </span>"", <span style=""color: #dfdfdf"">// Jibri will login to the xmpp server as a privileged user</span> ""<span style=""color: #c1651c"">control_login</span>"": <span style=""color: #c814c9"">{</span> <span style=""color: #dfdfdf"">// The domain to use for logging in</span> ""<span style=""color: #c1651c"">domain</span>"": ""<span style=""color: #b42419""> auth.meet.onplateau.com </span>"", <span style=""color: #dfdfdf"">// The credentials for logging in</span> ""<span style=""color: #c1651c"">username</span>"": ""<span style=""color: #b42419"">jibri</span>"", ""<span style=""color: #c1651c"">password</span>"": ""<span style=""color: #b42419"">JibrisPass</span>"" <span style=""color: #c814c9"">}</span>, <span style=""color: #dfdfdf"">// Using the control_login information above, Jibri will join</span> <span style=""color: #dfdfdf"">// a control muc as a means of announcing its availability</span> <span style=""color: #dfdfdf"">// to provide services for a given environment</span> ""<span style=""color: #c1651c"">control_muc</span>"": <span style=""color: #c814c9"">{</span> ""<span style=""color: #c1651c"">domain</span>"": ""<span style=""color: #b42419""> internal.auth.meet.onplateau.com </span>"", ""<span style=""color: #c1651c"">room_name</span>"": ""<span style=""color: #b42419"">JibriBrewery</span>"", ""<span style=""color: #c1651c"">nickname</span>"": ""<span style=""color: #b42419"">jibri-nickname</span>"" <span style=""color: #c814c9"">}</span>, <span style=""color: #dfdfdf"">// All participants in a call join a muc so they can exchange</span> <span style=""color: #dfdfdf"">// information. Jibri can be instructed to join a special muc</span> <span style=""color: #dfdfdf"">// with credentials to give it special abilities (e.g. not being</span> <span style=""color: #dfdfdf"">// displayed to other users like a normal participant)</span> ""<span style=""color: #c1651c"">call_login</span>"": <span style=""color: #c814c9"">{</span> ""<span style=""color: #c1651c"">domain</span>"": ""<span style=""color: #b42419""> recorder.meet.onplateau.com </span>"", ""<span style=""color: #c1651c"">username</span>"": ""<span style=""color: #b42419"">recorder</span>"", ""<span style=""color: #c1651c"">password</span>"": ""<span style=""color: #b42419"">RecordersPass</span>"" <span style=""color: #c814c9"">}</span>, <span style=""color: #dfdfdf"">// When jibri gets a request to start a service for a room, the room</span> <span style=""color: #dfdfdf"">// jid will look like:</span> <span style=""color: #dfdfdf"">// roomName@optional.prefixes.subdomain.xmpp _domain</span> <span style=""color: #dfdfdf"">// We'll build the url for the call by transforming that into:</span> <span style=""color: #dfdfdf"">// <span class=""nobr""><a href="" https://xmpp_domain/subdomain/roomName "" class=""external-link"" rel=""nofollow""> https://xmpp_domain/subdomain/roomName<sup><img class=""rendericon"" src=""/images/icons/linkext7.gif"" height=""7"" width=""7"" align=""absmiddle"" alt="""" border=""0""/></sup></a></span></span> <span style=""color: #dfdfdf"">// So if there are any prefixes in the jid (like jitsi meet, which</span> <span style=""color: #dfdfdf"">// has its participants join a muc at conference.xmpp_domain) then</span> <span style=""color: #dfdfdf"">// list that prefix here so it can be stripped out to generate</span> <span style=""color: #dfdfdf"">// the call url correctly</span> ""<span style=""color: #c1651c"">room_jid_domain_string_to_strip_from_start</span>"": ""<span style=""color: #b42419"">conference.</span>"", <span style=""color: #dfdfdf"">// The amount of time, in minutes, a service is allowed to continue.</span> <span style=""color: #dfdfdf"">// Once a service has been running for this long, it will be</span> <span style=""color: #dfdfdf"">// stopped (cleanly). A value of 0 means an indefinite amount</span> <span style=""color: #dfdfdf"">// of time is allowed</span> ""<span style=""color: #c1651c"">usage_timeout</span>"": ""<span style=""color: #b42419"">0</span>"" <span style=""color: #c814c9"">}</span> <span style=""color: #c814c9"">]</span> <span style=""color: #c814c9"">}</span> service jibri restart","{'title': 'jibri-install', 'id': '29196599', 'source': 'https://wiki.softtech.com.tr/display/SDO/jibri-install'}"
"_GoBack root user ile login olunur. > /etc/apt/sources.list.d/jitsi-stable.list
wget -qO - https://download.jitsi.org/jitsi-key.gpg.key | sudo apt-key add –
apt update && apt install jitsi-meet -y ]]> Encrypt seçeneği seçerek ilerlenir. Sertifikayı daha sonra değiştireceğiz. Yükleme bittikten sonra aşağıdaki script çalıştırılır Komut dosyası geçerli bir e-posta adresi girilir. Java versiyon kontrol edilir VMde IBM gibi farklı bir JDK varsa (GCP VM IBM jdk ile birlikte gelir) OpenJDK'yi değiştirmemiz gerekir. Aşağıdaki komut çalıştırılır jibri ve recorder kullanıcısı oluşturmamız gerekiyor. Birden fazla jibri'ye sahip olacaksak, yeni jibri ve recorder kullanıcıları eklemeliyiz org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= vi /etc/jitsi/meet/ meet.onplateau.com-config.js
fileRecordingsEnabled: true,
hiddenDomain: 'recorder. meet.onplateau.com', ]]> Yapılandırmayı bitirdikten sonra tüm hizmetleri yeniden başlatmanız gerekir Sertifika değiştirme: full chain sertifikasını ve anahtarı uygun yere kaydedilir, aşağdaki scriptle ilgili path verilir.","{'title': 'jitsi_install', 'id': '29196601', 'source': 'https://wiki.softtech.com.tr/display/SDO/jitsi_install'}"
Sunucu Bilgileri Sunucu Adı İşletim Sistemi Sunucu IPsi İşlemci Bellek Disk Açıklama STKLNSIQ01 Red Hat Enterprise Linux 8.3 (Ootpa) 10.80.36.45 16 (4 socket x 4 core) 32 Gb 1100 GB Kullanılan Portlar Port Tipi Kaynak IP Hedep IP Portlar Inbound * 10.80.36.45 8070 Inbound * 10.80.36.45 8071 Outbound 10.80.36.45 https://clm.sonatype.com 443,"{'title': 'Nexus IQ Server Kurulumu', 'id': '29885083', 'source': 'https://wiki.softtech.com.tr/display/SDO/Nexus+IQ+Server+Kurulumu'}"
,"{'title': 'Google Cloud Konfigürasyonları', 'id': '29885791', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29885791'}"
"Google Cloud üzerinde VPN konfigürasyonu aşağıdaki site üzerinden yapılabilir, https://console.cloud.google.com/hybrid/vpn Gelen sayfada VPN Setup Wiazrd çalıştırılır. Açılan sayfada Classic VPN işaretlenip Continue tıklanır. Açılan sayfada, Google Compute VPN ismi (Name) Hedef VPN'in IP Adresi (Remote peer IP addresses) GCP'de genel VPN kullanımı https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview GCP'de desteklenen IKE tip ve seviyeleri, https://cloud.google.com/network-connectivity/docs/vpn/concepts/supported-ike-ciphersS2SVPN05.PNGS2SVPN04.PNGS2SVPN03.PNGS2SVPN02.PNGS2SVPN01.PNG","{'title': 'Site-to-Site VPN Ayarları', 'id': '29885793', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29885793'}"
,"{'title': 'Genel Arşiv', 'id': '29885989', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29885989'}"
,"{'title': 'Projeler', 'id': '29887409', 'source': 'https://wiki.softtech.com.tr/display/SDO/Projeler'}"
Proje Künyesi Başlık Açıklama GCP Proje Adı softtech-demo-turib Proje Linki VPC Tannımları Başlık Açıklama VPC Adı softtech-demo-turib-vpc Subnet Adı subnet-10-223-16-0-24 Subnet Tanımı 10.223.16.0/24,"{'title': 'Türib Projesi', 'id': '29887411', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29887411'}"
,"{'title': 'GCP PoB', 'id': '29887419', 'source': 'https://wiki.softtech.com.tr/display/SDO/GCP+PoB'}"
"GCP üzerinde yapılan Site-to-Site VPN ayarları için kullanılan temel konfigürasyon ve paydaş bilgileri derlenmiştir, Kurum Kurumsal Kontakt İç Birimler Açıklama Anadolu Sigorta Hakan Diler Program Yöneticisi (+90) 850 744 02 74 hdiler@anadolusigorta.com.tr Yalçın Şanlı Sistem Mimarı (+90) 850 744 04 13 ysanli@anadolusigorta.com.tr PoB Celal Emre Koçak Emre.Kocak@softtech.com.tr 05334837173 İş Portföy İş Bankası İşbank Pasific ve Isbank İş Net Softtech Platform ve DevOps ekibi, Sinan Çayır, Hasan Çağlar İşNet'ten gitlab erişimleri İş Yatırım Murat Kaya KGF Şubat 2021 tarihinde silindi. GCP Site-to-Site VPN temel konfigürasyonları Kurum Remote GW IP Local GCP GW IP Protokol Karşı Taraf Networkleri Local Networkler (GCP) Ek Açıklamalar Anadolu Sigorta 193.202.18.2 35.222.157.121 IKEv2 10.1.48.0/24 10.1.37.0/24 10.223.2.0/23 10.223.8.0/23 anadolusigorta-site-to-site-vpn-tunnel-1 İş Portföy 213.161.152.133 35.192.84.151 IKEv2 172.30.240.21/32 10.200.120.14/32 185.57.245.93/32 10.254.18.0/24 10.10.249.35/32 185.96.112.119/32 10.223.2.0/23 10.52.0.0/14 10.61.0.0/17 10.223.8.0/23 10.70.0.0/15 is-portfoy-site-2-site-vpn-gateway1 İş Bankası Pasific İş Bankası İş Net İş Yatırım","{'title': 'Site-to-Site VPN Paydaşları', 'id': '29888791', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29888791'}"
"Private Catalog Üzerinden GKE Cluster Kurulumu Gcloud hesabınıza softtech mail'i ile giriş yapılır. Size oluşturulan projede işlemler yapılır. Sol üst köşedeki üç çizgiye tıklanır ve Navigasyon Paneli açılır. Sonrasında Private Catalog'a tıklanır. Cluster oluşturmak için Gke Cluster & Ingress Controller kartına tıklanır.Ardından gelen ekranda Deploy tuşuna basılır. Sırayla Deployment Name verilir. Oluşturulmak istenen Cluster'a isim verilir. Cluster içinde bulunan NodePool'a isim verilir. Enviroment ismi girilir ( dev,beta) Cluster'ın kaç node'dan oluşacağı girilir. Cluster AutoScaling kısmı değiştirilmez. Preview and Deploy Tuşuna basılır. Herhangi bir hata çıkmaması durumunda DEPLOY tuşuna basılarak deploy işlemi başlatılır. Deploy işlemi tamamlandığında aşağıdaki şekilde gözükecektir. Sol tarafta bulunan Navigasyon Paneline tıklayarak Kubernetes Engine 'e gidip Kubernetes'inizi kullanmaya başlayabilirsiniz. Private Catalog Üzerinden GKE Cluster'a Uygulama Kurulumu GKE Cluster kurulumu tamamlandıktan sonra Private Catalog'da bulunan uygulamalardan istediğinizi seçerek Cluster'ınıza deploy edebilirsiniz. Örnek Olarak ; Standalone Mysql kurulumu aşağıdaki gibi gerçekleştirilir. Private Catalog ekranı açılır. Kartlardan mysql seçilir. Deploy tuşuna basılır. Sırasıyla Deployment ismi verilir. Private Catalog üzerinden Kurulmuş olan GKE CLUSTER ismi verilir. Kubernetes Engine sekmesine tıklayarak alınabilir. Google Cloud Platform yazsının yanında bulunan proje ismi verilir. Oluşturulacak db ismi,kullanıcısı, kullanıcı parolası, root parolası belirlenir. DB'ye internet üzerinden erişilmek isteniyorsa NodePort yazan yere LoadBalancer yazılır.NodePort üzerinden sadece VPN üzerinden erişelebilir. Son olarak DB boyutu GB cinsinden belirlenir Preview and Deploy tuşuna basılır ve Preview işlemi tamanlandıktan sonra Deploy tuşuna basılır. Deploy işlemi tamamlandığında aşağıdaki şekilde gözükecektir. Uygulamanızı Kubernetes Engine üzerinden kullanabilirsiniz. Private Catalog Üzerinden GKE Cluster'dan Uygulama Silme İşlemi Sol tarafta bulunan Navigasyon Paneline tıklanır. Sırasıyla Private Catalog seçeği seçilir Açılan ekranda sol tarafta Terraform deployments'a tıklanır. Silinmek istenen deployment bulunur ve en sağda bulunan üst üste 3 noktaya basılır. Ardından Deprovison tuşuna basılır. Deprovisioned edilmiş Deployment , Delete tuşuna basılır ve silme işlemi tamamlanır. NOT : Private Catalog Üzerinden Kurulan GKE Cluster'ı Kaldırmak İçin Öncelikle Private Catalog Üzerinden Kurulan Uygulamalar Silinmelidir.worddavcf30cf437b7e90eefe874a0191279afa.pngworddav40a2031c422e678d07e5853d508d3cc0.pngworddav4af711dbc986f1fb5610a28788f81b07.pngworddaveb5837c5d24028fdf61d651fadfad027.pngworddav201521e3aa309c9551ddc0c3372bbf58.pngworddava15698e6dc202a3c61c44a4d8fc45395.pngworddav5f6c0e4ed2750a45cfb16de9d4db6dfb.pngworddav59b4c7defc7f206ae0f22337b71548f6.pngworddav2cbf08a7e4de312fc82bcbc6b94ac485.pngworddav6e8d226b772a7c750de2d80b971233f8.png","{'title': 'GCloud Private Catalog Kullanım Rehberi', 'id': '29889435', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29889435'}"
"Kurulum Adımları MSSQL Konfigürasyonu Keycloak Konfigürasyonu 1. MSSQL Kurulumu Oluşturulan Windows suncuusuna MSSQL kurulumu ardından Keycloak adında bir database oluşturulur. keycloakuser adında bir user oluşturulur. keycloakuser kullanıcısı Keycloak databese'ine db owner olarak eklenir. MSSQL TCP/IP erişimi aktif hale getirilir. 2. Keycloak Konfigürasyonu Keycloak'ın çalışması için gereken Java 8 JDK sisteme yüklenir. Örnek: OpenJDK şu bağlantı ile indirilebilir Redhat OpenJDK Database bağlantısı için gereken mssql-jdbc-8.2.2.jre8.jar linkten indirilir. plateu klasörü ve Deployment Jar dosyaları sağlanan repo üzerinden temin edilir. Keycloak (v11.0.3 ) Keycloak Download sayfasından zip olarak indilen Keycloak istenilen dizine çıkartılır. Örnek: C:\Keycloak\ Deployment Jar dosyaları <Keycloak Path>\standalone\deployments altına kopyalanır. Belirtilen klasör yapısı oluşturulur. <Keycloak Path>\modules\system\layers\keycloak\com\microsoft\main Bu klasör içine 2. adımda indirilen mssql-jdbc-8.2.2.jre8.jar dosyası koyulur. Aynı klasör içine module.xml adında bir dosya yaratılıp içine aşağıdakiler eklenir <?xml version=""1.0"" ?>
<module xmlns=""urn:jboss:module:3.4"" name=""com.microsoft"">
    <resources>
        <resource-root path=""mssql-jdbc-8.2.2.jre8.jar""/>
    </resources>
    <dependencies>
        <module name=""javax.api""/>
        <module name=""javax.transaction.api""/>
    </dependencies>
</module> <Keycloak Path>\modules altına plateau klasörü kopyalanır. standalone.xml dosyası şu şekilde düzenlenir <Keycloak Path>\standalone\configuration\standalone.xml <datasources>

    <!--   DÜZENLENECEK KISIM  -->
    <datasource jndi-name=""java:jboss/datasources/KeycloakDS"" pool-name=""KeycloakDS"" enabled=""true"" use-java-context=""true"">
    <connection-url>jdbc:sqlserver://[SUNUCU İSMİ]:1433;DatabaseName=[OLUŞTURULAN DATABASE];</connection-url>
        <driver>sqlserver</driver>
        <security>
            <user-name>[KEYCLOAK İÇİN OLUŞTURULAN USER]</user-name>
            <password>[KEYCLOAK İÇİN OLUŞTURULAN USER PAROLASI]</password>
        </security>
    </datasource>

    <drivers>
        <driver name=""h2"" module=""com.h2database.h2"">
            <xa-datasource-class>org.h2.jdbcx.JdbcDataSource</xa-datasource-class>
        </driver>

        <!--   EKLENECEK KISIM  -->
        <driver name=""sqlserver"" module=""com.microsoft"">
            <driver-class>com.microsoft.sqlserver.jdbc.SQLServerDriver</driver-class>
        </driver>
    </drivers>
</datasources>



<subsystem xmlns=""urn:jboss:domain:keycloak-server:1.1"">
    <!--   EKLENECEK KISIM  -->
    <providers>
        <provider>module:plateau.security.provider.notification-provider</provider>
    </providers>

    <!--   EKLENECEK KISIM  -->
    <spi name=""notification-spi"">
        <provider name=""default"" enabled=""true"">
            <properties>
                <property name=""url"" value=""http://notification.security.dev.rally.softtech""/>
            </properties>
        </provider>
    </spi> <Keycloak Path>\bin\standalone.bat çalıştırılır. Eğer hata alınmazsa Keycloak'a http://<sunucu adresi>:8080/auth adresinden erişebilirsiniz.","{'title': 'Keycloak Kurulumu (Windows Standalone)', 'id': '29889449', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29889449'}"
Softtech 2021 Baseline Ön Çalışması true,"{'title': '2021 Baseline', 'id': '29892287', 'source': 'https://wiki.softtech.com.tr/display/SDO/2021+Baseline'}"
"Dijital Onboarding uygulamasının OnPrem kurulumları Kubernetes, MySQL, Object Storage, Jitsi ve Jibri kurulumlarını gerektirmektedir. Sistem Gereksinimleri 3 2 Elastic Search, Kibana Elastic Search ve Kibanaya ilişkin bilgilere sitesinden erişebilirsiniz. Grafana ve Prometheus Platfarm üzerinde çalışan uygulamanın performans durumunu takip etmek için Grafana ve Prometheus kullanılabilir. MySQL Tercih ettiğiniz Linux işletim sistemi üzerinde MySQL 8.0.11 - MySQL 8.0.22 sürümlerinden birisi kurulu olmalıdır. Yüksek erişebilirlik ihtiyacına bağlı olarak Aktif - Pasif, Akitf - Aktif modda kurulu MySQL kurulumları da kullanılabilir. Kurulum yapılacak sunucular üzerinde en az 4 vCPU işlemci, 16 GB bellek ve 300 GB DB disk alanı bulunmalıdır. Ortamınıza uygun MySQL 8 kurulumunu sitesinde yer alan adımları takip ederek yapabilirsiniz. Object Storage - Minio Minio kurulumu için Ubuntu 20.04 LTS sürümü kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 4 vCPU, 16 GB bellek ve 2 TB kadar disk alanı bulunmalıdır. Ortamınıza uygun Minio sürümü kurulumunu sitesinden yapabilirsiniz. Yüz Tanıma Uygulaması Yüz tanıma servisi için Ubuntu 18.04 LTS üzeri kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 2 vCPU, 4 GB bellek ve 40GB kadar disk alanı bulunmalıdır. Not: Kurulum detayları ayrıca iletilecektir. Jitsi Sunucu Kurulumu Jitsi sunucusunda en az 4 vCPU işlemci, 16 GB bellek, 200 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Sistem Güncellemesi Kontrolü Sistemin güncel olduğu kontrol edilir bash Kullanılacak Alan Adı ve DNS Ayarları DNS alan adı Jitsi hizmet noktanızı tanımlar. bash Jitsi Repositorysinin Eklenmesi ve Jitsi Kurulumu Jitsi kurulumu için repolar sisteme tanıtılmalıdır. Sonrasında kurulum gerçekleştirilir. bash > /etc/apt/sources.list.d/jitsi-stable.list
wget -qO - https://download.jitsi.org/jitsi-key.gpg.key | sudo apt-key add –

apt update && apt install jitsi-meet -y ]]> Kurulum esnasında SSL/TLS sertifika seçenekleri sorulacaktır. Test ortamı kurulumları için Lets Encrypt sertifikaları tercih edilebilir. PROD ortamları için ise kurumsal sertifika sağlayıcınızdan gelen sertifikalar kullanılmalıdır. Encrypt seçeneği seçerek ilerlenir. Sertifikayı daha sonra değiştireceğiz. Yükleme bittikten sonra aşağıdaki script çalıştırılır bash Jitsi Üzerinde Performans Ayarları Prosody Port Manager Hatasının Düzeltilmesi Portmanager hataları için aşağıdaki ayarlar yapılır. bash Genel Performansı İyileştirmek Sistemin ses ve video ayarları yapılarak daha performanslı çalışması sağlanır. bash Jitsi Sunucusunun Jibriler için Hazırlanması Jibri sunucu kurulumlarına başlamadan önce mevcut ortamın Jibri sunucuları ve servisleri için hazırlanması gerekmektedir. Jitsi sunucusunda çalışan Java versiyon kontrol edilir. bash VMde IBM gibi farklı bir JDK varsa (GCP VM IBM jdk ile birlikte gelir) OpenJDK'yi değiştirmemiz gerekir. bash Jitsi Sunucusunun MUC Ayarları Aşağıdaki ayarları Jitsi ve Jibriler üzerinde MUC (Multi User Chat) bileşenin düzgün çalışması için yapılmalıdır. bash Jitsi Üzerinde Jibri ve Recorder Kullanıcılarını Oluşturma Jitsi sunucusu üzerinde jibri ve recorder kullanıcıları oluşturulur. Birden fazla Jibri sunucusu kullanılacak ise her biri için ayrı jibri ve recorder kullanıcıları ve gerekli ayarları eklenir. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= # Kayıt ayarlarının yapulması
vi /etc/jitsi/meet/meet.firmaadi.abc-config.js
fileRecordingsEnabled: true,
hiddenDomain: 'recorder.meet.firmaadi.abc', ]]> Yapılandırmadan sonra tüm jitsi servisleri yeniden başlatılır. bash Kurumsal Sertifikanın Yüklenmesi Sertifika değiştirilmesi için full chain sertifikasını ve anahtarı uygun yere kaydedilir, aşağdaki scriptle ilgili path verilir. bash Jitsi kurulumunda karşılaşılan sorunlar için GitHub ve Doküman sayfalarına başvurlabilir. Jitsi Sunucusunda NAT Yapılandırılması Jitsi sunucunuz bir NAT (Firewall)'un arkasında ise bu durumda sip-communicator.properties dosyasında aşağıdaki eklentiler yapılır. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= ]]> Jibri Sunucu Kurulumu Kuruluacak olan her bir Jibri sunucusunda en az 3 vCPU işlemci, 4 GB bellek, 50 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Diğer işletim sistemlerinde kurulum için Jibri sitesi takip edilmelidir. Sunucuda Güncelleme Yapılır bash Jibri Sunucusnda Host Kayıtları Jibri sunucusunda host kayıtları alan adı planalamanıza bağlı olarak güncellenir. bash Jibri Sunucularında Java Sürümünün Yüklenmesi Jibri sunucularında doğru java sürümünün yüklenmesi için aşağıdaki adımlar izlenir, bash > ~/.bash_profile
source ~/.bash_profile]]> ALSA Loopback Modülünün Yüklenmesi ALSA Loopback modülü sanal bir ses sürücüsüdür. bash > /etc/modules

# cihaza erişim test edilir
modprobe snd-aloop

# snd-allop modülünün yüklenip yüklenmediğini test etme
lsmod | grep snd-aloop]]> Jibri Sunucusunda Google Chrome Kurulumu Google Chrome Kurulumu bash /etc/apt/sources.list.d/google-chrome.list

# google chrome un yüklenmesi
apt update && apt install google-chrome-stable -y]]> Google Chrome Üzerinde Uyarıların Kapatılması bash >/etc/opt/chrome/policies/managed/managed_policies.json]]> Chrome Driver Kurulumu bash Jibrinin Kurulması Jibrinin kurulması için gereken anahtarlar ve kütüphane linkleri eklenir ve kurulum yapılır. bash /etc/apt/sources.list.d/jitsi-stable.list""

apt update && apt install jibri -y]]> Jibri Kullanıcısının Gerekli Gruplara Eklenmesi bash Kayıtların Saklanması Kayıtların saklanması için dizin tanımlanıp üzerinde izinler düzenlenir. bash Jibri Ayarlarının Yapılması Jibri sunucusuna ssh ile bağlandıktan sonra ilgil conf dosyasında Jitsi sununcusuna ilişkin baplantı ayarları ve diğer ortam ayarları yapılır. bash Config dosyasyonda sizin ortamınıza uygun ayarları yapılır.{ jfx Confluence Güncellemeler bittikten sonra jibri servisi yeniden başlatılır. bashimage2021-1-26_16-34-30.png","{'title': 'Softtech Dijital Onboarding OnPrem Kurulumu - Container', 'id': '29892610', 'source': 'https://wiki.softtech.com.tr/display/SDO/Softtech+Dijital+Onboarding+OnPrem+Kurulumu+-+Container'}"
"Dijital Onboarding uygulamasının OnPrem kurulumları Kubernetes, MySQL, Object Storage, Jitsi ve Jibri kurulumlarını gerektirmektedir. Sistem Gereksinimleri 3 2 Mikrok8s Dijital Onbaording Uygulaması Vanilla Kubernetes ortamında çalışmaktadır. Vanilla Kubernetes versiyonlarından 1.19.3 - 1.19.7 versiyonlardan uygun olan çalışma ortamında kurulu olmalıdır. Kubernetes ortamında Ingress Gateway kurulu ve kullanılabilir durumda olmalıdır. Kubernetes destekli kurulum yapılacak sunucu üzerinde en az 5 vCPU, 16 GB bellek ve 0.5 TB kadar disk alanı bulunmalıdır. Kubernetes dışındaki seçeneklerde (Linux sunucularda servis olarak ya da Windows sunucular üzerinde Apache Tomcat gibi) gereksinimlerin müşteri ile yeniden değerlendirilmesi uygun olacaktır. Mikrok8s Kurulumu Microk8s kurulumu ve ek bileşenleri için aşağıdaki komutların çalıştırılması gerekmektedir. bash Kubectl Kurulumu Kubernetes cluster'ını yönetmek için kubectl aşağıdaki komutlar ile kurulur. bash Microk8s conf dosyasının alınması Kubectl komutunun microk8s cluster'ı ile iletişimi için aşağıdaki komutlar çalıştırılır. bash config]]> Helm Kurulumu Digitalonboarding uygulamasını kurmak için gerekli olan Helm aşağıdaki komut ile kurulur. bash Elastic Search, Filebeat Kibana (Opsiyonel) Kubernetes üzerindeki uygulama loglarının takibi için uyumlu Elastic Search, Kibana ve Filebeat bileşenleri kurulu olmalı ve Kubernetes'e entegre edilmelidir. Elastic Search ve Kibanaya ilişkin bilgilere sitesinden erişebilirsiniz. Grafana ve Prometheus (Opsiyonel) Kubernetes platformunda çalışan podların ve genel olarak sistemin performans durumunu takip etmek için Grafana ve Prometheus kurulup Kubernetes'e entegre edilmelidir. MySQL Tercih ettiğiniz Linux işletim sistemi üzerinde MySQL 8.0.11 - MySQL 8.0.22 sürümlerinden birisi kurulu olmalıdır. Yüksek erişebilirlik ihtiyacına bağlı olarak Aktif - Pasif, Akitf - Aktif modda kurulu MySQL kurulumları da kullanılabilir. Kurulum yapılacak sunucular üzerinde en az 4 vCPU işlemci, 16 GB bellek ve 300 GB DB disk alanı bulunmalıdır. Ortamınıza uygun MySQL 8 kurulumunu sitesinde yer alan adımları takip ederek yapabilirsiniz. ""MySQL 8.0.11-22 error connect to caching_sha2_password the specified module could not be found"" Hatası init-db job'unun hata almadan çalışabilmesi için aşağıdaki adımlar uygulanır: /etc/mysql/my.cnf dosyasına aşağıdaki satır eklenir bash mysql servisi yeniden başlatılır Aşağıdaki SQL çalıştırılır sql ';
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY ' ';]]> Object Storage - Minio Minio kurulumu için Ubuntu 18.04 LTS sürümü kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 4 vCPU, 16 GB bellek ve 2 TB kadar disk alanı bulunmalıdır. Ortamınıza uygun Minio sürümü kurulumunu sitesinden yapabilirsiniz. Yüz Tanıma Uygulaması Yüz tanıma servisi için Ubuntu 18.04 LTS üzeri kullanılması tavsiye edilmektedir. Kurulum yapılacak sunucu üzerinde en az 2 vCPU, 4 GB bellek ve 40GB kadar disk alanı bulunmalıdır. Not: Kurulum detayları ayrıca iletilecektir. Dijital Onboarding Uygulamasının Kurulumu Çalışılacak klasöre plateau helm chartı ve values klasörü koyulur. Güncel helm chart ve values klasörü bizim tarafımızdan sağlanacaktır. Workspace/ ├── values/ │ ├── credentials.yaml │ ├── database.yaml │ └── product-values.yaml └── plateau/ Konfigürasyon dosyalarının düzenlenmesi Database Ip bilgisi Database ip adresi database.yaml dosyasında ilgili yere eklenir. Database root credentials Database root şifresi credentials.yaml dosyasında ilgili yere eklenir. SCM Private Key app_configs'i barındıracak scm'ye erişebilecek private key credentials.yaml dosyasında ilgili yere eklenir. Kubernetes ortamı üzerinde onboarding uygulamasının kurulacağı bir namespace oluşturulur. bash Oluşturulan namespace'in container registiry'den image çekebilmesi için gerekli olan secret namespace'e eklenir. bash Bu namespace içinde oluşturulan tüm deployment'ların öntanımlı image pull secretler'ının oluşturduğumuz secret olması için namespace'e bir patch yapılması gerekmektedir. bash Onboarding uygulamasının plateau helm chart'ı ile kurulması için ihtiyaç duyulan 3 dosya hazırlanmalıdır. Bunlar: credentials.yaml: Uygulama servislerinin git reposu ve database'lere erişiminde kullanacağı parolalar bulunur. database.yaml: Database adresi portu ve teknoloji bilgisi yer alır. product-values.yaml: Kullanılacak servislerin versiyonları ve ek ayarlarını içerir. Bu dosyaların yanı sıra bir git reposunda her servis için konfigürasyon dosyaları bulunmalıdır. Yukarıdaki hazırlıklar tamamlandığında uyguluma tek komut ile ayağa kalkacaktır. bash Uygulama tek komutla kaldırılabilir. bash Jitsi Sunucu Kurulumu Jitsi sunucusunda en az 4 vCPU işlemci, 16 GB bellek, 200 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Sistem Güncellemesi Kontrolü Sistemin güncel olduğu kontrol edilir bash Kullanılacak Alan Adı ve DNS Ayarları DNS alan adı Jitsi hizmet noktanızı tanımlar. bash Jitsi Repositorysinin Eklenmesi ve Jitsi Kurulumu Jitsi kurulumu için repolar sisteme tanıtılmalıdır. Sonrasında kurulum gerçekleştirilir. bash > /etc/apt/sources.list.d/jitsi-stable.list
wget -qO - https://download.jitsi.org/jitsi-key.gpg.key | sudo apt-key add –

apt update && apt install jitsi-meet -y ]]> Kurulum esnasında SSL/TLS sertifika seçenekleri sorulacaktır. Test ortamı kurulumları için Lets Encrypt sertifikaları tercih edilebilir. PROD ortamları için ise kurumsal sertifika sağlayıcınızdan gelen sertifikalar kullanılmalıdır. Encrypt seçeneği seçerek ilerlenir. Sertifikayı daha sonra değiştireceğiz. Yükleme bittikten sonra aşağıdaki script çalıştırılır bash Jitsi Üzerinde Performans Ayarları Prosody Port Manager Hatasının Düzeltilmesi Portmanager hataları için aşağıdaki ayarlar yapılır. bash Genel Performansı İyileştirmek Sistemin ses ve video ayarları yapılarak daha performanslı çalışması sağlanır. bash Jitsi Sunucusunun Jibriler için Hazırlanması Jibri sunucu kurulumlarına başlamadan önce mevcut ortamın Jibri sunucuları ve servisleri için hazırlanması gerekmektedir. Jitsi sunucusunda çalışan Java versiyon kontrol edilir. bash VMde IBM gibi farklı bir JDK varsa (GCP VM IBM jdk ile birlikte gelir) OpenJDK'yi değiştirmemiz gerekir. bash Jitsi Sunucusunun MUC Ayarları Aşağıdaki ayarları Jitsi ve Jibriler üzerinde MUC (Multi User Chat) bileşenin düzgün çalışması için yapılmalıdır. bash Jitsi Üzerinde Jibri ve Recorder Kullanıcılarını Oluşturma Jitsi sunucusu üzerinde jibri ve recorder kullanıcıları oluşturulur. Birden fazla Jibri sunucusu kullanılacak ise her biri için ayrı jibri ve recorder kullanıcıları ve gerekli ayarları eklenir. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= # Kayıt ayarlarının yapulması
vi /etc/jitsi/meet/meet.firmaadi.abc-config.js
fileRecordingsEnabled: true,
hiddenDomain: 'recorder.meet.firmaadi.abc', ]]> Yapılandırmadan sonra tüm jitsi servisleri yeniden başlatılır. bash Kurumsal Sertifikanın Yüklenmesi Sertifika değiştirilmesi için full chain sertifikasını ve anahtarı uygun yere kaydedilir, aşağdaki scriptle ilgili path verilir. bash Jitsi kurulumunda karşılaşılan sorunlar için GitHub ve Doküman sayfalarına başvurlabilir. Jitsi Sunucusunda NAT Yapılandırılması Jitsi sunucunuz bir NAT (Firewall)'un arkasında ise bu durumda sip-communicator.properties dosyasında aşağıdaki eklentiler yapılır. bash org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS= ]]> Jibri Sunucu Kurulumu Kuruluacak olan her bir Jibri sunucusunda en az 3 vCPU işlemci, 4 GB bellek, 50 GB disk bulunmalıdır. Aşağıdaki kurulum adımları Ubuntu 18.04 LTS işletim sistemi çalışan bir sunucu için verilmiştir. Diğer işletim sistemlerinde kurulum için Jibri sitesi takip edilmelidir. Sunucuda Güncelleme Yapılır bash Jibri Sunucusnda Host Kayıtları Jibri sunucusunda host kayıtları alan adı planalamanıza bağlı olarak güncellenir. bash Jibri Sunucularında Java Sürümünün Yüklenmesi Jibri sunucularında doğru java sürümünün yüklenmesi için aşağıdaki adımlar izlenir, bash > ~/.bash_profile
source ~/.bash_profile]]> ALSA Loopback Modülünün Yüklenmesi ALSA Loopback modülü sanal bir ses sürücüsüdür. bash > /etc/modules

# cihaza erişim test edilir
modprobe snd-aloop

# snd-allop modülünün yüklenip yüklenmediğini test etme
lsmod | grep snd-aloop]]> Jibri Sunucusunda Google Chrome Kurulumu Google Chrome Kurulumu bash /etc/apt/sources.list.d/google-chrome.list

# google chrome un yüklenmesi
apt update && apt install google-chrome-stable -y]]> Google Chrome Üzerinde Uyarıların Kapatılması bash >/etc/opt/chrome/policies/managed/managed_policies.json]]> Chrome Driver Kurulumu bash Jibrinin Kurulması Jibrinin kurulması için gereken anahtarlar ve kütüphane linkleri eklenir ve kurulum yapılır. bash /etc/apt/sources.list.d/jitsi-stable.list""

apt update && apt install jibri -y]]> Jibri Kullanıcısının Gerekli Gruplara Eklenmesi bash Kayıtların Saklanması Kayıtların saklanması için dizin tanımlanıp üzerinde izinler düzenlenir. bash Jibri Ayarlarının Yapılması Jibri sunucusuna ssh ile bağlandıktan sonra ilgil conf dosyasında Jitsi sununcusuna ilişkin baplantı ayarları ve diğer ortam ayarları yapılır. bash Config dosyasyonda sizin ortamınıza uygun ayarları yapılır.{ jfx Confluence Güncellemeler bittikten sonra jibri servisi yeniden başlatılır. bashimage2021-1-26_16-34-30.png","{'title': 'Softtech Dijital Onboarding Uygulaması OnPrem Mikrok8s Kurulumu', 'id': '29892694', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29892694'}"
"Scope Test and Prod Installation of Azure DevOps Onprem version SDLC implentation on Azure DevOps Onprem version Configure CI/CD Pipelines Integrate Continous Deployment Include Rollback Integrate Code Quality Gateays SonarQube Integration Unit Test Integration Migrating Projects from TFS Server to Azure DevOps Server (Git, TFS) Current State Assessment Roadmap and Future State Strategy, Design, and Implementation Effective DevOps toolsets that help you achieve CI/CD Alert and security automation Collaboration tools and team integration DevOps advisory services for your whole team Ongoing performance optimization automate measure and report List of the projects Git TFS Repo Unit Test SDLC DB DevOps - projeye bağlı Integration Testing Versioning Branching Strategy Project Migration Python projesi","{'title': 'Pasha Life Consultancy', 'id': '29897192', 'source': 'https://wiki.softtech.com.tr/display/SDO/Pasha+Life+Consultancy'}"
"Sunucu Altyapısı No Sunucu Ad Ortam IP vCPU RAM (GB) Disk  (GB) Açıklama 1 TUOEDX01 Beta 192.168.99.138 2 8 100 Ubuntu 20.04 LTS, Open EDX Sunucusu 2","{'title': 'Uzaktan Eğitim Projesi', 'id': '29899188', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29899188'}"
"Softtech içinde kullanılan NGINX sunucularının listesi,","{'title': 'NGINX Sunucuları', 'id': '29899381', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=29899381'}"
"WSO2 Prod Kurulumu WSO2 Prod kurulumu aşağıdaki parametreler kullanılarak yapılmıştır. 1- Values values.yml wso2:
  subscription:
    username: """"
    password: """"

  choreoAnalytics:
    enabled: false
    endpoint: """"
    onpremKey: """"

  deployment:
    dependencies:
      mysql: true

      nfsServerProvisioner: false

    persistentRuntimeArtifacts:
      storageClass: ""nfs-storageclass""

      apacheSolrIndexing:
        enabled: false

        capacity:
          carbonDatabase: 50M

          solrIndexedData: 50M

    am:
      imageName: ""wso2am""
      imageTag: ""4.0.0""

      imagePullPolicy: Always

      resources:
        requests:
          memory: ""1Gi""
          cpu: ""1000m""
        limits:
          memory: ""2Gi""
          cpu: ""2000m""

        jvm:
          heap:
            memory:
              xms: ""512m""

              xmx: ""512m""

      livenessProbe:
        initialDelaySeconds: 60

        periodSeconds: 10

      readinessProbe:
        initialDelaySeconds: 60

        periodSeconds: 10

      websub:
        ingress:
          hostname: ""websub.wso2.prod.st""

          annotations:
            kubernetes.io/ingress.class: ""nginx""
            nginx.ingress.kubernetes.io/backend-protocol: ""HTTPS""

      gateway:
        ingress:
          hostname: ""api.softtech.com.tr""

          annotations:
            kubernetes.io/ingress.class: ""nginx""
            nginx.ingress.kubernetes.io/backend-protocol: ""HTTPS""

        replicas: 2

        strategy:
          rollingUpdate:
            maxSurge: 2

            maxUnavailable: 0

      cp:
        livenessProbe:
          initialDelaySeconds: 180

          periodSeconds: 10

        readinessProbe:
          initialDelaySeconds: 180

          periodSeconds: 10

        ingress:
          hostname: ""developer.softtech.com.tr""

          annotations:
            kubernetes.io/ingress.class: ""nginx""
            nginx.ingress.kubernetes.io/backend-protocol: ""HTTPS""
            nginx.ingress.kubernetes.io/affinity: ""cookie""
            nginx.ingress.kubernetes.io/session-cookie-name: ""route""
            nginx.ingress.kubernetes.io/session-cookie-hash: ""sha1""

        replicas: 2

        resources:
          requests:
            memory: ""2Gi""
            cpu: ""2000m""
          limits:
            memory: ""3Gi""
            cpu: ""3000m""

          jvm:
            heap:
              memory:
                xms: ""1024m""

                xmx: ""1024m""

    mi:
      imageName: ""wso2mi""
      imageTag: ""4.0.0""

      replicas: 2
      strategy:
        rollingUpdate:
          maxSurge: 1

          maxUnavailable: 0

      livenessProbe:
        initialDelaySeconds: 35

        periodSeconds: 10

      readinessProbe:
        initialDelaySeconds: 35

        periodSeconds: 10

      resources:
        requests:
          memory: ""512Mi""

          cpu: ""500m""
        limits:
          memory: ""1Gi""

          cpu: ""1000m""

      envs:

      synapseTest:
        enabled: false

      ingress:
        management:
          hostname: ""management.wso2.prod.st""

          annotations:
            kubernetes.io/ingress.class: ""nginx""
            nginx.ingress.kubernetes.io/backend-protocol: ""HTTPS""

kubernetes:
  serviceAccount: ""wso2am-pattern-3-svc-account""

mysql-am:
  mysql:
    persistence:
      storageClass: nfs-storageclass 2- Helm install helm --kubeconfig /home/ubuntu/app-cluster/prod-app-kubeconfig install wso2 wso2/am-pattern-3 --version 4.0.0-1 --namespace wso2 --create-namespace --values values.yaml","{'title': 'WSO2 API Gateway', 'id': '29899416', 'source': 'https://wiki.softtech.com.tr/display/SDO/WSO2+API+Gateway'}"
1-Checkmarx Proje listeleme adımından Export As CSV File Butonuna basıp dosyayı Excel Formatında alırız. 2-Açılan Excel de tüm kolonlar seçip pivot table oluştururuz. 3-Oluşan pivot tablo da Rows ve Values alanlarına Team kolonunu sürükle bırak yöntemiyle bırakırız. 4-Oluşan tablodan aşağıdaki gibi özet bilgi çıkarırız.image2021-8-6_10-58-18.pngimage2021-8-6_10-56-51.pngimage2021-8-6_10-55-54.pngimage2021-8-6_10-53-14.pngimage2021-8-6_10-52-40.pngimage2021-8-6_10-51-41.pngimage2021-8-6_10-51-0.pngimage2021-8-6_10-49-35.png,"{'title': 'Checkmarx ta Özet Rapor Nasıl Alınır', 'id': '38409778', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=38409778'}"
"Altyapı Github Enterprise VM olarak çalışan ve tüm servislerini kendi üzerinde barındıran, appliance mantığında yönetilen bir servistir. İşletim sistemi olarak linux bazlı kendi imajı ile gelmektedir ve güncelleme/kurulum işlemlerinde bu imaj kullanılmalıdır. İlgili imaj resmi public cloud repository'lerinden çekilmektedir. Bu dokümanın hazırlandığı tarihte Github Enterprise instance'ları Azure üzerinde 3.8.0 sürümü ile çalışmaktadır. 2 farklı kurulum bulunmaktadır: Prod: ""github.rally.softtech"" adresi ile erişilebilen prod instance'ları Application Load Balancer arkasında çalışan 2 VM instance idir. Primary - passive replica şeklinde çalışmaktadır. Test: ""github-test.rally.softtech"" adresi ile erişilen test instance'tır. Tek VM olarak çalışmaktadır. Bu instance genel uygulama üzerindeki testler için kullanılmaktadır, kritik veri barındırmaz. Devops Hattı Geliştirme Ekibine bilgi verilerek gerekli durumlarda baştan kurulabilir, resetlenebilir. Github instance'larına ek olarak Actions kullanılarak pipeline çalıştırılabilmesi için Runner instance'ları da ayrıca ihtiyaca göre kurulabilir. Başlangıç olarak test için1 instance, prod için 2 instance runner kurulmuştur. Kurulum adımlarına buna da yer verilmiştir. Kurulan instance'lar Azure üzeirnde farklı resource group'lara bölünerek yönetimsel kolaylık sağlanmıştır. Test ve Prod'a ait kaynaklar, runner'lar ve backup'lar yine ayrı resource group'lara bölünmüştür. Test Instance Kurulum Adımları Öncelikle Azure üzerinde ""rg-github-test"" resource group yaratılmıştır. Kurulumlar buraya yapılacaktır. VM yaratmak için aşağıdaki Azure CI komutu kullanılabilir. Versiyon bilgisi istenilen versiyona göre değiştirilmelidir. VM Create true Yaratılan VM için NSG rule'lar ile erişim izinleri verilir. Şu an için source IP olarak bir kısıt uygulamıyoruz, bu nedenle kurallar source Any olacak şekilde belirtilmiştir, ilerideki güvenlik politikalarına göre sıkılaştırma yapılabilir. NSG Rule Create true Yaratılan instance running duruma geçmesine rağmen uygulama başlamamış durumda olacaktır. Bunun nedeni yarattığımız VM için yalnızca OS diski ile yaratılmış olmasıdır. Github tüm verileri ikincil disk üzerinde saklamaktadır. Böylece upgrade esnasında OS diskini yeni sürüm ile güncelleyerek veri ve instance data'sını ayırmaktadır. Github repo ve diğer tüm bilgileri saklayacak ikinci bir disk yaratıp makineye attach edelim. Boyut olarak güncel veri boyutları değerlendirilerek seçilmelidir. Attach Data Disk true //FIX_ME: DNS kayıt işlemleri Github Actions kullanabilmek için Runner yaratmamız gerekmektedir. Bunun için bir Ubuntu makine yaratıyoruz. Ubuntu sürümü için kurulum anında yayınlanmış olan  LTS sürümü ve minimal imajdan en son patch sürümünde olanı tercih edebiliriz, Github sürümü ile bir bağlantısı bulunmamaktadır. Makine tipi olarak burstable bir instance seçiyoruz. Böylece fiyat avantajını korumakla birlikte gün içinde aktif iş yükü olmadığı durumda kullanmadığımız CPU gücünü anlık iş yüklerinde VM limitinin üzerinde kullanabilmesini sağlıyoruz. (2 cpu varsa anlık olarak 8 - 10 CPU gücüne çıkabiliyo). Runner Create true //FIX_ME: Runner OS işlemleri ve github'a register işlemleri //FIX_ME: Backup schedule adımları eklenmeli Prod Instance Kurulum Adımları VM yaratmak için aşağıdaki Azure CI komutu kullanılabilir. Versiyon bilgisi istenilen versiyona göre değiştirilmelidir. VM Create true Yaratılan VM için NSG rule'lar ile erişim izinleri verilir. Şu an için source IP olarak bir kısıt uygulamıyoruz, bu nedenle kurallar source Any olacak şekilde belirtilmiştir, ilerideki güvenlik politikalarına göre sıkılaştırma yapılabilir. NSG Rule Create true Yaratılan instance running duruma geçmesine rağmen uygulama başlamamış durumda olacaktır. Bunun nedeni yarattığımız VM için yalnızca OS diski ile yaratılmış olmasıdır. Github tüm verileri ikincil disk üzerinde saklamaktadır. Böylece upgrade esnasında OS diskini yeni sürüm ile güncelleyerek veri ve instance data'sını ayırmaktadır. Github repo ve diğer tüm bilgileri saklayacak ikinci bir disk yaratıp makineye attach edelim. Boyut olarak güncel veri boyutları değerlendirilerek seçilmelidir. Attach Data Disk true //FIX_ME: Application Load Balancer ekleme adımları //FIX_ME: DNS kayıt işlemleri Github Actions kullanabilmek için Runner yaratmamız gerekmektedir. Bunun için bir Ubuntu makine yaratıyoruz. Ubuntu sürümü için kurulum anında yayınlanmış olan  LTS sürümü ve minimal imajdan en son patch sürümünde olanı tercih edebiliriz, Github sürümü ile bir bağlantısı bulunmamaktadır. Makine tipi olarak burstable bir instance seçiyoruz. Böylece fiyat avantajını korumakla birlikte gün içinde aktif iş yükü olmadığı durumda kullanmadığımız CPU gücünü anlık iş yüklerinde VM limitinin üzerinde kullanabilmesini sağlıyoruz. (2 cpu varsa anlık olarak 8 - 10 CPU gücüne çıkabiliyo). Runner Create true //FIX_ME: Runner OS işlemleri ve github'a register işlemleri //FIX_ME: Backup schedule adımları eklenmeli Upgrade Prosedürü Upgrade öncesi dikkat edilecek hususlar: Her zaman öncelikle Test instance upgrade edilmeli. 1 haftalık bir süre beklendikten ve Devops Hattı Ekibi tarafından genel testler yapıldıktan sonra Prod upgrade planlanmalıdır. Upgrade esnasında 1 saatlik bir erişim kesinti olacak şekilde planlamamız gerekiyor. Instance upgrade ederken veri erişimi yaptırmıyor. Upgrade öncesi instance'lar maintenance mode'a alınarak instance gracefull shutdown ile kapatılarak backup alınmalıdır. Makine çalışırken alınacak backup'larda veri eksiklikleri olabilir. Sürüm geçişleri için güncel Github politikaları incelenmelidir ancak şu anki sürüm kurallarına göre 2 minör sürüm yukarıya doğrudan çıkabiliyoruz. Çok sık güncelleme yapmamak adına eğer güvenlik riski yoksa 2 sürümde bir upgrade edebiliriz. Patch sürümlerinde bir ön koşul bulunmamaktadır. Dolayısıyla örneğin 3.6.1'den doğrudan 3.8.5'e çıkabiliriz. Eğer 3 minör yukarı çıkacaksak o zaman 2 aşamalı upgrade yapmamız gerekmektedir. Bu nedenle sürüm güncellemelerini düzenli olarak en çok 5 ayda bir yapmamız gerekiyor. Upgrade için aşağıdaki adımlar takip edilebilir: 8443 portu üzerinden aktif olan sunucuya bağlanılıp Maintenance Mode enable edilmelidir. Sadece checkbox'ın işaretlenmesi yeterlidir ancak işlemin bitmesi beklenmelidir. Aktif operasyonlar bittikten sonra maintenance mode aktif olur. VM'e ssh ile bağlanılarak makine komut satırından kapatılır. SSH erişimi için admin kullanıcısı ve 122 portu kullanılmalıdır. VM, Azure portal'da kapalı duruma geçtikten sonra yedekleme başlatılır. Yedek almak için VM objesi açılır. Sol menüden ""Backup"" seçilir ve üst bardaki ""Backup now"" seçilir. Backup retaion tarihi olarak 15 günü aşmayacak bir tarih seçilir ve OK tıklanarak backup başlatılır. Başlayan backup sağ üst köşedeki Notifications alanıda görünmelidir. Öncelikle ""Triggering"" adımında kalır, sonrasında backup işlemi başlar. Backup işlemi başladıktan sonra (""Triggering"" kısmı yeşil olur) üzerine tıklanarak backup job detayına gidilir. ""In progress"" status'ta olan işin en sağdaki ""View details"" alanına tıklayarak detayları görüntülenir. ""Take snapshot"" adımının tamamlanması beklenir, yaklaşık 8-10 dakika sürmektedir. Bu adım tamamlanmadan VM başlatılmamalıdır. ""Transfer data to vault"" adımını beklemeye gerek yoktur. VM, portal üzerinde başlatılır. Hazır duruma gelmesi zaman almaktadır, 8443 üzerinden erişim geldikten sonra upgrade başlatılmadan önce console üzerinde Settings ekranındaki ayarlarının tümümün tamamlanması gerekir. Eğer Github instance'ın internet erişimi varsa bu adım atlanabilir. Yoksa Internet erişimi olan bir makineden yeni sürüm paketin indirilip bu makineye kopyalanması gerekmektedir. İndirilecek paket için https://support.github.com/enterprise/server-upgrade sayfasında sürüm ve ortam bilgileri seçilerek link elde edilebilir. Paket yerel bilgisayara ya da GCP üzerindeki bir makineye indirilebilir. Sonrasında scp ile Github VM'e kopyalanabilir. Upg VM'e ssh ile bağlanılır ve geri kalan adımlar için şu doküman takip edilir: https://docs.github.com/en/enterprise-server@3.8/admin/enterprise-management/updating-the-virtual-machine-and-physical-resources/upgrading-github-enterprise-server#upgrading-with-an-upgrade-package Dokümandaki sürüm bilgisi hali hazırdaki sürüme göre değiştirilmelidir. Upgrade işlemi bittikten sonra testler yapılmalıdır. Upgrade Hata verirse Eğer single instance olan bir ortamda (Örn: Test ortamında single vm çalışıyor) hata verirse: VM kapatılır. Backuptan geri dönülür. Eğer birden fazla instance olan bir ortamda (Örn: Prod ortamında multiple vm var): Eğer ilk upgrade edilen VM'de hata alındıysa Backup'tan geri dönülür. Eğer ilk instance upgrade edildi ve çalışır durumda ise replica'ların backup'larının bir önemi yok, Yeni instance kurar gibi replica kurulabilir. //FIX_ME: Backup'tan dönüş adımları","{'title': 'GitHub Enterprise', 'id': '38411316', 'source': 'https://wiki.softtech.com.tr/display/SDO/GitHub+Enterprise'}"
"Proje Künyesi Başlık Açıklama GCP Proje Adı softtech-demo-capera Proje Linki VPC Tannımları Başlık Açıklama VPC Adı softtech-demo-capera-vpc Subnet Adı subnet-10-223-32-0-24 Subnet Tanımı 10.223.32.0/24 GKE Settings Cluster name: demo-capera Cluster pod network: 172.16.0.0/17 Cluster service network: 172.16.128.0/17 gcloud beta container --project ""softtech-capera-demo"" clusters create ""demo-capera"" --zone ""us-central1-a"" --no-enable-basic-auth --cluster-version ""1.20.8-gke.900"" --release-channel ""regular"" --machine-type ""n2-standard-8"" --image-type ""COS_CONTAINERD"" --disk-type ""pd-standard"" --disk-size ""100"" --node-labels environment=demo,name=demo-capera-gke-cluster --metadata disable-legacy-endpoints=true --scopes "" https://www.googleapis.com/auth/devstorage.read_only"",""https://www.googleapis.com/auth/logging.write"",""https://www.googleapis.com/auth/monitoring"",""https://www.googleapis.com/auth/servicecontrol"",""https://www.googleapis.com/auth/service.management.readonly"",""https://www.googleapis.com/auth/trace.append "" --max-pods-per-node ""110"" --num-nodes ""3"" --logging=SYSTEM,WORKLOAD --monitoring=SYSTEM --enable-ip-alias --network ""projects/softtech-capera-demo/global/networks/softtech-demo-capera-vpc"" --subnetwork ""projects/softtech-capera-demo/regions/us-central1/subnetworks/subnet-10-223-32-0-24"" --cluster-ipv4-cidr ""172.16.0.0/17"" --services-ipv4-cidr ""172.16.128.0/17"" --no-enable-intra-node-visibility --default-max-pods-per-node ""110"" --no-enable-master-authorized-networks --addons HorizontalPodAutoscaling,HttpLoadBalancing,Istio,NodeLocalDNS,GcePersistentDiskCsiDriver --istio-config auth=MTLS_PERMISSIVE --enable-autoupgrade --enable-autorepair --max-surge-upgrade 1 --max-unavailable-upgrade 0 --autoscaling-profile optimize-utilization --enable-vertical-pod-autoscaling --enable-shielded-nodes --tags ""demo-capera-gke-cluster"" --node-locations ""us-central1-a""","{'title': 'Capera Demo Projesi', 'id': '38411628', 'source': 'https://wiki.softtech.com.tr/display/SDO/Capera+Demo+Projesi'}"
"Network bilgileri Network Adı Aralığı Defualt Gateway Açıklama Genel Network 172.17.1.0/24 172.17.1.1 Sunuculara erişim, Site-to-Site VPN networkü IDRac Networkü 172.17.2.0/24 172.17.2.1 IDRac Yönetim Networkü Uygulama Networkü 172.17.3.0/24 172.17.3.1 Aeron haberleşmesi için ayrılmış network, sadece suınucular arasında","{'title': 'Turis Projesi', 'id': '43328071', 'source': 'https://wiki.softtech.com.tr/display/SDO/Turis+Projesi'}"
"Dış IP İç IP Açılan Portlar Proje(ler) Açıklama Ayrıntılar 90.158.37.50 10.224.3.193 80, 443 AHE Beta Yeni Beta Cluster Nginx Ingress Controller 212.98.16.6 192.168.105.10 TCP: 80, 443, 8443, 8000 UDP: 10000-20000 Onboarding Jitsi Eski Prod Cluster, Extranet Jitsi sunucusuna servisler kapsamında yapılan erişim tanımlarıdır. 212.98.16.16 (eski: 212.98.16.5) 192.168.99.131 TCP: 80, 443, 8443, 8000 UDP: 10000-20000 Onboarding Jitsi https://meet.onboardinguat.onplateau.com Beta Cluster Jitsi sunucusuna servisler kapsamında yapılan erişim tanımlarıdır. 212.98.16.17 192.168.99.133 TCP: 80, 443, 8443, 8000 UDP: 10000-20000 Onboarding Jitsi https://meet.onboardingdemo.onplateau.com Beta Cluster Jitsi sunucusuna servisler kapsamında yapılan erişim tanımlarıdır. 90.158.37.49 192.168.105.21 TCP: 80, 443, 8443, 8000 UDP: 10000-20000 Onboarding Jitsi Eski Prod Cluster, Extranet Jitsi sunucusuna servisler kapsamında yapılan erişim tanımlarıdır. 90.159.29.29 192.168.8.65 80,443 https://admin.pob.com.tr/ https://panel.pob.com.tr/ https://api.pob.com.tr/ Eski Prod Cluster, Istio Gateway Istio Ingress Gateway bağlantısı 90.159.29.30 192.168.100.65 80, 443 studiobeta.onplateau.com https://adminbeta.pob.com.tr/ https://panelbeta.pob.com.tr/ https://apibeta.pob.com.tr/ Eski Beta Cluster, Istio Gateway Istio Ingress Gateway bağlantısı 212.98.16.7 192.168.8.68 80, 443 studio.onplateau.com Nginx Ingress Controller 90.158.37.54 192.168.105.21 80, 443 https://face-reg.onplateau.com Eski Prod Cluster, Extranet Face Recognition Prod Sunucusu İzin verilen IPler: 212.174.79.65 (ÜnlüCo) 90.159.29.28 (Anthos) Softtech GCP 104.197.131.52 34.70.54.43 104.198.62.88 34.72.82.218 35.224.10.59 34.123.208.72 34.69.232.76 34.121.242.139 35.239.176.22 90.158.37.53 192.168.99.135 80, 443 https://face-reg.onboarding.ga Eski Test Cluster Face Recognition Test Sunucusu 212.174.79.65 (ÜnlüCo) 90.159.29.28 (Anthos) Softtech GCP 104.197.131.52 34.70.54.43 104.198.62.88 34.72.82.218 35.224.10.59 34.123.208.72 34.69.232.76 34.121.242.139 35.239.176.22 90.158.37.51 10.224.6.200 80, 443 maxifarm.com.tr Smart Farm Projesi Nginx Ingress Controller 212.98.16.11 10.224.7.200 80, 443 Emtia Projesi, PreProd Ortamı Nginx Ingress Controller 212.98.16.12 10.224.7.20 80, 443 Emtia Projesi, Prep Prod ortamı Data Plataeau Sunucusu Standalone + Nginx Ingress 212.98.16.13 10.224.8.200 80, 443 https://apigateway.proemtia.com https://identity-provider.proemtia.com https://internal-ui.proemtia.com https://ui.proemtia.com Emtia Projesi, Prod Ortamı Nginx Ingress Controller 212.98.16.8 10.224.8.20 80, 443 https://dp.proemtia.com Emtia Projesi, Prot ortamı Data Plataeau Sunucusu Standalone + Nginx Ingress 212.98.16.4 192.168.100.101 80, 443 uat.360survey.softtech.com.tr Common Beta cluster Nginx Ingress 212.98.16.5 192.168.8.101 80, 443 360survey.softtech.com.tr Common Prod cluster Nginx Ingress 212.98.16.21 10.224.4.202 80, 443 digital-ui-tfs.isfaktoring.com.tr İş Faktoring Tedarikçi Finansmanı Nginx Ingress External class-name: nginx-external, İç bağlantılar için ayrı bir ingress var (class-name: nginx, 10.224.4.200)","{'title': 'Anthos Dış Bağlantıları', 'id': '43328673', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=43328673'}"
"Elastic - Filebeat - Kibana Kurulumu Tüm bileşenlerin kurulumu için Elastic Helm Chart 7.16.2 versiyonu kullanılmıştır. Elastic Helm Repo'su aşağıdaki gibi eklenmiştir. helm repo add elastic https://helm.elastic.co
helm repo update Elasticsearch Kurulumu Elasticsearch values.yaml şu şekilde düzenlendi. Kaynaklar arttırıldı esJavaOpts: ""-Xmx8g -Xms8g"" # example: ""-Xmx1g -Xms1g""
resources:
  requests:
    cpu: ""4000m""
    memory: ""16Gi""
  limits:
    cpu: ""4000m""
    memory: ""16Gi"" Storage class NFS olarak düzenlendi, PVC RWX olarak ayarlandı ve boyutu replica başına 500Gi olarak belirlendi volumeClaimTemplate:
  storageClassName: nfs-storageclass
  accessModes: [""ReadWriteMany""]
  resources:
    requests:
      storage: 500Gi elk namespace'i yaratıldı kubectl create ns elk Elasticsearch helm ile kuruldu helm -n elk install elasticsearch elastic/elasticsearch --values values.yaml Elasticsearch servisi nodeport olarak açıldı apache-nifi aracı ile onboarding ve pob uygulamalarının loglarını harici bir diske yedeklemek için açılmıştır. Filebeat Kurulumu Filebeat values.yaml dosyası şu şekilde düzenlenmiştir. Memory request ve limiti arttırıldı. daemonset:
  resources:
    requests:
      cpu: ""100m""
      memory: ""1024Mi""
    limits:
      cpu: ""1000m""
      memory: ""2048Mi""
  tolerations: [] Filebeat helm ile kuruldu helm -n elk install filebeat elastic/filebeat --values values.yaml Kibana Kurulumu Kibana values.yaml dosyası şu şekilde düzenlenmiştir. Memory ve CPU limitleri arttırıldı. resources:
  requests:
    cpu: ""1000m""
    memory: ""2Gi""
  limits:
    cpu: ""2000m""
    memory: ""4Gi"" Kibana helm ile kuruldu helm -n elk install kibana elastic/kibana --values values.yaml Kibana için ingress oluşturuldu ve uygulandı apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kibana namespace: efk spec: rules: - host: kibana.prod.st http: paths: - backend: service: name: kibana-kibana port: number: 5601 path: /* pathType: ImplementationSpecific kubectl apply f kibana ingress.yaml Elastic - Filebeat - Kibana Kurulumu Security Kurulumu Elasticsearch, Filebeat ve Kibana, x-pack kurulumu için Elastic helm-charts github reposu v7.16.2 indirilip unzip edilir ve helm-charts-7.16.2 klasörüne gidilir. Normal kurulumda düzenlenen values dosyaları; elastic , filebeat ve kibana klasörlerinin altındaki values.yaml dosyaları üzerine yazılır. Elasticsearch kurulumu Elasticsearch security klasörüne gidilir. cd elasticsearch/examples/security Makefile çalıştırılır. make secrets
make install Filebeat Kurulumu Filebeat security klasörüne gidilir. cd filebeat/examples/security Makefile çalıştırılır. make install Kibana Kurulumu Kibana security klasörüne gidilir. cd kibana/examples/security Makefile çalıştırılır. make secrets
make install Security Kurulum Notları Makefile'lar, içerisindeki helm ve kubectl komutlarına --kubeconfig /home/ubuntu/app-cluster/prod-app-kubeconfig -n elk kubeconfig ve namespace eklenerek düzenlenmiştir.","{'title': 'Anthos Ortamına ELK Kurulumu', 'id': '43331838', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=43331838'}"
250 250 250 250WSO2 KURULUMU.docxSCHEMASPY KURULUMU.docxNFS_KURULUMU.docxHELM KURULUMU.docxANSIBLE_KURULUMU.docxDOCKER KURULUMU.docxRANCHER_SINGLE_NODE_ALMA_LINUX.docxZabbix_Node_Exporter_Install.docxZabbix_Kurulum.docxUBUNTU_SUNUCUSUNDA_SELINUX_DISABLE_ETME.docxMssql_Install_Rhel.docxAERON_RESTART.docxAeron_Message_Bus_Installation.docx,"{'title': 'Infra Installation', 'id': '48300328', 'source': 'https://wiki.softtech.com.tr/display/SDO/Infra+Installation'}"
Confluence Upgrade Uat Confluence ekibi upgrade öncesi backup alır Confluence web arayüzünden uygulama backup'ı Sunucu tarafında aşağıdaki dosyaların yedeği /opt/atlassian/confluence/bin/setenv.sh Environment bilgisi Yeni versiyon kurulum binary dosyası indirilir ve çalıştırılır Yedeği alınan dosyalar geri yüklenir Prod Confluence ekibi upgrade öncesi backup alır Confluence web arayüzünden uygulama backup'ı Sunucu tarafında aşağıdaki dosyaların yedeği /opt/atlassian/confluence/bin/setenv.sh Environment bilgisi /opt/atlassian/confluence/conf/server.xml Server konfigürasyonu /opt/atlassian/confluence/jre/lib/security/cacerts Java sertifika store Yeni versiyon kurulum binary dosyası indirilir ve çalıştırılır Yedeği alınan dosyalar geri yüklenir,"{'title': 'Confluence Upgrade', 'id': '48300342', 'source': 'https://wiki.softtech.com.tr/display/SDO/Confluence+Upgrade'}"
"Qcloud Hosts Tanımları Provider Servis Erişimleri Anthos Beta ve Prod ortamlarında qcloud namespace'i altında bulunan provider servisinin gcp ortamına erişimi için deployment'ına aşağıdaki tanımlar girilmiştir. spec:
  .
  .
  template:
    .
    .
    spec:
      .
      .  
      hostAliases:
      - hostnames:
        - gitlab.rally.softtech
        - jenkins.rally.softtech
        ip: 10.223.2.100
      - hostnames:
        - authorization.fafsmartfarm.dev.rally.softtech
        - authorization.cmbdemo.dev.rally.softtech
        - authorization.capera.dev.rally.softtech
        - authorization.sdesigner.dev.rally.softtech
        ip: 10.223.8.6
      - hostnames:
        - authorization.turib-int.turis.plateaux.softtech
        - authorization.cmbdemo-int.cmbdemo.plateaux.softtech
        ip: 10.223.4.148","{'title': 'Qcloud Hosts Tanımları', 'id': '48300776', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=48300776'}"
"İçerik Turis Teknik Altyapı dokümanı, proje kapsamında yer alan uygulamaların düzgün bir şekilde ortamlarda çalıştırılabilmesi için gereken sunucu sistemleri, sanallaştırma sistemleri, ağ mimarileri, container orkestrasyon ortamları ve benzeri altyapı bileşenlerini içerkmektedir. Bu bileşenlerin şematik gösterimleri, açıklamaları, kullanım amaçları eklenmiştir. Türis Proje Altyapısı Turis altyapısının genel şeması aşağıdaki diagramda paylaşılmıştır. (Kaynak: Turis Ürün Ekibi) Şekil 1. Turis Uygulamasının Genel Bileşen Şeması Türis Projesinde yer alan uygulamalar temel olarak iki farklı ortamda çalışmaktadırlar, Borsa Yönetim Sistemi: Mikro Servis tabanlı geliştirilmiş ve container orkestrasyon platformunda (Kubernetes) çalışan bileşenlerdir. Bu platform altyapısında PostgreSQL veritabanı, Kafka vb. üçüncü parti alt yapı bileşenlerini kullanır. Aynı zamanda Matching Engine Sistemi ve bazı dış servislerle haberleşir. Matching Engine Sistemi: Ağırlıklı olarak Fiziksel sunucular üzerinde ve UDP tabanlı çalışan Aeron bileşenlerini içermektedir. Aeron Matching Engine Sistemi yüksek performanslı ve yedekli mimaride çalışan bir mesajlaşma servisidir. Aeron Raft Consesus algoritmasına dayanır. Aeron ve Raft'a ilişkin bağlantılar Referanslar bölümünde verilmiştir. Borsa Yönetim Sistemi Altyapı Gereksinimleri Borsa Yönetim Sistemi sayıları 40ı aşan Mikro Servis'in çalıştığı yapısıyla kendi aralarında ve dış servislerle haberleştikleri bir yapıya ihtiyaç duyarlar. Aynı zamanda otomatik olarak gerekli bakım ve güncellemelerin yapılmasına olanak sağlayan Kubernetes Mimarilerinden birine ihtiyaç duyar. Kubernetes Cluster ve Altyapısı Kubernetes Cluster yapısı itibariyle yedekli mimaride çalışır ve en az üç Worker Node'dan oluşur. Kubernetes Cluster'ın kendisi yedekli bir sanallaştırma mimarisi üzerinde çalışmalıdır ve bu mimarinin altyapısında minimum 3 fiziksel sunucudan oluşmalıdır. Şekil 2. Kubernetes Cluster Mimarisi Türis Borsa Yönetim Sistemi kapsamında Prod, Test, vb. ortamlar için farklı ağ bazında ayrıştırılmış Kubernetes Clusterlar kurulması tavsiye edilmektedir. Bu anlamda aşağıdaki Cluster'ların belirtilen Worker kapasiteleri ile açılması performans anlamında yararlı olacaktır. Tablo 1: Kubernetes Cluster İhtiyaçları Cluster İşlevi Minimum Kapasite İhtiyacı Prod Prod Clusterı 48 vCPU, 384 GB bellek Pre Prod Pre Prod Clusterı, uygulama öncesi clusterı 24 vCPU, 192 GB bellek UAT UAT testlerinin uygulanacağı Cluster 24 vCPU, 192 GB bellek Utils Gitlab, Jenkins ve diğer araçlarına kurulacağı Cluster 24 vCPU, 192 GB bellek Yedek Prod Yedek veri merkezinde çalışan yedek clusterdır. CI/CD hattı gerekmesi durumunda kurulur. 48 vCPU, 384 GB bellek Dev, Test CI hattı kurulması durunda gereken clusterdır 24 vCPU, 192 GB bellek Clusterlara ilişkin aşağıdaki kısıtlara dikkat edilmelidir, Utils Clusterından diğer clusterlara network erişimi sağlanmalı. Prod, UAT, Pre Prod, Dev Test clusterları arasında kesinlikle network erişimi olmamalıdır . Kapasite ihtiyaçları için güncel uygulama ihtiyaçları göz önünze alınmalıdır. Turis Borsa Yönetim Sistemi uygulamasının düzgün çalışması ayağa kaldırılacak olan her bir Kubernetes Cluster en azından aşağıdaki özelliklere sahip olmalıdır. Tablo 1: Kubernetes Cluster Gereksinimleri Özellik Tanım Kubernetes Sürümü v1.21.x ve üzeri sürümler Admin Load Minimum 3 node Worker Load 8 vCPU x 48-64 GB per Node, Minimum 3 node, tercihen 6 node olmalı Container Runtime Containerd CNI Calico, Flannel, vb. desteklenen güncel network altyapı sağlayıcıları Storage NFS, GlusterFS, VsphereVolume, Local (Kalıcı veriler için NFS gibi çözümler kullanılmalı), altyapıda CEPH gibi bir çözüm olması ayrıca kullanışlı olur Ingress Controller NGINX, Istio, Service Mesh Load Balancers MetalLB, NGINX, Istio (L4 seviyesinde bir LB gereksinimi ile HTTP/HTTPS dışındaki TCP tarfiği için LB gereksinimi ayrıca değerlendirilmeli) Cluster Level Logging ELK Stack ya da EFK Stack Cluster Level Resource Monitoring Prometheus ve Grafana Service Address Range Tercihinize bağlı, diğer clusterlarla çakışmaması tercih edilebilir Pod Address Range Tercihinize bağlı, diğer clusterlarla çakışmaması tercih edilebilir Kubernetes Cluster'ın üzerinde çalışacağı sanallaştırma ortamı en az üç adet fiziksel sunucudan oluşan bir altyapı üzerinde çalışmalıdır. Bu altyapıda aşağıdaki özelliklere sahip fiziksel sunucular çalıştırılabilir. Tablo 2: Fiziksel Sunucuların Özellikleri Ortam Min Sunucu Sayısı Sunucu Başına Eş Zamanlı Thread Sayısı Sunucu Başına RAM Sunucu Başına Disk Ana Veri Merkezi 3 64 1000 2 x 480 GB Yedek Veri Merkez 3 64 1000 2 x 480 GB Bu sunucular arka planda yüksek performanslı (tercihen SSD) SAN sistemlerine bağlı olmalıdırlar. Kubernetes altyapısında çalışan podlar ve sanallaştırma katmanında çalışan sunucular bu SAN sistemi üzerinde disklere bağlanacaklardır. Sunucular SAN'e Fiber/Channel üzerinden bağlanmış olmalıdır. Kubernetes Üzerinde Yapılan Kurulumların Mimarisi Kubernetes üzerinde yapılan kurulumlarda her bir servis bir deployment olarak tanımlanır. Deployment'larda ise Ingress Controller, Servis tanımları, Podlar, Replica Setler ve Podlar tarafından kullanılanılan diskler (PVC) yer alır. Borsa Yönetim Sistemi Yedekli Mimarisi Borsa Yönetim Sistemi için tasarlanan alt yapılarda aşağıdaki gibi yedekli bir mimari kullanılabilir. Matching Engine Sistemi Altyapı Gereksinimleri Diğer Yardımcı Sistemler VeriTabanları Sunucu Görevi Sunucu Versiyon Template LC_COLLATE LC_CTYPE PostgreSql 11.13 en_US.UTF-8 PostgreSql 11.13 en_US.UTF-8 en_US.UTF-8 Referanslar Areon - https://github.com/real-logic/aeron Raft Consensus Algorithm - https://raft.github.io/SampleDeployment.pngYedekliKubernetesMimarisi.pngKubernetesVerticalStructure.pngTURIS_LayoutV5-Segmented.png","{'title': 'Turis Projesi Teknik Altyapı Gereksinimleri', 'id': '48300928', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=48300928'}"
"Unluco Prod Kurulumu Microk8s Kurulumu Microk8s tüm node'lara aşağıdaki komut ile kurulur. node'ların hostname'leri büyük harf veya alttire içermemelidir. 10.0.15.185 unlupolmicrok8s01 10.0.15.186 unlupolmicrok8s02 10.0.15.187 unlupolmicrok8s03 snap install microk8s --classic Tüm node'larda microk8s'in başladığından emin olunur. microk8s.status --wait-ready İlk node üzerinde add-node komutu çalıştırılır ve token üretilir. microk8s add-node Oluşan token kopyalanır ve diğer nodelarda join komutu ile çalıştırılır. 10.0.15.186 unlupolmicrok8s02 10.0.15.187 unlupolmicrok8s03 microk8s join 10.0.15.185:25000/<token> Node'lar cluster'a eklendikten sonra kontrol edilir. microk8s.kubectl get nodes -o wide Ekstra adımlar Servislerin iletişimi ve depolama ihtiyaçları için dns ve storage aktif edilir. Helm chart ile uygulama kurmak için ekstra olarak helm3 aktif edilir. microk8s enable storage dns helm3 Floating IP adreslerini loadBalancer servislere verebilmek için metallb aktif edilir. microk8s enable metallb:10.0.15.200-10.0.15.204 Nginx-Ingress kurulumu için: Komutu çalıştırılır. microk8s enable ingress Nginx ingress servisi aşağıdaki yaml dosyası ile oluşturulur. apiVersion: v1
kind: Service
metadata:
  name: ingress
  namespace: ingress
spec:
  selector:
    name: nginx-ingress-microk8s
  type: LoadBalancer
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
    - name: https
      protocol: TCP
      port: 443
      targetPort: 443 Metallb kurulumunda belirtilen ip havuzundan bir adres alıp almadığı kontrol edilir. Havuzdan rastgele IP yerine elle ip vermek için spec altına loadBalancerIP: x.y.z.a şeklinde adres belirtilebilir. Cluster'a NFS storageClass ve NFS Provisioner eklenmesi Aşağıdaki yaml dosyası apply edilir. ---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-pod-provisioner-sa
  namespace: nfs
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-clusterRole
  namespace: nfs
rules:
  - apiGroups: [""""]
    resources: [""persistentvolumes""]
    verbs: [""get"", ""list"", ""watch"", ""create"", ""delete""]
  - apiGroups: [""""]
    resources: [""persistentvolumeclaims""]
    verbs: [""get"", ""list"", ""watch"", ""update""]
  - apiGroups: [""storage.k8s.io""]
    resources: [""storageclasses""]
    verbs: [""get"", ""list"", ""watch""]
  - apiGroups: [""""]
    resources: [""events""]
    verbs: [""create"", ""update"", ""patch""]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-rolebinding
  namespace: nfs
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-clusterRole
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: nfs
rules:
  - apiGroups: [""""]
    resources: [""endpoints""]
    verbs: [""get"", ""list"", ""watch"", ""create"", ""update"", ""patch""]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: nfs
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: nfs
roleRef:
  kind: Role
  name: nfs-pod-provisioner-otherRoles
  apiGroup: rbac.authorization.k8s.io
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-pod-provisioner
  namespace: nfs
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-pod-provisioner
  template:
    metadata:
      labels:
        app: nfs-pod-provisioner
    spec:
      serviceAccountName: nfs-pod-provisioner-sa
      containers:
        - name: nfs-client-provisioner 
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-provisioner-v
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME 
              value: nfs-provisioner
            - name: NFS_SERVER
              value: 10.0.15.198 #NFS Server IP
            - name: NFS_PATH
              value: /srv/nfs/nfsdata
      volumes:
        - name: nfs-provisioner-v
          nfs:
            server: 10.0.15.198 #NFS Server IP
            path: /srv/nfs/nfsdata 
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storageclass
provisioner: nfs-provisioner
reclaimPolicy: Retain
allowVolumeExpansion: true
parameters:
  archiveOnDelete: ""false"" Minio Kurulumu Unluco tarafından sağlanan sunucularda roota bağlanan diskin tamamının kullanılmadığı farkedilmiştir. Bu duruma özel olarak aşağıdaki komutlar ile diskin boyutu arttırılmıştır. lvresize -l +100%free /dev/mapper/ubuntu--vg-ubuntu--lv
resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv Sunucuya Docker kurulur. apt install docker.io Minio verilerinin tutulacağı klasör oluşturulur. mkdir /srv/minio Minio aşağıdaki komut ile kurulur docker run -d -p 9000:9000 --name minio --restart always -v /srv/minio:/data -e ""MINIO_ACCESS_KEY=admin"" -e ""MINIO_SECRET_KEY=<minio-secret-key>"" minio/minio server /data NFS Server Kurulumu Unluco tarafından sağlanan sunucularda roota bağlanan diskin tamamının kullanılmadığı farkedilmiştir. Bu duruma özel olarak aşağıdaki komutlar ile diskin boyutu arttırılmıştır. lvresize -l +100%free /dev/mapper/ubuntu--vg-ubuntu--lv
resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv Nfs Server kurulumu için NFS Server ve Tüm Nodelar üzerinde yapılacak işlemler şu şekildedir. NFS Server Üzerindeki İşlemler Hosts (/etc/hosts) Dosyasına Microk8s Node'ları eklenir 10.0.15.185 master1
10.0.15.186 master2
10.0.15.187 master3 NFS Server için gerekli paketler kurulur apt update
apt install nfs-kernel-server nfs-common portmap NFS Server'in paylaşacağı klasör yaratılır mkdir -p /srv/nfs/nfsdata
chown nobody:nogroup /srv/nfs/nfsdata ""/etc/exports"" dosyasına aşağıdaki satırlar eklenir /srv/nfs/nfsdata    master1(rw,sync,no_root_squash,no_subtree_check)
/srv/nfs/nfsdata    master2(rw,sync,no_root_squash,no_subtree_check)
/srv/nfs/nfsdata    master3(rw,sync,no_root_squash,no_subtree_check) NFS Server enable edilir ve restart 'lanır. systemctl enable nfs-server
systemctl restart nfs-kernel-server Tüm Node'larda Yapılan İşlemler Hosts (/etc/hosts) Dosyasına NFS Server eklenir 10.0.15.198 nfs NFS Server'ı bağlamak için gerekli paket kurulur. apt install -y nfs-common NFS Server'in mount edileceği klasör oluşturulur mkdir -p /mnt/nfsdata Oluşturulan klasör mount edilir mount -t nfs nfs:/srv/nfs/nfsdata /mnt/nfsdata ""/etc/fstab""'a aşağıdaki satır eklenir. nfs:/srv/nfs/nfsdata /mnt/nfsdata nfs4 rw,sync 0 0 İsteğe bağlı olarak sunucunun restart olduğunda NFS bağlantısını koruyup korumayacağı aşağıdaki gibi kontrol edilebilir umount /mnt/nfsdata
mount | grep nfs
mount -a
mount | grep nfs MYSQL Kurulumu Master ve Replica sunucusundaki ortak ayarlar Mysql Kurulur apt update
apt install mysql-server mysql-client
mysql_secure_installation ""/etc/mysql/mysql.conf.d/mysqld.cnf"" dosyasında aşağıdaki düzenlemeler yapılır bind-address = 0.0.0.0
mysqlx-bind-address = 0.0.0.0
server-id = 1
log_bin = /var/log/mysql/mysql-bin.log
log_bin_index = /var/log/mysql/mysql-bin.log.index
relay_log = /var/log/mysql/mysql-relay-bin
relay_log_index = /var/log/mysql/mysql-relay-bin.index server-id slave üzerinde 2 olarak ayarlanır Mysql restart edilir systemctl restart mysql
systemctl status mysql Master Üzerinde Yapılanlar Replica sunucusu için kullanıcı yaratılır mysql -u root -p
mysql> CREATE USER 'replication_user'@'10.0.15.189' IDENTIFIED BY '<replication_user_password>';
mysql> GRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'10.0.15.189'; Position ve File not edilir. mysql> SHOW MASTER STATUS;

*************************** 1. row ***************************
             File: mysql-bin.000001
         Position: 733
     Binlog_Do_DB:
 Binlog_Ignore_DB:
Executed_Gtid_Set:
1 row in set (0.00 sec) Slave Üzerinde Yapılanlar Aşağıdaki komutlar çalıştırılır Not edilen Position ve File aşağıdaki MASTER_LOG_FILE ve MASTER_LOG_POS alanlarında kullanılır. mysql -u root -p
mysql> STOP SLAVE;
mysql> CHANGE MASTER TO MASTER_HOST ='10.0.15.188', MASTER_USER ='replication_user', MASTER_PASSWORD ='<replication_user_password>', MASTER_LOG_FILE = 'mysql-bin.000001', MASTER_LOG_POS = 733;
mysql> START SLAVE; Replicalı mysql sunucusunun çalışıp çalışmadığını denemek için master üzerinde bir db yaratıp slave üzerinden kontrolü sağlanabilir master üzerinde mysql> CREATE DATABASE replication_db; slave üzerinde SHOW DATABASES;","{'title': 'Ünlüco Altyapı Kurulumları', 'id': '48301268', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=48301268'}"
"Unluco Filebeat to Graylog Create namespace kubectl create ns filebeat Create unluco-filebeat.yaml ---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: filebeat
  labels:
    k8s-app: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: ""/var/log/containers/""

    # To enable hints based autodiscover, remove `filebeat.inputs` configuration and uncomment this:
    #filebeat.autodiscover:
    #  providers:
    #    - type: kubernetes
    #      host: ${NODE_NAME}
    #      hints.enabled: true
    #      hints.default_config:
    #        type: container
    #        paths:
    #          - /var/log/containers/*${data.kubernetes.container.id}.log

    processors:
      - add_host_metadata:
    output.logstash:
      hosts: [""${LOGSTASH_HOST:logstash}:${LOGSTASH_PORT:5044}""]
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: filebeat
  labels:
    k8s-app: filebeat
spec:
  selector:
    matchLabels:
      k8s-app: filebeat
  template:
    metadata:
      labels:
        k8s-app: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:8.0.0
        args: [
          ""-c"", ""/etc/filebeat.yml"",
          ""-e"",
        ]
        env:
        - name: LOGSTASH_HOST
          value: 10.0.10.111
        - name: LOGSTASH_PORT
          value: ""5044""
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
          # If using Red Hat OpenShift uncomment this:
          #privileged: true
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0640
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart
      - name: data
        hostPath:
          # When filebeat runs as non-root user, this directory needs to be writable by group (g+w).
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: filebeat
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: filebeat
  namespace: filebeat
subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: filebeat
roleRef:
  kind: Role
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: filebeat-kubeadm-config
  namespace: filebeat
subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: filebeat
roleRef:
  kind: Role
  name: filebeat-kubeadm-config
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    k8s-app: filebeat
rules:
- apiGroups: [""""] # """" indicates the core API group
  resources:
  - namespaces
  - pods
  - nodes
  verbs:
  - get
  - watch
  - list
- apiGroups: [""apps""]
  resources:
    - replicasets
  verbs: [""get"", ""list"", ""watch""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: filebeat
  # should be the namespace where filebeat is running
  namespace: filebeat
  labels:
    k8s-app: filebeat
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs: [""get"", ""create"", ""update""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: filebeat-kubeadm-config
  namespace: filebeat
  labels:
    k8s-app: filebeat
rules:
  - apiGroups: [""""]
    resources:
      - configmaps
    resourceNames:
      - kubeadm-config
    verbs: [""get""]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: filebeat
  labels:
    k8s-app: filebeat
--- Apply unluco-filebeat.yaml kubectl apply -f unluco-filebeat.yaml","{'title': 'Unluco Filebeat to Graylog', 'id': '49875988', 'source': 'https://wiki.softtech.com.tr/display/SDO/Unluco+Filebeat+to+Graylog'}"
Sunucu Bilgileri İşNet'te kurulmuş olan sunucular ve IP billgileri HOSTNAME VLAN 1 (Standart) VLAN 2 (UDP) aeron01 172.17.1.11 172.17.3.11 aeron02 172.17.1.12 172.17.3.12 aeron03 172.17.1.13 172.17.3.13 matchengine01 172.17.1.14 172.17.3.14 matchengine02 172.17.1.15 172.17.3.15 gateway01 172.17.1.16 172.17.3.16 gateway02 172.17.1.17 172.17.3.17 repo01 172.17.1.18 172.17.3.18 aerondb01 172.17.1.19 172.17.3.19 console01 172.17.1.20 172.17.3.20,"{'title': 'İşNet Fiziksel Sunucu Altyapısı', 'id': '53839135', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=53839135'}"
,"{'title': 'Altyapı Değişiklik Yönetimi', 'id': '53840356', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=53840356'}"
250 250,"{'title': 'Altyapı Değişiklikleri', 'id': '53840358', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=53840358'}"
Intro AHE Projesinde Kurulan Sunucular Sunucu Adı Ortam IP vCPU RAM (GB) Disk (GB) Açıklama,"{'title': 'AHE Projesi', 'id': '55902472', 'source': 'https://wiki.softtech.com.tr/display/SDO/AHE+Projesi'}"
"GCP üzerindeki firewall kurallarını Güvenlik ekibi ile birlikte yönetiyoruz. Burada kuralların genel kullanımı ile ilgili politika belirleme ve denetleme yetkisi Güvenlik ekibinde iken, bu kuralların yönetimi platform olarak bizim görevimiz. Bu işlemleri yaparken öncelikle kuralları statik olarak export edip güvenlik ekibi ile birlikte eylemleri belirliyoruz. Sonrasında bu liste üzerindeki kural isimleri ile işlem yapıyoruz. Bu dokümanda bu işlemler için kullanılan komutları anlatacağım. Tüm komutlar için WSL üzerindeki Ubuntu-20.04 kullanılmıştır. Öncesinde gcloud komutunun kurulumu google repolarındaki önerilen yöntemle kurulmalı ve oturum açılmalıdır. Firewall Kurallarını CSV dosyası olarak export etme Usage : ./getGcpFirewallRulesToCSV.sh project-name bash getGcpFirewallRulesToCSV.sh true true &2
  exit 1
fi

#eğer belirtildi ise output dosyasını oku, yoksa proje isminden türet ve ekrana path'i yaz
outputFile=""${projectName}-firewalls.csv""
if [[ ""${2}"" != """" ]]
then
  outputFile=""${2}""
fi
echo ""Output file is ${outputFile}""

#csv header bilgisini tab ile ayırarak basalım, sonrasında virgüle çevireceğiz
echo -e ""name\tnetwork\tdirection\tpriority\tsource\tsourceTags(matchCount)\tdestination\ttargetTags(matchCount)\tallowed\tdenied\tdisabled"" > ${outputFile}

#kuralları custom formatta al.
gcloud compute firewall-rules list --project=""${projectName}"" --format=""value(name,network.basename(),direction,priority,sourceRanges,sourceTags,destinationRanges,targetTags,allowed[].map().firewall_rule().list():label=ALLOW,denied[].map().firewall_rule().list():label=DENY,disabled)"" >> ${outputFile}

#Port numraları ya da ip listeleri bu şekilde yazılıyor ama aynı hücre içierisinde boşlukla istiyoruz
#tüm virgül ve noktalı virgüllerden kurtul
#gcloud komutu alanlar arasında tab karateri ayracını kullanıyor,tab karakterlerini csv separator olarak değiştir
#csv seperator exxcel diline göre virgül ya da noktalı virgül olabilir, o yüzden değişkenden okuyoruz
sed -i -e 's/,/ /g' -e 's/;/ /g' -e ""s/\t/${csvSeparator}/g"" ${outputFile}

#kodun bundan sonrası network tag'lerin yanına kaç makineden kullanıldığını (N) şekinde yazabilmek için
#IFS değiştireceğimiz için subshell açalım, sonrasında kolaylık olsun
(
#for döngüsü boşluklara göre değil newline karakterlerine göre dönsün diye IFS değiştirelim
IFS=$'\n'
#proje içerisindeki compute instance'ların
#  network tag'lerini okuyalım
#  boş satırları silelim
#  ; ile ayrılmış birden fazla tag varsa satırlara ayrıalım
#  sıralayıp tekrar edenlerin saydıralım
for tagLine in $(gcloud compute instances list --project=""${projectName}"" --format=""value(tags.items)"" | sed -e '/^$/d' -e 's/;/\n/g' | sort | uniq -c)
do
  #her tag için
  countOfTag=$(echo ""${tagLine}"" | awk '{print$1}')
  nameOfTag=$(echo ""${tagLine}"" | awk '{print$2}')

  #csv üzerindeki tagname'in sonuna (N) şekilnde kaç makinede kullanıldığını yazalım. Burada kural tek başına ise başında ve sonunda csv ayracı, eğer kuralda birden fazla tag varsa sırasıyla başta, sonda ve ortada geçenleri değiştirelim
  #burada özellikle 4 farklı expression vermemizin sebebi contains diye bakmadan exact match tag bakabilmek
  sed -i \
      -e 's#'""${csvSeparator}${nameOfTag}${csvSeparator}""'#'""${csvSeparator}${nameOfTag}(${countOfTag})${csvSeparator}#g"" \
      -e 's#'""${csvSeparator}${nameOfTag} ""'#'""${csvSeparator}${nameOfTag}(${countOfTag}) #g"" \
      -e 's#'"" ${nameOfTag}${csvSeparator}""'#'"" ${nameOfTag}(${countOfTag})${csvSeparator}#g"" \
      -e 's#'"" ${nameOfTag} ""'#'"" ${nameOfTag}(${countOfTag}) #g"" \
      ${outputFile}
done
)

#hiç kullanılmayan firewall tag'lerinin yanına (0) ekleyelim.
for nameOfTag in $(awk -F ""${csvSeparator}"" '{print$6""\n""$8}' ""${outputFile}"" | grep -vF '(' | sort -u)
do
  #eğer kural bir önceki döngüde isminin yanına (N) şekilnde bir ekleme yapılmadıysa (0) eklenecek
  #sed kuralları için bir önceki döngüdeki expressionların benzeri kullanıldı
  sed -i \
      -e 's#'""${csvSeparator}${nameOfTag}${csvSeparator}""'#'""${csvSeparator}${nameOfTag}(0)${csvSeparator}#g"" \
      -e 's#'""${csvSeparator}${nameOfTag} ""'#'""${csvSeparator}${nameOfTag}(0) #g"" \
      -e 's#'"" ${nameOfTag}${csvSeparator}""'#'"" ${nameOfTag}(0)${csvSeparator}#g"" \
      -e 's#'"" ${nameOfTag} ""'#'"" ${nameOfTag}(0) #g"" \
      ${outputFile}
done]]> Firewall Kurallarını Disable Etme Usage: ./disableGcpFirewallRules.sh project-name rule-list-file bash disableGcpFirewallRules.sh true true &2
  exit 1
fi

#dosya var mı kontrol edelim
#kural isimleri bu dosyanın ierisinde whitespace ile ayrılmalı (space, tab, newline vs.)
if [[ ! -f ""${inputFile}"" ]]
then
  echo ""Rule list file is not exist"" >&2
  exit 1
fi

#kuralları tek tek okuyalım
for ruleName in $(cat ""${inputFile}"")
do
  #her kural için disable attr ile update edelim
  gcloud compute firewall-rules update ""${ruleName}"" --disabled --project=""${projectName}""
done]]> Firewall Source IP Kısıtlama Usage: ./setSourceFilterOfGcpFirewallRules.sh project-name rule-list-file bash setSourceFilterOfGcpFirewallRules.sh true true &2
  exit 1
fi

#dosya var mı kontrol edelim
#kural isimleri bu dosyanın ierisinde whitespace ile ayrılmalı (space, tab, newline vs.)
if [[ ! -f ""${inputFile}"" ]]
then
  echo ""Rule list file is not exist"" >&2
  exit 1
fi

#kuralları tek tek okuyalım
for ruleName in $(cat ""${inputFile}"")
do
  #her kural için src range update edelim
  gcloud compute firewall-rules update ""${ruleName}"" --source-ranges='10.0.0.0/8' --project=""${projectName}""
done]]>","{'title': 'GCP Firewall Analiz ve Operasyonları', 'id': '55902549', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=55902549'}"
"softtech-rally projesi altında birbirine girmiş bir yapımız var. Bu eskiyen yapıyı gcp üzerinde farklı projelere ve yeni standartlara geçiş yapılacak şekilde bölmek yönünde bir planlama yapılmıştır. Proje yapılandırması şu şekilde olacaktır: Yetkilendirme Softtech AD ile GCP arasında grup ve user senkronlama BT Destek ile birlikte yapıldı. Yetkilendirmeler gcp-platform-admins grubu ile yapılacak. Kişi yetkilendirmesi olmayacak. Instance'lara eklenecek ssh-key'ler makineye değil, user'a eklenecek. Manual key eklemesi yapılmayacak Network Tüm network vpc-platform isimli shared vpc ile yönetilecek. Bu network ""mgmt-platform"" isimli gcp projesi üzerinde yaratılacak. ""mgmt-platform"" üzerinde yalnızca network servisleri ile ilgili yapılandırma ve vm'ler bulunacak. Örn: openvpn, vpc için haproxy vs. Servis seviyesindeki işlemler için bu proje kullanılmayacak, ayrıca bir ""infra-platform"" projesi yaratılacak. Tüm ip'ler private olacak, vm ya da k8s üzerinde doğrudan public ip kullanımı olmayacak. Bu kurala tek istisna mgmt-platform projesi altında olabilir. Örn: openvpn server. Puplic servis olması gerekiyorsa External Load Balancer üzerinde proxy yapılacak GCP üzerinde ""platform"" isminde bir folder yaratıldı. Tüm projeler bu dizin altına toplanacak ve yönetimi bizde olacak. Temel projeler: mgmt-platform : GCP network altyapısı ve erişim yöntemleri ile ilgili yapılandırmalar olacak shared vpc, firewall, openvpn, site2site vpn tanımları, nat gateways, nat proxies etc. infra-platform : Tüm platform tarafından ortak kullanılacak infra servisleri burada yaratılacak dns, custom load balancers etc. TBD... Yukarıdakilere ek olarak mikroservislerin mr ve int ortamları ile product, release ve solution'lar için ayrı projeler yaratılması planlanmaktadır. Ayrıca demo amacıyla internete açık deploy yapılması durumunda proje özelinde küçük clusterler da yaratılacaktır.","{'title': 'Altyapı Değişikliği Tasarımı ve Planları', 'id': '55902564', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=55902564'}"
"Google cloud üzerindeki network'lerimiz bunlardır: Network Project VPC Name Subnet Name Subnet CIDR Resource Project Description mgmt.platform vpc-platform vpn-vm-mgmt-platform 10.32.1.16/28 mgmt-platform Gcloud Openvpn sunucu (vpn.platform.softtech.com.tr) mgmt.platform vpc-platform vpnclients-vm-mgmt-platform 10.32.4.0/22 mgmt-platform Openvpn için rezerve. mgmt.platform vpc-platform ca-vm-infra-platform 10.32.1.32/28 infra-platform Step-ca sunucusu (ca.platform.softtech) mgmt.platform vpc-platform dns-vm-infra-platform 10.32.1.80/28 infra-platform Azure için DNS sunucuları (dns0X.platform.softtech) mgmt.platform vpc-platform master-artemis-ocp-platform-softtech 10.32.8.0/24 artemis-ocp-platform-softtech Artemis Openshift Master makineları mgmt.platform vpc-platform worker-artemis-ocp-platform-softtech 10.32.9.0/24 artemis-ocp-platform-softtech Artemis Openshift Worker makineleri softtech-rally vpc-network subnet-10-223-2-0 subnet-10-223-4-0-22 subnet-10-223-8-0-23 10.223.2.0/23 10.62.128.0/19 10.62.0.0/17 10.61.0.0/17 10.61.128.0/19 10.60.128.0/19 10.60.0.0/17 10.65.128.0/19 10.65.0.0/17 10.223.4.0/22 10.68.0.0/16 10.66.0.0/15 10.223.8.0/23 10.70.0.0/15 10.72.0.0/16 softtech-rally Eski projelerin ve clusterların subnetleri. Yeni açılacak servisler için altyapıda artık platform.softtech uzantısı altına yapılandırıyoruz ve 10.32.X.Y altında yeterli en küçük subnet boyutu neyse (min /28) oluşturuyoruz. Böylece birden fazla vm olacak cluster'lar için de imkan sağlıyoruz. Firewall izinleri de bu subnet'ler üzerinden veriyoruz. Eski ortamlarda yeniden tasarıma girmemiş sistemleri ise yine softtech-rally altında 10.223.2.0/23 network'ü altında yaratabiliriz. İki vpc arasında peering mevcut. Azure üzerinde ise aşağıdaki şekilde network yapılandırmamız mevcut: Network Subs Vnet Name Subnet Name Subnet CIDR Description subs-softtech-plateau-core vnet-softtech-hub (10.96.2.0/23) snet-management 10.96.2.0/28 Management subnet subs-softtech-plateau-core vnet-softtech-hub (10.96.2.0/23) GatewaySubnet 10.96.3.0/26 Gateway subnet subs-softtech-plateau-core vnet-softtech-hub (10.96.2.0/23) snet-dns-in 10.96.2.16/28 Private DNS resolver inbound için oluşturulan subnet subs-softtech-plateau-core vnet-softtech-hub (10.96.2.0/23) snet-dns-out 10.96.2.32/28 Private DNS resolver outbound için oluşturulan subnet subs-softtech-plateau-dev vnet-plateau-aks-dev-1 (10.190.16.0/20) snet-aks-svc 10.190.24.0/23 Kubernetes service network subs-softtech-plateau-dev vnet-plateau-aks-dev-1 (10.190.16.0/20) snet-vm 10.190.31.0/24 Azure vm'leri network (Github vb.) subs-softtech-plateau-dev vnet-plateau-aks-dev-1 (10.190.16.0/20) snet-aks-pod 10.190.16.0/21 Kubernetes pod network subs-softtech-plateau-dev vnet-plateau-aks-dev-1 (10.190.16.0/20) AzureBastionSubnet 10.190.26.0/26 Bastion network Azure üzerindeki iki netowrk arasında (vnet-softtech-hub ve vnet-plateau-aks-dev-1) peering var, Tüm network operasyonlarını (vpn, subnet vb.) core üzerinden yönetiyoruz. VPN bağlantısı peering üzerinden paylaşılıyor. GCP üzerindeki iki network arasında (vpc-platform ve vpc-network) peering var. Ancak vpn bağlantıları peering'ten paylaşılmadığı için her iki network ile ayrı ayrı vpn yapmak zorundayız müşteriler ve Azure için.","{'title': 'Network Map', 'id': '55902574', 'source': 'https://wiki.softtech.com.tr/display/SDO/Network+Map'}"
"Proje Yaratılması mgmt-platform projesi BTDESTEK tarafından yaratıldı. Proje yaratılırken ""us-central1"" bölgesi maliyet açısından seçildi. Shared VPC Network Yaratılması Diğer projelerde kullanılacak subnet'leri tek yerden yönetmek için shared olarak bir vpc network yaratılacak. Öncelikle projede shared vpc aktifleştirelim. gcloud compute shared-vpc enable mgmt-platform VPC network yaratalım. gcloud compute networks create vpc-platform --project=mgmt-platform --subnet-mode=custom --mtu=1460 --bgp-routing-mode=regional Cloud DNS Yapılandırması Zone yaratalım gcloud beta dns --project=mgmt-platform managed-zones create platform-softtech --description=""platform.softtech private dns zone"" --dns-name=""platform.softtech."" --visibility=""private"" --networks=""vpc-platform"" Public domain'lerde maskeleme yapacağımız kayıtlar olacak, bunlar için Response policy zone tanımlayalım. gcloud beta dns --project=mgmt-platform response-policies create vpc-platform-default-response-policy --description=""generic responce policy"" --networks=""vpc-platform"" Gerekli Servislerin Aktifleştirilmesi Kullanılacak google servisleri baştan aşağıdaki liste ile aktifleştirilebilir ya da ilerleyen adımlarda hata aldıkça parça parça aktifleştirilebilir. gcloud services enable autoscaling.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com cloudapis.googleapis.com clouddebugger.googleapis.com cloudtrace.googleapis.com compute.googleapis.com container.googleapis.com containerfilesystem.googleapis.com containerregistry.googleapis.com datastore.googleapis.com deploymentmanager.googleapis.com dns.googleapis.com iam.googleapis.com iamcredentials.googleapis.com iap.googleapis.com logging.googleapis.com monitoring.googleapis.com networkmanagement.googleapis.com oslogin.googleapis.com pubsub.googleapis.com runtimeconfig.googleapis.com servicemanagement.googleapis.com serviceusage.googleapis.com sql-component.googleapis.com storage-api.googleapis.com storage-component.googleapis.com storage.googleapis.com --project=mgmt-platform Snapshot Schedule Yaratılması VM'leri yaratmadan önce günlük, haftalık ve aylık snapshot schedule yaratalım. gcloud compute resource-policies create snapshot-schedule platform-daily-15 --project=mgmt-platform --region=us-central1 --max-retention-days=15 --on-source-disk-delete=apply-retention-policy --daily-schedule --start-time=01:00 --storage-location=us-central1 --description=""platform daily snapshot policy, delete after 15 days"" gcloud compute resource-policies create snapshot-schedule platform-weekly-8 --project=mgmt-platform --region=us-central1 --max-retention-days=56 --on-source-disk-delete=apply-retention-policy --weekly-schedule=monday --start-time=01:00 --storage-location=us-central1 --description=""platform weekly snapshot policy, delete after 8 weeks"" Proje Ayarları Serial Console Enable Tüm vmler için serial console bağlantısını proje genelinde aktifleştirelim. gcloud compute project-info add-metadata --project=mgmt-platform --metadata serial-port-enable=TRUE VM Erişimlerini Hazırlama Öncelikle tüm vm'lerde varsayılan olarak os-login enable edelim. Bu VM'lerin gcloud kullanıcıları kullanılarak login olunmasını sağlıyor. gcloud compute project-info add-metadata --project=mgmt-platform --metadata enable-oslogin=TRUE Her kişi kendi ssh key'ini projeye bu şekilde ekleyebilir. Tek tek vm bazlı eklemeye ihtiyaç duymuyoruz. gcloud compute os-login ssh-keys add --project=mgmt-platform --ttl 0 --key-file=.ssh/id_rsa.pub VPN Gateway (S2S VPN) Yaratma Tek bir VPN Gateway yaratıp aynı ip üzerinden farklı farklı tüneller yaratacağız. O yüzden bu gateway'i yaratalım gcloud compute target-vpn-gateways create vpngateway-mgmt-platform --project=mgmt-platform --description=s2s\ vpn\ gateway\ for\ vpc-platform\ in\ mgmt-platform --region=us-central1 --network=vpc-platform

gcloud compute forwarding-rules create vpngateway-mgmt-platform-rule-esp --project=mgmt-platform --region=us-central1 --address=35.232.31.161 --ip-protocol=ESP --target-vpn-gateway=vpngateway-mgmt-platform

gcloud compute forwarding-rules create vpngateway-mgmt-platform-rule-udp500 --project=mgmt-platform --region=us-central1 --address=35.232.31.161 --ip-protocol=UDP --ports=500 --target-vpn-gateway=vpngateway-mgmt-platform

gcloud compute forwarding-rules create vpngateway-mgmt-platform-rule-udp4500 --project=mgmt-platform --region=us-central1 --address=35.232.31.161 --ip-protocol=UDP --ports=4500 --target-vpn-gateway=vpngateway-mgmt-platform Firewall Varsayılanda ingress firewall kuralı deny olarak gelmektedir. Bu nedenle kural olmadığında izin yok demektir. Ancak egress trafiği varsayılanda allow olduğu için firewall kural setinin en sonuna (priority 65000) tüm egress trafiği yasaklayacak bir genel kural girelim. Instance eğer herhangi bir network trafiği yapacaksa firewall kuralı eklenmesi gerekecek. gcloud compute --project=mgmt-platform firewall-rules create platform-egress-all-deny --description=""Deny all egress traffic"" --direction=EGRESS --priority=65000 --network=vpc-platform --action=DENY --rules=all --destination-ranges=0.0.0.0/0 Kurulum/Debug esnasında genel firewall kuralı bu şekilde eklenebilir ancak sürekli açık tutulmamalı: gcloud compute --project=mgmt-platform firewall-rules create debug-temp-access-for-$(gcloud config get-value account | sed -e 's/\./-/g' -e 's/@/-/g') --description=""gecici kisisel erisim, lutfen hemen silin"" --direction=INGRESS --priority=100 --network=vpc-platform --action=ALLOW --rules=all --source-ranges=$(curl ifconfig.me)/32 Sunuculara ssh ile bağlantı için kullanıcı adındaki noktalar alt çizgi ile değiştirilmelidir. Örnek: baransel_bagci@softtech_com_tr softtech-rally Backward Compatibility VPC Peer softtech-rally projesi altındaki kaynaklara geçiş süresince ulaşılabilmesi için iki network arasında peer kurulur. Öncelikle mgmt-platform altından peer yaratılır: Networking altından VPC network -> VPC network peering seçilir Create Peering Connection seçilir Name vpc-platform-to-softtech-rally-vpc-network Your VPC network vpc-platform Peered VPC network In another project Project ID softtech-rally VPC network name vpc-network Exchange custom routes none Exchange subnet routes with public IP none Aynı işlem softtech-rally altından da yapılır: Networking altından VPC network -> VPC network peering seçilir Create Peering Connection seçilir Name vpc-platform-to-softtech-rally-vpc-network Your VPC network vpc-network Peered VPC network In another project Project ID mgmt-platform VPC network name vpc-platform Exchange custom routes none Exchange subnet routes with public IP none Cloud DNS Records softtech-rally projesi altında açılan static DNS kayıtlarını geçici olarak burada responce policy olarak tanımlayalım. Özellikle zone açmıyoruz, geçiş işlemi bittikten sonra rally ve plateau olan kayıtların silinmesi lazım. gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-dev-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.dev.rally.softtech"" --local-data=name=""*.dev.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.8.6""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-dev2-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.dev2.rally.softtech"" --local-data=name=""*.dev2.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.3.101""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-github-test-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.github-test.rally.softtech"" --local-data=name=""*.github-test.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.165""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-github-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.github.rally.softtech"" --local-data=name=""*.github.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.3.17""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-isleasing-plateau-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.isleasing.plateau.softtech"" --local-data=name=""*.isleasing.plateau.softtech."",type=""A"",ttl=300,rrdatas=""10.223.8.6""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-plateau-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.plateau.softtech"" --local-data=name=""*.plateau.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.211""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-plateaux-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.plateaux.softtech"" --local-data=name=""*.plateaux.softtech."",type=""A"",ttl=300,rrdatas=""10.223.4.6""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.rally.softtech"" --local-data=name=""*.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.100""

gcloud beta dns --project=mgmt-platform response-policies rules create wildcard-util-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""*.util.rally.softtech"" --local-data=name=""*.util.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.100""

gcloud beta dns --project=mgmt-platform response-policies rules create ahepowerdev-anadoluhayat-com-tr --response-policy=""vpc-platform-default-response-policy"" --dns-name=""ahepowerdev.anadoluhayat.com.tr"" --local-data=name=""ahepowerdev.anadoluhayat.com.tr."",type=""A"",ttl=300,rrdatas=""172.16.4.12""

gcloud beta dns --project=mgmt-platform response-policies rules create awx-plateau-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""awx.plateau.softtech"" --local-data=name=""awx.plateau.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.103""

gcloud beta dns --project=mgmt-platform response-policies rules create cyberark-turib-com-tr --response-policy=""vpc-platform-default-response-policy"" --dns-name=""cyberark.turib.com.tr"" --local-data=name=""cyberark.turib.com.tr."",type=""A"",ttl=300,rrdatas=""10.50.150.35""

gcloud beta dns --project=mgmt-platform response-policies rules create dev-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""dev.rally.softtech"" --local-data=name=""dev.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.8.6""

gcloud beta dns --project=mgmt-platform response-policies rules create discourse-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""discourse.rally.softtech"" --local-data=name=""discourse.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.57""

gcloud beta dns --project=mgmt-platform response-policies rules create editor-quick-plateau-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""editor.quick.plateau.softtech"" --local-data=name=""editor.quick.plateau.softtech."",type=""A"",ttl=300,rrdatas=""10.223.3.124""

gcloud beta dns --project=mgmt-platform response-policies rules create github-test-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""github-test.rally.softtech"" --local-data=name=""github-test.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.165""

gcloud beta dns --project=mgmt-platform response-policies rules create github-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""github.rally.softtech"" --local-data=name=""github.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.3.17""

gcloud beta dns --project=mgmt-platform response-policies rules create schemainfo-ngsp-dev-rally-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""schemainfo.ngsp.dev.rally.softtech"" --local-data=name=""schemainfo.ngsp.dev.rally.softtech."",type=""A"",ttl=300,rrdatas=""10.223.3.100""

gcloud beta dns --project=mgmt-platform response-policies rules create turisrancher-turib-com-tr --response-policy=""vpc-platform-default-response-policy"" --dns-name=""turisrancher.turib.com.tr"" --local-data=name=""turisrancher.turib.com.tr."",type=""A"",ttl=300,rrdatas=""10.10.47.51""

gcloud beta dns --project=mgmt-platform response-policies rules create ui-isleasing-plateau-softtech --response-policy=""vpc-platform-default-response-policy"" --dns-name=""ui.isleasing.plateau.softtech"" --local-data=name=""ui.isleasing.plateau.softtech."",type=""A"",ttl=300,rrdatas=""10.223.2.211""","{'title': 'mgmt-platform', 'id': '55902581', 'source': 'https://wiki.softtech.com.tr/display/SDO/mgmt-platform'}"
"OpenVPN Access Server Kurulumu Platform Operasyonları Subnet Yaratılması VPN için kullanılacak iki subnet bulunuyor: 10.32.1.16/28 : vpn-vm-mgmt-platform : OpenVPN sunucusunun bulunacağı subnet 10.32.4.0/22 : vpnclients-vm-mgmt-platform : VPN istemcilere dağıtılacak IP subnet Öncelikle subnet'ler yaratılır: gcloud compute networks subnets create vpn-vm-mgmt-platform --project=mgmt-platform --range=10.32.1.16/28 --network=vpc-platform --region=us-central1
gcloud compute networks subnets create vpnclients-vm-mgmt-platform --project=mgmt-platform --range=10.32.4.0/22 --network=vpc-platform --region=us-central1 VM Yaratılması Internal IP adresini yaratalım: gcloud compute addresses create openvpn1-vpn-vm-mgmt-platform --description=""openvpn access server private ipv4 address"" --subnet=vpn-vm-mgmt-platform --project=mgmt-platform Internal service dns kaydımızı yaratalım gcloud dns --project=mgmt-platform record-sets create vpn.platform.softtech. --rrdatas=10.32.1.18 --type=A --ttl=300 --zone=platform-softtech External IP adresini yaratalım: gcloud compute addresses create vpn-platform-softtech-com-tr --project=mgmt-platform --description=""openvpn access server public ipv4 address"" --network-tier=STANDARD --region=us-central1 Yukarıdaki komut sonucunda oluşan public ip adresi için vpn.platform.softtech.com.tr olarak DNS kaydı BT Destek tarafından oluşturulur. Bu adrese vpn'e bağlı iken private ip ile gelinebilmesi için Cloud Dns üzerinde responce policy rule yaratılır: gcloud beta dns --project=mgmt-platform response-policies rules create vpn-platform-softtech-com-tr --response-policy=""vpc-platform-default-response-policy"" --dns-name=""vpn.platform.softtech.com.tr"" --local-data=name=""vpn.platform.softtech.com.tr."",type=""A"",ttl=300,rrdatas=""10.32.1.18"" VM instance için standart bir kural seti belirlendi. Bu standarta göre instance yaratalım: gcloud compute instances create openvpn1-vpn-vm-mgmt-platform --project=mgmt-platform --zone=us-central1-a --description=Openvpn\ Access\ Server --machine-type=e2-small --network-interface=address=35.208.140.93,network-tier=STANDARD,nic-type=GVNIC,private-network-ip=10.32.1.18,subnet=vpn-vm-mgmt-platform --metadata=enable-oslogin=true --can-ip-forward --maintenance-policy=MIGRATE --no-service-account --no-scopes --tags=openvpn1-vpn-vm-mgmt-platform,vpn-vm-mgmt-platform --create-disk=auto-delete=yes,boot=yes,device-name=openvpn1-vpn-vm-mgmt-platform,disk-resource-policy=projects/mgmt-platform/regions/us-central1/resourcePolicies/softtech-daily,image=projects/ubuntu-os-pro-cloud/global/images/ubuntu-pro-2004-focal-v20220204,mode=rw,size=20,type=projects/mgmt-platform/zones/us-central1-a/diskTypes/pd-balanced --shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --deletion-protection Firewall İzinleri ingress tcp-443 https izni eklenir: gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-ingress-tcp443 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:443 --source-ranges=0.0.0.0/0 --target-tags=openvpn1-vpn-vm-mgmt-platform BT Destek admin panel erişimi için softtech içindeki BT Destek yönetimindeki jump server ip'sine izin verilir gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-ingress-tcp943 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:943 --source-ranges=90.158.30.41/32 --target-tags=openvpn1-vpn-vm-mgmt-platform ingress udp-1194 vpn izni eklenir: gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-ingress-udp1194 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=udp:1194 --source-ranges=0.0.0.0/0 --target-tags=openvpn1-vpn-vm-mgmt-platform Let's encrypt ile sertifika yenileme işleminde domain doğrulaması için 80 portu üzerinden standalone certbot çalışması gerekiyor, bunun için 80 portu açılır gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-ingress-tcp80 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:80 --source-ranges=0.0.0.0/0 --target-tags=openvpn1-vpn-vm-mgmt-platform Let's encrypt api sunucularına sertifika yenileme isteklerini gönderebilmek için egress tcp-443 portunun da açılması gerekiyor gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-egress-tcp443 --direction=EGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:443 --destination-ranges=0.0.0.0/0 --target-tags=openvpn1-vpn-vm-mgmt-platform Ldap ile parola doğrulama yapılması için egress izni verilir gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-egress-tcp636 --direction=EGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:636 --destination-ranges=90.158.30.20/32 --target-tags=openvpn1-vpn-vm-mgmt-platform VPN sunucusunun internal tüm networklere erişebilmesi için egress izni verilir gcloud compute --project=mgmt-platform firewall-rules create openvpn1-vpn-vm-mgmt-platform-egress-all --direction=EGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=all --destination-ranges=10.0.0.0/8 --target-tags=openvpn1-vpn-vm-mgmt-platform VM Operasyonları OS Hazırlıkları Host işlemleri yapılır ve güncellenir sudo apt update -y
sudo apt upgrade -y
sudo apt autoremove -y Install Repo ve openvpn paketleri kurulur: sudo apt update && sudo apt -y install ca-certificates wget net-tools gnupg
wget -qO - https://as-repository.openvpn.net/as-repo-public.gpg | sudo apt-key add -
sudo sh -c 'echo ""deb http://as-repository.openvpn.net/as/debian focal main"" > /etc/apt/sources.list.d/openvpn-as-repo.list'
sudo apt update && sudo apt -y install openvpn-as Init with GCP Settings GCP özel ayarlar ile Openvpn'i tekrar yapılandıralım. Buranın sonunda openvpn kullanıcısı ile bir rastgele parola oluşturacak, bununla sunucuya bağlanabilir olacağız. sudo /usr/local/openvpn_as/bin/ovpn-init --force --batch --gcp Çıktı olarak şu şekilde bir sonuç verecektir: Admin UI: https://35.208.140.93:943/admin Client UI: https://35.208.140.93:943/ To login please use the ""openvpn"" account with ""*************"" password. Admin Panel Settings Configuration Network Settings TLS Settings TLS options for Web Server TLS 1.2 VPN Server Hostname or IP Address vpn.platform.softtech.com.tr Interface and IP Address ens4: 10.32.1.18 Web Service forwarding settings Admin Web Server forwarding Disable Admin Web Server Interface and IP Address ens4: 10.32.1.18 VPN Settings Dynamic IP Address Network 172.27.240.0/28 Group Default IP Address Network (Optional) 10.32.4.0/22 Routing Should VPN clients have access to private subnets Yes, using NAT Specify the private subnets to which all clients should be given access (one per line): 169.254.169.254/32 DNS Settings Have clients use the same DNS servers as the Access Server host Enable DNS resolution zones platform.softtech,platform.softtech.com.tr,rally.softtech,plateau.softtech,plateaux.softtech,anadoluhayat.com.tr,turib.com.tr Advanced VPN Settings Multiple Sessions per User Enable TLS Control Channel Security tls-auth CWS Settings Customize Client Web Server UI Desktop Clients OpenVPN Connect v2 for Windows Disable OpenVPN Connect v2 for Mac OS X Disable Linux Disable Mobile Clients iOS Disable Android Disable Profiles server-locked profile Enable user-locked profile Disable autologin profile Disable Change Password Button Allows Users to change their own password Disable User Management Group Permissions New Group gcp-platform-admins Admin Enable Access Control 10.32.0.0/11
10.223.2.0/23
10.223.4.0/22
10.223.8.0/23
10.200.200.0/23
10.10.46.0/23
10.50.150.0/24 New Group gcp-platform-all-users Access Control 10.32.1.18/32:tcp/443
10.223.2.0/23
10.223.4.0/22
10.223.8.0/23
10.200.200.0/23
10.10.46.0/23
10.50.150.0/24 Default Group Permissions to use for any User not in any Group: gcp-platform-admins Authentication General LDAP Enable Google Authenticator Multi-Factor Authentication Enable LDAP LDAP Settings Allow LDAP authentication for assigned users and groups Enable Primary server stdc01.softtech.local Use SSL to connect to LDAP servers Enable Credentials for Initial Bind Use these credentials Enable Bind DN CN=vpn-platform-bind-user,OU=Application Users,OU=Softtech,DC=softtech,DC=local Password ***************** Base DN for User Entries OU=Softtech,DC=softtech,DC=local Username Attribute sAMAccountName Additional LDAP Requirement: (Advanced) memberOf=CN=GoogleCloudUsers,OU=Application Users,OU=Softtech,DC=softtech,DC=local Sertifika Ayarları Softtech DC root ca sertifikası sunucuya trusted root olarak eklenir. cat <<STEOF > /usr/local/share/ca-certificates/stdc-2019-2024.crt
-----BEGIN CERTIFICATE-----
MIIDdzCCAl+gAwIBAgIQcZMGB0M5GJ5HHer7jHQcWTANBgkqhkiG9w0BAQsFADBO
MRUwEwYKCZImiZPyLGQBGRYFbG9jYWwxGDAWBgoJkiaJk/IsZAEZFghzb2Z0dGVj
aDEbMBkGA1UEAxMSc29mdHRlY2gtU1REQzAyLUNBMB4XDTE5MDQxMTEwNDAzM1oX
DTI0MDQxMTEwNTAzM1owTjEVMBMGCgmSJomT8ixkARkWBWxvY2FsMRgwFgYKCZIm
iZPyLGQBGRYIc29mdHRlY2gxGzAZBgNVBAMTEnNvZnR0ZWNoLVNUREMwMi1DQTCC
ASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKo/SVvEruKKxQOIBo6zJFBd
TAxASxHFfeyF/nZJD6axjnwbd8R+he71fKt+nEKMkYebBrU7//q1gExBTJCKLM81
Norv9+ACCd/wFBG6IDltM4SFLWiZTVDsmDqlyUns7pIuRl/nQOSC+pWuoUQaMcAl
xx7rmI2wmcDFqxoX2284gtisqlcGR+JfELkyykiJQtRpzLWwTfduSd6vZ+ckfa1v
evOC3Nu5hqRYuzWrL81wrjW8OirmBlbJ771SiLx5vLsKXI0jJKHdxFl6L4LEuqyf
B3y0gcHS9pjf/FXIcXO977cKVuJFrBQSWVYqyj2l7flz7PVN00+4hOUsINFJDdUC
AwEAAaNRME8wCwYDVR0PBAQDAgGGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYE
FJwMLpb3Cd6cywj9nQUqN2VjPuH8MBAGCSsGAQQBgjcVAQQDAgEAMA0GCSqGSIb3
DQEBCwUAA4IBAQBsrFgHp1fprPcTOMzT4DskzFhe9zd1ip8s1qegvm5w2OIyOpq3
Pwoix0FnTklPgKajChC2haqV7s9UID1crlarzumQ2gGhCc15lZ5yRyoECntdtCVd
2+SLwHMJCFad6l6DOsH27HA3EKZPucqAFFm/ZxNnXPzNfkCvXB+IRpZ6LW0PpI5+
akjbP5b7pIvPRvH6jF8b2Z/oycV2tAtWlt1IgMfVgM9WZK+gWJZB1s5RL2jJh5ll
z4n54VhWrlwfyoFyVRvF6v7gjTWTxyEfiISjRcsko+1k7zdOrY+DEv+0FMcS1QCh
TVUDAZOCrwtR4tqk5phamyBYhVJb3tRTaF0F
-----END CERTIFICATE-----
STEOF
update-ca-certificates DC sunucularına verilen public nat ip adresi host dosyasına eklenir, böylece sertifikadaki isim ile eşleşmesi sağlanır. sed -i 's/^90.158.30.20 .*/90.158.30.20 stdc01.softtech.local stdc02.softtech.local/' /etc/hosts Root CA openvpn ldap ayarına trusted olması için eklenir. sacli --key ""auth.ldap.0.ssl_verify"" --value demand ConfigPut
sacli --key ""auth.ldap.0.ssl_ca_cert"" --value /usr/local/share/ca-certificates/stdc-2019-2024.crt ConfigPut
sacli start AD Grup Senkronizasyonu AD grup üyeliklerinin login esnasında otomatik eklenmesi için hook script kullanılacak mkdir -p externalFiles/{sources,modified}
wget https://swupdate.openvpn.net/scripts/post_auth_ldap_autologin_dbsave.py -O externalFiles/sources/ldapGroupHook-$(date +%FT%T).py Orjinal script'te aşağıdaki değişikliklerin uygulanmış hali modified dizini altına yerleştirilir diff externalFiles/{sources,modified}/ldapGroupHook-2022-02-23* Output: 47c47
<     group = """"
---
>     group = ""gcp-platform-all-users""
82,89c82,83
<                 if 'Administrators' in ldap_groups:
<                     group = ""admin""
<                 elif 'Sales' in ldap_groups:
<                     group = ""sales""
<                 elif 'Finance' in ldap_groups:
<                     group = ""finance""
<                 elif 'Engineering' in ldap_groups:
<                     group = ""engineering""
---
>                 if 'gcp-platform-admins' in ldap_groups:
>                     group = ""gcp-platform-admins"" Bu script aşağıdaki şekilde Openvpn'e yüklenir: sacli -k auth.module.post_auth_script --value_file=/root/externalFiles/modified/ldapGroupHook-2022-02-23T12\:16\:18.py ConfigPut
sacli start SSL Sertifika Ayarları vpn.platform.softtech.com.tr olarak public erişimde let's encrpt üzerinden bir sertifika alınarak işlem yapılacak. Bunun için gerekli paket kurulur ve sertifika ilgili parametreler ile yaratılır snap install certbot --classic
/snap/bin/certbot certonly -n --standalone --preferred-challenges http --preferred-chain ""ISRG Root X1"" --agree-tos -m devopsekibi@softtech.com.tr -d vpn.platform.softtech.com.tr Yenilenen sertifikayı openvpn'e uygulayacak script hazırlanır cat <<'STEOF' > /usr/local/bin/openvpnCertRenew.sh
#!/bin/bash

publicDomain=""vpn.platform.softtech.com.tr""
certPath=""/etc/letsencrypt/live/${publicDomain}/cert.pem""
caPath=""/etc/letsencrypt/live/${publicDomain}/chain.pem""
keyPath=""/etc/letsencrypt/live/${publicDomain}/privkey.pem""

/usr/sbin/sacli --ca_bundle=${caPath} --cert=${certPath} --priv_key=${keyPath} TestWebCerts  | grep -vF -- '-----BEGIN '
[[ $? == 0 ]] || { echo ""Sertifikalar dogrulanamadi"" >&2; exit 1; }

/usr/sbin/sacli --key ""cs.priv_key"" --value_file ""${keyPath}"" ConfigPut \
&& \
/usr/sbin/sacli --key ""cs.cert"" --value_file ""${certPath}"" ConfigPut \
&& \
/usr/sbin/sacli --key ""cs.ca_bundle"" --value_file ""${caPath}"" ConfigPut

[[ $? == 0 ]] || { echo ""Sertifika configleri eklenemedi"" >&2; exit 1; }

/usr/sbin/sacli start

[[ $? == 0 ]] || { echo ""Openvpn baslatilamadi"" >&2; exit 1; }
STEOF

chmod +x /usr/local/bin/openvpnCertRenew.sh
/usr/local/bin/openvpnCertRenew.sh
echo 'renew_hook = /usr/local/bin/openvpnCertRenew.sh' >> /etc/letsencrypt/renewal/vpn.platform.softtech.com.tr.conf Disable Local Admin User GÜvenlik nedeniyle kurulum esnasında yaratılan openvpn kullanıcısının giriş yapmasını engelleyelim sacli --user ""openvpn"" --key ""prop_deny"" --value ""true"" UserPropPut
sacli --user ""openvpn"" RemoveLocalPassword
sacli start Not: Eğer ldap login ile bir problem olursa ilk komut değeri false yapılır ve ikinci komut da SetLocalPassword yapılarak yeni bir parola belirlenerek giriş yapılabilir. Session Limit Güvenlik nedeniyle 24 saat'ten fazla oturumun açık kalmasını istemiyoruz. Bu nedenle oturum sürelerini kısıtlayalım. Burada iki farklı değeri atamamız gerekiyor. session expire : Parola doğrulaması ile başlayan oturumun geçerlilik süresi tls refresh interval : Oturumun bağlantısının yenilenme aralığı Bu değerleri sırasıyla 64720 saniye (18 saat - 1 dakika) ve 1 saat olarak ayarlıyoruz. Böylece bir kişinin oturumu en kötü olasılıkla 19 saatin sonunda sonlanacaktır. sacli --key ""vpn.server.session_expire"" --value 64720 ConfigPut
sacli --key ""vpn.tls_refresh.interval"" --value 60 ConfigPut
sacli start","{'title': 'openvpn-vm-mgmt-platform', 'id': '55902585', 'source': 'https://wiki.softtech.com.tr/display/SDO/openvpn-vm-mgmt-platform'}"
"-- Spring boot uygulamalarının metriclerini Prometheus ile takip etmek için /actuator/prometheus endpoint’inin kod içerisinde gerekli tanımlarla aktif olması gerekir. -- https://turisrancher.turib.com.tr adresi üzerindeki Prometheus tarafından izlenebilmesi için ilgili namespace altında aşağıdaki yaml kullanılarak Service Monitor objesi oluşturduk. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: annotations: kubectl.kubernetes.io/last-applied-configuration : | {""apiVersion"":"" monitoring.coreos.com/v1"",""kind"":""ServiceMonitor"",""metadata"":{""annotations"":{""meta.helm.sh/release-name"":""rancher-monitoring"",""meta.helm.sh/release-namespace"":""cattle-monitoring-system""},""labels"":{""app"":""rancher-monitoring-node-exporter"",""app.kubernetes.io/instance"":""rancher-monitoring"",""app.kubernetes.io/managed-by"":""Helm"",""app.kubernetes.io/part-of"":""rancher-monitoring"",""app.kubernetes.io/version"":""100.1.0_up19.0.3"",""chart"":""rancher-monitoring-100.1.0_up19.0.3"",""heritage"":""Helm"",""release"":""rancher-monitoring""},""name"":""rally-microservices"",""namespace"":""cattle-monitoring-system""},""spec"":{""endpoints"":[{""interval"":""10s"",""path"":""/actuator/prometheus"",""port"":""api""}],""jobLabel"":""rally-microservices"",""namespaceSelector"":{""any"":true},""selector"":{""matchLabels"":{""rally"":""microservice ""}}}} meta.helm.sh/release-name: rancher-monitoring meta.helm.sh/release-namespace: cattle-monitoring-system creationTimestamp: ""2022-06-13T15:40:40Z"" generation: 1 labels: app: rancher-monitoring-node-exporter app.kubernetes.io/instance : rancher-monitoring app.kubernetes.io/managed-by : Helm app.kubernetes.io/part-of : rancher-monitoring app.kubernetes.io/version : 100.1.0_up19.0.3 chart: rancher-monitoring-100.1.0_up19.0.3 heritage: Helm release: rancher-monitoring name: rally-microservices namespace: cattle-monitoring-system resourceVersion: ""70877853"" uid: 1268226a-1d46-4d55-a800-3d5d91f82be2 spec: endpoints: - interval: 10s path: /actuator/prometheus port: api jobLabel: rally-microservices namespaceSelector: any: true selector: matchLabels: rally: microservice -- Grafana dashboard için http://grafana.plateaux.softtech/dashboards adresindeki Rally uygulama metriclerina ait dashboardların exportu alınarak Turis ortamdaki https://turisrancher.turib.com.tr/ grafana uygulamasına import edilir. Aşağıdaki user password bilgileri ile giriş yapılır. http://grafana.plateaux.softtech/dashboards – rallyuser/rallyuser https://turisrancher.turib.com.tr/ – admin/prom-operator","{'title': 'Spring Boot Uygulamasının Metriclerini Prometheus ile Takip Etmek', 'id': '60260470', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=60260470'}"
"Linux komut satırından OpenSSL kullanarak sertifikanın konu alanına giren değerler sorulmadan CSR (Certificate Signing Request) dosyasını aşağıdaki yönergelerle oluşturabiliriz. Bize iletilen domain ve parametre bilgilerine göre aşağıdaki alanlarda düzenleme yapılarak csr-config.txt dosyası oluşturulur. [ req ] prompt = no default_bits = 2048 distinguished_name = req_distinguished_name req_extensions = req_ext [ req_distinguished_name ] C=TR ST=Istanbul L=Levent O=Softtech Software Technologies OU=DevOps and Platform Management CN= imeceplatform.com [ req_ext ] subjectAltName = @alt_names [alt_names] DNS.1 = imeceplatform.com DNS.2 = www.imeceplatform.com DNS.3 = iamauthorization.imeceplatform.com DNS.4 = ui.imeceplatform.com DNS.5 = internal-ui.imeceplatform.com DNS.6 = fafsmartfarm.imeceplatform.com DNS.7 = apigateway.imeceplatform.com DNS.8 = identity-provider.imeceplatform.com Aşağıdaki komut ile ""imeceplatform.key"" ve ""imeceplatform.csr"" dosyaları oluşturulmuş olur. # openssl req -nodes -newkey rsa:2048 -keyout imeceplatform.key -out imeceplatform.csr -config csr-config.txt Oluşturulan CSR dosyası text olarak decode eder. # openssl req -text -in imeceplatform.csr -noout Aşağıdaki linkler referans alınabilir. https://www.shellhacks.com/create-csr-openssl-without-prompt-non-interactive/","{'title': 'OpenSSL Kullanarak CSR Sertifika Dosyası Oluşturma', 'id': '62394292', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62394292'}"
Gelen talepler doğrultusunda bazı paketler Checkmarx tarama süreci kapsamından istisna edilebilir. Aşağıdaki endpoint kullanılarak paketi TEMP_PASSIVE statusune alabilir ve Checkmarx tarama süreçlerinden çıkarabiliriz; https://devops.softtech/backend/api/CxSecure/MakeProjectTemporaryPassive?aciCode=ACI38625 Endpoint i çağıdabilmek için tarayıcıya ya da Postman e auth token set edilmesi gerekiyor.Devops.softtech e giriş yapıp  F12 ile araçları açıp Cookie Editor den token a erişebilirsiniz. Projeyi tekrar aktif etmek için Aşağıdaki endpoint kullanılarak paketi ACTIVATE statusune alabilir ve Checkmarx tarama süreçlerine tekrar dahil edebilirsiniz.Yine Token set etmeniz gerekiyor. https://devops.softtech/backend/api/CxSecure/ActivateProject?aciCode=ACI38625image2022-9-7_17-16-49.png,"{'title': 'Devops.Softtech aracılığı ile paketleri Checkmarx tarama sürecinden çıkarma veya tekrar dahil etme.', 'id': '62394874', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62394874'}"
,"{'title': 'DevOps Softtech', 'id': '62395240', 'source': 'https://wiki.softtech.com.tr/display/SDO/DevOps+Softtech'}"
1-Genom da bulunan Checkmarx Trend Değeri Postman Aracılığı ile Kontrol Edilir. https://genomapi.kube.isbank/api/v3/package 1.A. Checkmarx Trend Değerinin 3 olması durumu; 1.A.1. Genom sorgusunda paketin repo bilgisi olup olmadığı kontrol edilmeli.Paketin Repo bilgisi yok ise Repo bilgisinin eksik olduğu ekibe iletilmeli ve güncellemeleri gerektiği söylenmelidir.Repo bilgisi güncellendikten sonra Background processlerimiz Repo bilgisini otomatik olarak güncelleyecektir.Ama eğer ekibin acil olarak sürüme çıkması gerekiyorsa repo bilgisi manuel olarak güncellenebilir.Bu durumda aşağıdaki adımlar uygulanacaktır; 1.A.1.1. Repo bilgisi güncellendikten sonra repo kontrol edilir.Eğer repoda taranabilir kod bulunmuyorsa aşağıdaki  API çağrılarak paketin kapsam dışında bırakılması sağlanır. API çağrısı için Token bilgisi gerekiyor.Token ın nasıl set edileceği diğer yazılarda belirtildi. https://devops.softtech/backend/api/CxSecure/DeleteProjectFromCheckmarxAndInsertToExcluded?aciCode=ACI67519 1.A.1.2. Taranabilir kod var ise API YAZILACAK (Veritabanındaki ve Checkmarx uygulamasındaki Repo bilgisinin güncellenmesi sağlanacak) 1.A.2. Repo biligisi var ise yine taranabilir kod olup olmadığı kontrol edilmeli.Taranabilir kod yok ise yukarıdaki adımlar izlenmeli.Var ise ilgili ekiplere paket için tarama başlatmaları gerektiği söylenmeli. 1.B. Checkmarx Trend Değerinin 2 olması durumu; Bu durum paketin high bulgu seviyesinin threshold değeri veye bu değerin altında olmadığını belirtiyor.Bu durumda ilgili ekiplere high bulgu sayısının threshold değerinin altına getirilmesi gerektiği söylenmeli.image2022-9-15_11-42-56.pngimage2022-9-15_11-33-47.png,"{'title': 'XlRelease de Checkmarx Trend Değeri Hatası Alındığında Yapılması Gerekenler', 'id': '62395777', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62395777'}"
"1-Paketin Genom da bulunan sahiplik bilgileri kontrol edilmeli. https://genomapi.kube.isbank/api/v3/package 2-TouchR dan Kullanıcının bağlı olduğu direktörlük ve müdürlük bilgileri kontrol edilmeli. SuccessFactors 3-Yukarıdaki bilgilerin eşleşip eşleşmediği kontrol edilmeli.Paketin Genom da bulunan unit bilgisi kullanıcının Depertman bilgisi ile,subunit biligisi ise kullanıcının Birim bilgisi ile aynı olmalı.Karakter farkı bile olmamalı.Paketin görüntülenebilmesi için bu bilgiler eşleşmek zorunda.Bilgiler eşleşiyor ise aşadağı gibi veritabanından gerekli kontroller sağlanabilir. A.Paketin veritabanındaki sahiplik bilgileri kontrol edilir; SELECT TOP (1000) [PackageId] ,[Name] ,[UKCode] ,[Owner] ,[ACICode] ,[Version] ,[DirectorshipCode] FROM [DevOps].[dbo].[Package] where ACICode='ACI11502' Sorgu sonucundaki DirectorshipCode ile aşağıdaki sorgu çalıştırılır; SELECT TOP (1000) [DirectorshipMapId] ,[Name] ,[DirectorshipCode] ,[SimilarityRate] ,[DirectorshipMapCode] FROM [DevOps].[dbo].[DirectorshipMap] where DirectorshipCode='10000446' Devamında Employee tablosundan Kullanıcı bilgileri kontrol edilir.Sorgu sonucunda kullanıcının DirectorshipMapCode,DirectoshipCode ve DirectorateCode bilgilerinin eşleşip eşleşmediği kontrol edilir.Bilgilerin GENOM'da(paket için) ve TouchR'da (kullanıcı için) bulunan bilgiler ile de eşleşmesi gerekiyor.Eşleşmiyorsa gerekli güncelleme işlemleri yapılabilir. select * from Employee where RegistrationNumber=XXXimage2022-9-15_12-6-46.pngimage2022-9-15_12-4-29.pngimage2022-9-15_12-3-47.pngimage2022-9-15_12-0-34.pngimage2022-9-15_11-55-10.pngimage2022-9-15_11-51-54.png","{'title': 'Devops.Softtech Arayüzünde Kullanıcı Paketi Göremediğinde Yapılması Gerekenler', 'id': '62395789', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62395789'}"
"Checkmarx SAST,  kaynak kodun güvenlik zafiyetlerini derleme gerektirmeden,  OWASP güvenlik açıklarını tarayan, statik kod tarama aracıdır. Bank Projeleri (ACI kodu olan ) ve Softtech Projleri (Rally ismiyle sınıflandırılan)  için gerçekleştirilir ve sonuçları DevopsSofttech sayfasında gösterilir. Arka planda concurrent çalışan  job sayısı 3, ama bunlardan biri pasif .  Concurrent çalışan işlem sayısı 4.","{'title': 'Checkmarx', 'id': '62396857', 'source': 'https://wiki.softtech.com.tr/display/SDO/Checkmarx'}"
"1- Open jdk-11 indirilir. sudo apt update sudo && apt install openjdk-11-jdk 2- JAVA_HOME environment variable'ı ayaralanır. /usr/lib/jvm/ dizini altında java-1.x.x-openjdk bulunur ve not edilir. ""vim /etc/profile"" veya ""vim ~/.bashrc"" ile dosyaları açılıp dosyanın sonuna alttaki kısımlar eklenir. export JAVA_HOME= ""/usr/lib/jvm/ java-1.x.x-openjdk "" export PATH=$JAVA_HOME/bin:$PATH Düzenlediğiniz dosyaya göre ""source /etc/profile"" veya ""source ~/.bashrc"" yapılır. 3- https://wso2.com/api-manager/# sayfası üzerinden ""TRY IT NOW"" a tıklanarak açılan pencerede mail adresinizi girerek Zip Archive olarak dosyayı indiriniz. 4- <Dosya'nın dizini>/repository/conf/deployment.toml dosyasındaki hostaname kısmı sunucu ip'si ile değiştirilir. Exp: [server] hostname = ""192.168.1.110"" 5- WSO2 servisi ayarlanır. vim /etc/systemd/system/wso2.service Aşağıda verilenler yazılır değişkenler indirdiğiniz dosya dizini ve çalıştırmak istediğiniz kullanıcıya göre düzenlenir. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # For usage with systemd # On Ubuntu: # install by running sudo systemctl enable <absolute path>/wso2esb.service # start by running sudo systemctl start wso2esb.service # stop by running sudo systemctl stop wso2esb.service # check status by running systemctl status wso2esb.service [ Unit ] Description = WSO2 # If network requirement is not specified, the esb can slow down shutdown # of the system because of timeouts towards the RDBMS running on another # server or other dependent remote components. Requires = network - online . target After = network - online . target [ Service ] # The Type=simple approach is not safe: if a user starts the server using the # wso2server.sh script, systemd doens't know about it and another instance will # be started when 'systemctl start wso2esb' is executed if the server is still running. # Type=simple # ExecStart=CARBON_HOME/bin/wso2server.sh Environment = "" /usr/lib/jvm/ java-1.x.x-openjdk "" # Can optionally be set, but the wso2server.sh script determines it automatically # Cannot be used within this unit file for variable substitution # CARBON_HOME = /opt/wso2/wso2esb/wso2esb-5.0.0 Type = forking # Use WSO2 PID file -> this allows to still use wso2server.sh start|stop commands # directly from command line in combination with systemctl. 'Systemctl status' will # not detect that wso2server.sh start has been executed directly, but will correctly # stop the server when the system is shutdown or 'systemctl stop' is executed. Systemctl # will also not start another instance of the server when 'systemctl start' is executed. PIDFile =/home/wso2/wso2_setup/wso2am-4.1.0 / wso2carbon . pid ExecStart =/home/wso2/wso2_setup/wso2am-4.1.0/bin/api-manager.sh start ExecStop =/home/wso2/wso2_setup/wso2am-4.1.0/bin/api-manager.sh stop User = wso2 [ Install ] WantedBy = multi - user . target systemctl daemon-reload systemctl status wso2.service systemctl enable wso2.service systemctl start wso2.service 6- Servisinizin çalıştığını  https://<sunucu-ip>:9443 adresine giderek kontrol edebilirsiniz.Screen Shot 2022-09-27 at 11.14.53.png","{'title': 'WSO2 Kurulum', 'id': '62397732', 'source': 'https://wiki.softtech.com.tr/display/SDO/WSO2+Kurulum'}"
"DevopsSofttech (DS) uygulaması taranacak Projeleri genomdan otomatik çekerek checkmarkx taraması başlatır ya da uygulama ekranından proje bilgisi aratılarak da tarama başlatılabilir. Ancak DS de proje yoksa  mesela third-party için Checkmarkx'a harici bir projeyi de tara diyebiliriz, bunun için projenin Repo bilgisi (mesela , https://scoretfs.isbank/ISBANK/ ...) ve repoya erişim için kullanıcı yetkisi verilmesi lazım, bu bilgileri ile checkmarksta yeni bir proje yaratarak tarama başlatılabilir. Manuel yeni tarama projesi yaratmak için 1- Menüden Projects&Scans => Create New Project Açılan ekranda projeye bir isim verilir. Present değeri default bırakılabilir ya da Spesifik bir tarama sorgusu seçebilirsiniz mesela Android . 2- Click < Next > Daha Sonra Source Control seçilir. Projenin reposu ve credential bilgileri girilir. Test connection success olduğu görülür. Eğer Local seçilirse soruce dosyasını zip olarak 200MB a kadar proje ekleyebilirsin. 3- Click < Next > Now olarak default seçili zaten 4- Click < Next > Post-Scan kutucuğuna Tarama sonrası e-postayı şuraya gönder diyebilirsin. ve Finish diyip kapatabilirsin. Tarama bittikten sonra ayrıca Menüden => All Scan ile tüm taramaları listeleyip (dilersen Filters ile PROJECT adı kısmandan aratabilirsin) => Action kolonunda bulunan Create Reporta tıklayarak, => Açılan pencerede Generate Report ile dosyayı indirebilirsin.image2022-10-17_16-32-43.pngimage2022-10-17_16-32-20.pngimage2022-10-17_16-31-32.pngimage2022-10-17_16-21-47.pngimage2022-10-17_15-58-1.pngimage2022-10-17_15-56-4.png","{'title': 'Manuel Proje Reposu Taratma ve Raporu <third-party>', 'id': '62400632', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62400632'}"
"Q_ NexusIQ Jenkins tarama pipeline'ında (Softtech Projeleri için)  Gradle hatasından tarama yapılamıyor. http://10.80.36.112:8080/view/nexusiq Jenkins makinasında build hata verenlerin Gradle versiyonunu indirip https://services.gradle.org/distributions/ 10.80.36.112 makinasında C:\CustomGradleRepo dizine kopyalamak gerekli. Q_ Genomda Banka paketini manuel nasıl guncellerim ? Saatte bir çalışan CheckmarxJobs. CreateCheckmarxProjects hangdire jobı ile yeni projelerin Genomdan DevopsSofttech 'e akması sağlanır ancak bu servis hata alırsa manuel müdahale için https://devops.softtech/backend/api/CxSecure/ChangeCheckmarxProjectRepoWithAciCode?aciCode=ACI70145 ile hızlıca genomda ki proje yaratılır, Ardından devops Softtech üzerinden tarama başlatırılır. Q_ Checkmarx taramaları nerden trigger ediliyor ? DevopsSofttech te projeler Bnka Projeleri ve Rally(Softtech) olarak ikiye ayrılıyor. İki farklı sayfadan yönetiliyor. Portal üstünden tarama başlatılabiliyor ayrıca, Banka projeleri Checkmarx taraması =>DevopsSofttech üzerinden Hangfire jobları ile schedule olarak tetikleniyor.( http://10.80.36.89:8081/hangfire ) NexusIQ taraması => Paket deploy olurken (Banka sürüm ekibinin kontrolündeki XLRelease 'de bir adım altından) ScoreTFS altındaki SOFTWARE_QA adındaki pipeline ile tetikleniyor  ( https://scoretfs.isbank/ISBANK/SOFTWARE_QA/_build ) Rally projeleri Checkmarx taraması ve NexusIQ taraması Jaynı yerden Jenkins pipeline dan tetikleniyor, Label olarak ayrıldı. ( http://10.80.36.112:8080/ ) Q_ Checkmarx taramasında aynı repo altında multi-ACI oldugunda diğer paketlerin trend değeri guncellenir mi ? => Evet ,  ""UpdateSameRepoProjectsInformation"" jobı ile aynı repo altındaki paketlerin en son taranan ACI'ın trend değeri ile trend değerleri güncellenir Böylece diğer ACI'lar için ayrıca tarama başlatmaya gerek yoktur. Q_Bazı projeler Checkmarx taramasında hata alıyor. Exclude edilen projeleri nerden görebilirim ? select * from ExcludedCheckmarxProject where AciCode = 'ACI70212' Q_Hangi projeleri Exclude etmeliyim? select * from SystemConfiguration where ConfigurationName = 'ExcludedChekmarxProjects' ; Genom servisinden paketin config bilgisine erişip, aşağıdakiler için kodda zaten paketi devops.softtech te yaratırken exclude ediyor. Ayrıca Istisna olarak tanıntılması gereken paketler için mail ile talep ediliyor. External DataPower Config; Container Base Image; Openshift Project; IDEA Service Version; Q_Ben manuel tarama yaptım ama Ds de release statusu hala çarpı görünüyor nasıl güncelleyebilirim. => Hemen yansıması için, Get last scan and insert DB apisini postman den çalıştır. https://devops.softtech/backend/background/jobs/getLastScanAndInsertToDb?aciCode=ACI69900 Q_Harici bir projeyi nasıl taratırım => wiki de Checkmarx altında , Manuel Proje Reposu Taratma ve Raporu  sayfası. Q_Checkmarx uygulması dondu, Hangfire jobları calısmadı ne yapmalıyım. => 10.80.36.89 sunuya login olup, IIS full restart yapılır. Q_Git clone failed hatası gelediğinde ne yapmalıyım? => Git clone failed: repository ' https://******:******@scoretfs.isbank/ISBANK/ROBOTIKP/_git/bkkob-amex-outgoing-mutabakat-robotu/ ' not found Hatası geldiğinde önce Genomdan guncel repoyu kontrol et.  Müşteriden güncel repo bilgisi alıp karsılaştır, Eğer farklı ise GENOM da repo bilgisini güncellemeleri için mail at. Q_Citrix makinasını talebi nasıl alınır? 1. Sanal makine talebi yapabilmek için maximo üzerinden ""Softtech Citrix Erişim Talebi Formu"" na erişmemiz gerekmektedir. Maximo üzerinden bu forma talep açabilmek için öncelikle LDAP grubuna eklenmemiz gerekmektedir. 2. LDAP grubuna eklenmek için http://kimlik.isbank sayfasına giriş yapmanız, sonrasında Yeni Bir Talepte Bulun/Oracle LDAP grup işlemleri/Gruba Eklenme/Atama kısmına tıklanarak “SOFTCTRX” ldap grubu seçilirek talebi onaya göndermeniz gerekmektedir. 3. LDAP Grubuna eklenme işlemi tamamlandıktan sonra maximo üzerinden (Maximo Linki: Welcome (ism.isbank)), ""Softtech Citrix Erişim Talebi"" formunu görebiliyor olacaksınız. Bu alan üzerinden talebinizi açıklayarak yönetici onayına göndermeniz gerekmektedir. 4. Talebiniz onaylandıktan sonra https://virtual.isbank.com.tr/Citrix/multifactorWeb/ sayfasından citrix sanal makineye erişilebilmektedir. Q_ Sanal Makina geldi ama ilk defa bağlanırken sorun yaşıyorum. Desktop'a tıklayınca indirilen *.ica uzantılı dosyası aşağıdaki linkteki yönergeleri izleyerek Citrix ile açmalısınız. https://support.citrix.com/article/CTX804493/users-prompted-to-download-run-open-ica-file-instead-of-launching-connection - Q_ Twislock nasıl erişebilirim. Ancak sanal makineye erişmeniz Twistlock bulgularının raporladığı Prisma uygulamasına erişmeniz için yeterli olmamaktadır. Sanal Makine üzerinden twistlock bulgularına erişmek için ek firewall talebi yapmanız gerekmektedir. Firewall talebinizi Maximo üzerinden ""Network Erişimi Firewall Tanımı"" alanı üzerinden yapmanız gerekmektedir. Tanım içerisinde bilgileri aşağıda belirtilen şekilde giriş yapmanız gerekmektedir. • Ortam: Production • Kaynak IP Adresi: Kendi IP Adresinizi yazmanız gerekmektedir. • Hedef IP Adresi: 10.192.99.199 • Hedef Alan: https://twistlock-console.apps.vega.kube.isbank (Talep içerisinde belirtebilirsiniz.) • Port/Servis: HTTP/HTTPS WEB erişimi (tcp/80,tcp/443) • Geçerli Olması Gereken Süre: 3 Yıl Talebiniz tamamlandıktan sonra citrix sanal makine üzerinden İşbank sicilinizi “ISBANK\xxxxxxx” formatında girerek link üzerinden Prisma Cloud (kube.isbank) erişim sağlayabilirsiniz.image-2023-1-13_0-45-4.pngimage-2023-1-13_0-44-44.pngimage-2023-1-12_14-19-5.pngimage-2023-1-12_14-17-50.pngimage-2022-11-21_19-45-14.pngimage-2022-11-21_19-44-24.pngimage2022-10-20_9-23-51.png","{'title': 'Ne yapmalıyım ?', 'id': '62400981', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62400981'}"
"Panel Softtech üzerinde tutulan defect sayılarını Mirket üzerinden yeni veriler manuel olarak postman üzerinden  GovernanceController daki SetFunctionalDefects  ile yapılır. Ama bu işlem manuel yapılır. herhangi bir job ya da otomasyon yok. TEST OTOMASYON jobları : DevopsSofttech üzerinden DB ve API aracı olarak kullanılır. backgroundJob.Start(""GetMochaScenarios"", Cron.Daily(0, 0), () => mochaJobs.GetMochaScenarios(), ""mocha""); backgroundJob.Start(""GetMochaActions"", Cron.Daily(0, 0), () => mochaJobs.GetMochaActions(), ""mocha""); backgroundJob.Start(""GetMochaRuns"", Cron.Daily(0, 0), () => mochaJobs.GetMochaRunResults(), ""mocha""); Senaryo&Defect Sayıları(Banka) - Panel.Softtech https://jira.isbank/confluence/display/TOY/Panel+verilerinin+olusturulmasi için yetki jira.isbank ve confluence yetkisi gerekebilir.image2022-10-24_11-29-19.png","{'title': 'Panel.Softtech ilişkisi', 'id': '62401332', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=62401332'}"
"Screen Name Json Main object DataType CI Image ""image"": ""scorenexus.isbank:9443/cashflow-platform/cashflow-platform:0.1.107"", ""entityInfo"": {""instances"": [ { ""image"": ""scorenexus.isbank:9443/cashflow-platform/cashflow-platform:0.1.107"", ""host"": ""KLTFSBDA02"", ""modified"": ""2022-11-17T14:02:49.552Z"", ""tag"": ""0.1.107"", ""repo"": ""cashflow-platform/cashflow-platform"", ""registry"": ""scorenexus.isbank:9443"" }, Json: Array Host ""host"": ""KLTFSBDA02"", ""entityInfo"": { Json: Array Project ? Build ? Vulnerabilities ""critical"": 99, ""high"": 525, ""medium"": 624, ""low"": 121, ""total"": 1369 ""entityInfo"": {  ""vulnerabilityDistribution"": { ""critical"": 0, ""high"": 0, ""medium"": 2, ""low"": 0, ""total"": 2 }, Risk Factors ? Scan Time ""scanTime"": ""2022-11-17T14:02:49.552Z"", Status ? Collection ? CI- Image Details Image ""image"": ""scorenexus.isbank:9443/cashflow-platform/cashflow-platform:0.1.107"", ""entityInfo"": {""instances"": [ { ID ""id"": ""b4c3aaaec6c68088f0ab152d7f4e571c91b6623dfc18ef1a98e96be95eceb7ed"", ""entityInfo"": { OS Distribution ""distro"": ""CentOS Linux release 7.6.1810 (Core)"", ""entityInfo"": { OS Release ""osDistroRelease"": ""RHEL7"", ""entityInfo"":  { Tags ""tags"": [ { ""tag"": ""latest"" ""entityInfo"": { ""tags"": [ Scan Staus ""vulnFailureSummary"": ""Scan failed due to vulnerability policy violations: block-critical, 1 vulnerabilities, [critical:1]"", if ""pass"": false pass: false, CI- Vulnerabilities Type ""type"": ""image"", ""vulnerabilities"": [] Highest Severity ""severity"": ""high"", ""vulnerabilities"": [] Description ""description"": ""The Hashicorp go-gette "", ""vulnerabilities"": [] Fix status status: ""fixed in 1.0.7"" ""vulnerabilities"": [] CVE ""cve"": ""CVE-2021-23343"" Packages ""packageName"": ""path-parse"", Package Version ""packageVersion"": ""1.0.0"", Vulnerability Link link: "" https://nvd.nist.gov/vuln/detail/CVE-2020-7754 "" CI- Compliance ID ""id"": 425, ""complianceIssues"": [] Category ? Aşağı görseli konmuştur. Severity ""severity"": ""high"", ""complianceIssues"": [] Description ""title"": ""Private keys stored in image"", ""complianceIssues"": [] Screen Name Json Main object DataType CD- Deployed Registry ""registry"": ""scorenexus.isbank:9443"", ""instances"": [ { ""image"": ""scorenexus.isbank:9443/ai-h2o-dai/ai-h2o-driverless-ai-1.9.0.6:latest"", ""host"": ""workergpu9.vega.kube.isbank"", ""modified"": ""2022-11-17T13:23:16.101Z"", ""tag"": ""latest"", ""repo"": ""ai-h2o-dai/ai-h2o-driverless-ai-1.9.0.6"", ""registry"": ""scorenexus.isbank:9443"" } ], Json: Array Repository ""repo"": ""ai-h2o-dai/ai-h2o-driverless-ai-1.9.0.6"", instances Json: Array Tag ""tag"": ""latest"" instances Json: Array Host ""host"": ""workergpu9.vega.kube.isbank"", instances Json: Array Clusters ""vega"" ""clusters"": [ ""vega"" ], Json: Array Apps ? Vulnerabilities ""critical"": 99, ""high"": 525, ""medium"": 624, ""low"": 121, ""vulnerabilityDistribution"": { ""critical"": 99, ""high"": 525, ""medium"": 624, ""low"": 121, ""total"": 1369 }, Risk Factors ? Collections ? Pass pass: true CD Image Details Image ""image"": ""scorenexus.isbank:9443/ai-h2o-dai/ai-h2o-driverless-ai-1.9.0.6:latest"", ""instances"": [ { ""image"": ID ""id"": b4c3aaaec6c68088f0ab152d7f4e571c91b6623dfc18ef1a98e96be95eceb7ed"", ""id"" OS Distribution ""distro"": ""CentOS Linux release 7.6.1810 (Core)"", ""distro"" OS Release ""osDistroRelease"": ""RHEL7"", ""osDistroRelease"" Diggest ""repoDigests"": [ ""scorenexus.isbank:9443/ai-h2o-dai/ai-h2o-driverless-ai-1.9.0.6@sha256:753933e9812a4b28fad59c9cf6c4747399496b70ec8a84d1386749a1fd35e06a"" ], ""repoDigests"":[] Tag ""tag"": ""latest"" instances CD Vulnerabilities Type ""type"": ""image"", ""vulnerabilities"": [] Highest Severity ""severity"": ""high"", ""vulnerabilities"": [] Description ""description"": ""The Hashicorp go-gette "", ""vulnerabilities"": [] CD Compliance ID ""id"": 425, ""complianceIssues"": [] Category ? Severity ""severity"": ""high"", ""complianceIssues"": [] Description ""title"": ""Private keys stored in image"", ""complianceIssues"": []","{'title': 'Prisma (Twsitlock) Entegrasyonu', 'id': '71630984', 'source': 'https://wiki.softtech.com.tr/display/SDO/Prisma+%28Twsitlock%29+Entegrasyonu'}"
,"{'title': 'ÇKB_Analiz Yetkinlik Hattı', 'id': '72548402', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=72548402'}"
,"{'title': 'Yetkinlik Hattı Konseyi (Kasım Ayı) Toplantı Notları', 'id': '76120381', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=76120381'}"
"jira.softtech.com.tr Örnek Jira kaydı , SubCategory de Checkmarx seçilir.image-2023-1-5_14-52-5.png","{'title': 'Jira kaydı açma', 'id': '77727367', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=77727367'}"
"Backlog In Progress description,duedate,assignee,location, labels SDO 40 PLT Jira issuekey,summary,issuetype,created,updated,duedate,assignee,reporter,priority,status,resolution key,summary,type,created,updated,due,assignee,reporter,priority,status,resolution 20 project = Plateau AND ""Main / Sub Category"" in (DEVOPS, ""DevOps Platform"") AND Status != RESOLVED AND Status != DONE  ORDER BY PRIORITY, UPDATED DESC 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 DONE description,duedate,assignee,location,completedate,labels SDO 40 complete PLT DONE Jira issuekey,summary,issuetype,created,updated,duedate,assignee,reporter,priority,status,resolution key,summary,type,created,updated,due,assignee,reporter,priority,status,resolution 20 project = Plateau AND ""Main / Sub Category"" in (DEVOPS, ""DevOps Platform"") AND Status = DONE  ORDER BY UPDATED, PRIORITY DESC 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 PLT CHARTS true false Jira project%20%3D%20Plateau%20AND%20%22Main%20%2F%20Sub%20Category%22%20in%20(DEVOPS%2C%20%22DevOps%20Platform%22)%20AND%20Status%20!%3D%20RESOLVED%20AND%20Status%20!%3D%20DONE assignees pie true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 true false Jira all project%20%3D%20Plateau%20AND%20%22Main%20%2F%20Sub%20Category%22%20in%20(DEVOPS%2C%20%22DevOps%20Platform%22)%20 false createdvsresolved daily false 90 true 2f3948c0-ae52-36e0-9a8c-3c79fe243c41 WEEKLY REPORT description,duedate,assignee,location,completedate,labels SDO 40 complete true Roadmap Q1 d0eb600b-d782-42cf-8101-021f990443f0~~~~~782c3011-6367-491f-a7cd-86de2b76d117 true %7B%22title%22%3A%22Roadmap%20Planner%22%2C%22timeline%22%3A%7B%22startDate%22%3A%222023-02-20%2000%3A00%3A00%22%2C%22endDate%22%3A%222023-04-30%2000%3A00%3A00%22%2C%22displayOption%22%3A%22WEEK%22%7D%2C%22lanes%22%3A%5B%7B%22title%22%3A%22devops.softtech%22%2C%22color%22%3A%7B%22lane%22%3A%22%23f6c342%22%2C%22bar%22%3A%22%23fadb8e%22%2C%22text%22%3A%22%23594300%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Checkmarx%20baseline%22%2C%22description%22%3A%22This%20is%20the%20first%20bar.%22%2C%22startDate%22%3A%222023-03-11%2016%3A23%3A45%22%2C%22duration%22%3A4.772277227722772%2C%22rowIndex%22%3A0%2C%22id%22%3A%22d0eb600b-d782-42cf-8101-021f990443f0%22%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%2C%7B%22title%22%3A%22Checkmarx%20Upgrade%22%2C%22description%22%3A%22This%20is%20the%20second%20bar.%22%2C%22startDate%22%3A%222023-02-25%2011%3A04%3A23%22%2C%22duration%22%3A2.6435643564356437%2C%22rowIndex%22%3A1%2C%22id%22%3A%2241c32136-9eb9-4813-a94d-87410a8fbc23%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-04-09%2012%3A21%3A23%22%2C%22id%22%3A%2216a7fbb9-86b3-456c-b563-34cbc272f3eb%22%2C%22title%22%3A%22Checkmarx%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.0792079207920793%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-04-16%2002%3A22%3A34%22%2C%22id%22%3A%222b982a2b-c899-4d00-8732-f1f0b5e29aef%22%2C%22title%22%3A%22Provide%20API%20for%20Backstage%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.118811881188119%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-04-01%2018%3A03%3A33%22%2C%22id%22%3A%22782c3011-6367-491f-a7cd-86de2b76d117%22%2C%22title%22%3A%22NexusIQ%20paketlerin%20derlenmesi%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.118811881188119%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%5D%7D%2C%7B%22title%22%3A%22Cost%20Dashboard%22%2C%22color%22%3A%7B%22lane%22%3A%22%233b7fc4%22%2C%22bar%22%3A%22%236c9fd3%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Define%20Labels%22%2C%22description%22%3A%22This%20is%20the%20third%20bar.%22%2C%22startDate%22%3A%222023-02-25%2004%3A45%3A08%22%2C%22duration%22%3A1.4554455445544554%2C%22rowIndex%22%3A0%2C%22id%22%3A%22b1f46af1-dece-44ad-ab11-a236da12d4b4%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-03-08%2012%3A32%3A47%22%2C%22id%22%3A%2268a710b1-ac6c-4182-8921-852b0576a24a%22%2C%22title%22%3A%22Design%20%26%20Development%20Dashboard%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.554455445544554%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Application%20Support%22%2C%22color%22%3A%7B%22lane%22%3A%22%23d04437%22%2C%22bar%22%3A%22%23dc7369%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-02-24%2021%3A45%3A58%22%2C%22id%22%3A%221d675184-1ee6-42fd-b4a4-e1b30c268e74%22%2C%22title%22%3A%22Onboarding%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-02-28%2014%3A55%3A21%22%2C%22id%22%3A%22375993f0-b670-4654-8c2f-9a7da8362be2%22%2C%22title%22%3A%22Prepare%20Guidelines%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.3564356435643563%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-03-11%2004%3A25%3A11%22%2C%22id%22%3A%22e70a6de9-76ec-4f57-9377-44721774e936%22%2C%22title%22%3A%22Zabbix%20Dashboard%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.405940594059406%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-03-25%2012%3A44%3A11%22%2C%22id%22%3A%22ad69b1e9-4a52-49e5-84be-93cdc3b58cd9%22%2C%22title%22%3A%22Logs%20%26%20Authorization%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.5742574257425743%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-04-04%2023%3A34%3A20%22%2C%22id%22%3A%22e60c3d43-78f4-459b-9391-d48437e21fcd%22%2C%22title%22%3A%22Alerts%20%26%20Notification%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.5643564356435644%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22%C4%B0%C5%9Fnet%20Anthos%22%2C%22color%22%3A%7B%22lane%22%3A%22%238eb021%22%2C%22bar%22%3A%22%23aac459%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-02-25%2009%3A24%3A35%22%2C%22id%22%3A%22fde460bd-a90c-4112-adf5-3be75b502eb3%22%2C%22title%22%3A%22Evaluate%20Alternatives%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.5544554455445545%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-03-09%2000%3A11%3A24%22%2C%22id%22%3A%22412c1707-9085-43cf-8f98-9698c8d52710%22%2C%22title%22%3A%22T%C3%BCris%20Physical%20Servers%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.1584158415841586%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-04-05%2012%3A32%3A47%22%2C%22id%22%3A%2213c655e8-a947-4821-917f-da8084f08f5e%22%2C%22title%22%3A%22License%20Agreement%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.0396039603960396%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-03-03%2021%3A06%3A03%22%2C%22id%22%3A%220bf9990c-aac4-4285-bc18-e25e9d722bd5%22%2C%22title%22%3A%22Emtia%20Prod%20Installation%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.217821782178218%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-03-06%2012%3A38%3A29%22%2C%22id%22%3A%22b1063237-d2e8-49c9-b54a-f1a31cdfc5c0%22%2C%22title%22%3A%22Smartfarm%2FPob%2FAHE%20Prod%20maintenance%20%26%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.465346534653466%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Cloud%22%2C%22color%22%3A%7B%22lane%22%3A%22%23ea632b%22%2C%22bar%22%3A%22%23ef8a60%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-04-14%2005%3A07%3A57%22%2C%22id%22%3A%229ce3f68c-7057-48f3-9f07-a6376ec59d75%22%2C%22title%22%3A%22Nexus%2FGCR%20disk%20cleanup%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.386138613861386%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-02-25%2022%3A21%3A48%22%2C%22id%22%3A%2205a8cc4d-66a6-4014-927f-e8d6f8068d78%22%2C%22title%22%3A%22GKE%20version%20upgrade%22%2C%22description%22%3A%22Devnew%20cluster%20version%20upgrade%22%2C%22duration%22%3A2.6732673267326734%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-03-18%2018%3A43%3A29%22%2C%22id%22%3A%222b0334d6-9891-48f5-95e3-2acd9e24340e%22%2C%22title%22%3A%22Azure%2Fgithun%20migration%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.01980198019802%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-03-19%2018%3A40%3A38%22%2C%22id%22%3A%22925e333a-115a-47ab-a7a4-91235d1070af%22%2C%22title%22%3A%22Providing%20integrated%20tools%20for%20monitoring%20%26%20Logs%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-03-05%2003%3A42%3A25%22%2C%22id%22%3A%2217fc8227-cea5-407a-bc14-b6fed79cee53%22%2C%22title%22%3A%22Providing%20Monitoring%20dashboard%20to%20track%20availability%20of%20devops%20tools%22%2C%22description%22%3A%22%22%2C%22duration%22%3A8.089108910891088%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Consultancy%22%2C%22color%22%3A%7B%22lane%22%3A%22%23654982%22%2C%22bar%22%3A%22%238c77a1%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-02-26%2023%3A40%3A02%22%2C%22id%22%3A%22d99e2602-5c4a-4edb-9236-a57d2e391902%22%2C%22title%22%3A%22Qpay%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.069306930693069%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-03-01%2018%3A52%3A02%22%2C%22id%22%3A%222865b9bb-f124-4ab2-8b2b-65c9f4150b6c%22%2C%22title%22%3A%22%C4%B0%C5%9FFaktoring%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.178217821782178%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%5D%2C%22markers%22%3A%5B%7B%22title%22%3A%22Pointer%22%2C%22markerDate%22%3A%222023-04-14%2012%3A07%3A07%22%7D%5D%7D Roadmap%20Planner 12c22e69ef02e1b57897ca3a0263230c Q2 d0eb600b-d782-42cf-8101-021f990443f0~~~~~782c3011-6367-491f-a7cd-86de2b76d117 true %7B%22title%22%3A%22Roadmap%20Planner%22%2C%22timeline%22%3A%7B%22startDate%22%3A%222023-05-01%2000%3A00%3A00%22%2C%22endDate%22%3A%222023-07-31%2000%3A00%3A00%22%2C%22displayOption%22%3A%22WEEK%22%7D%2C%22lanes%22%3A%5B%7B%22title%22%3A%22devops.softtech%22%2C%22color%22%3A%7B%22lane%22%3A%22%23f6c342%22%2C%22bar%22%3A%22%23fadb8e%22%2C%22text%22%3A%22%23594300%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Checkmarx%20baseline%22%2C%22description%22%3A%22This%20is%20the%20first%20bar.%22%2C%22startDate%22%3A%222023-03-11%2016%3A23%3A45%22%2C%22duration%22%3A4.772277227722772%2C%22rowIndex%22%3A0%2C%22id%22%3A%22d0eb600b-d782-42cf-8101-021f990443f0%22%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%2C%7B%22title%22%3A%22Checkmarx%20Upgrade%22%2C%22description%22%3A%22This%20is%20the%20second%20bar.%22%2C%22startDate%22%3A%222023-02-25%2011%3A04%3A23%22%2C%22duration%22%3A2.6435643564356437%2C%22rowIndex%22%3A1%2C%22id%22%3A%2241c32136-9eb9-4813-a94d-87410a8fbc23%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-04-09%2012%3A21%3A23%22%2C%22id%22%3A%2216a7fbb9-86b3-456c-b563-34cbc272f3eb%22%2C%22title%22%3A%22Checkmarx%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.0792079207920793%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-04-16%2002%3A22%3A34%22%2C%22id%22%3A%222b982a2b-c899-4d00-8732-f1f0b5e29aef%22%2C%22title%22%3A%22Provide%20API%20for%20Backstage%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.118811881188119%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-04-01%2018%3A03%3A33%22%2C%22id%22%3A%22782c3011-6367-491f-a7cd-86de2b76d117%22%2C%22title%22%3A%22NexusIQ%20paketlerin%20derlenmesi%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.118811881188119%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-01%2000%3A00%3A00%22%2C%22id%22%3A%222b024a9b-910e-4039-9259-5ae08aff0615%22%2C%22title%22%3A%22NexusIQ%20Isbank%20Release%20Integraiton%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.9405940594059405%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-07-10%2023%3A17%3A13%22%2C%22id%22%3A%22c0686373-ef14-47d2-bbde-b0727580773d%22%2C%22title%22%3A%22Backstage%20API%20Integration%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.6930693069306932%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-20%2006%3A24%3A57%22%2C%22id%22%3A%226b9053fb-1ece-4c21-952c-24158efc29f7%22%2C%22title%22%3A%22Checkmarx%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.6138613861386135%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-19%2010%3A27%3A19%22%2C%22id%22%3A%22e78a806c-f66e-4a9c-81d9-73439ef69430%22%2C%22title%22%3A%22NexusIQ%20developer%20panel%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.762376237623762%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-07-04%2010%3A55%3A50%22%2C%22id%22%3A%228d72ff59-b93a-4d80-939a-a27bda1240df%22%2C%22title%22%3A%22NexusIQ%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.801980198019802%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22%C4%B0%C5%9Fnet%20Anthos%22%2C%22color%22%3A%7B%22lane%22%3A%22%23d04437%22%2C%22bar%22%3A%22%23dc7369%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2021%3A51%3A40%22%2C%22id%22%3A%223d0ee80c-5f03-4fcd-9bbf-d16295fcefe5%22%2C%22title%22%3A%22HA%20Cluster%20component%20upgrade%20(Smartfarm%2C%20Emtia)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.5841584158415842%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-30%2019%3A14%3A51%22%2C%22id%22%3A%2237e48b57-0fb3-46be-9173-491b6e7ce544%22%2C%22title%22%3A%22Rollout%20Tests%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.5148514851485149%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-13%2000%3A57%3A01%22%2C%22id%22%3A%2221805466-6803-477c-b6d8-69c385d2ef6d%22%2C%22title%22%3A%22Disaster%20Tests%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.7227722772277227%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-28%2000%3A14%3A15%22%2C%22id%22%3A%226be75c62-3df5-40ea-904e-5d395eeecbd4%22%2C%22title%22%3A%22SQL%20Runner%20(ldap%2C%20audit%20logs)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.376237623762377%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-07%2005%3A42%3A10%22%2C%22id%22%3A%22e155f422-42ba-44c1-bde8-6cfc9f57e4fa%22%2C%22title%22%3A%22Migrate%20projets%20and%20Remove%20old%20Clusters%20(1.4%2C%201.11)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.475247524752476%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-01%2001%3A39%3A48%22%2C%22id%22%3A%22f714dba0-4278-4c7a-af04-b195dd81920e%22%2C%22title%22%3A%22License%20Agreement%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.00990099009901%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-05-04%2017%3A49%3A18%22%2C%22id%22%3A%223243c4be-bc1c-460c-ba45-fab818e64f06%22%2C%22title%22%3A%22SmartFarm%2C%20Emtia%2C%20HR360%2C%20Pob%20Maintenance%20%26%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A13.297029702970297%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-06-09%2023%3A45%3A44%22%2C%22id%22%3A%2262b6a28e-af47-4107-b8d4-9542ca4598c0%22%2C%22title%22%3A%22VMWare%20Upgrade%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.732673267326733%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Cloud%22%2C%22color%22%3A%7B%22lane%22%3A%22%233b7fc4%22%2C%22bar%22%3A%22%236c9fd3%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2003%3A33%3A51%22%2C%22id%22%3A%229b7ff55d-de80-4e6b-9a19-5b2abc2e9be6%22%2C%22title%22%3A%22GKE%20version%20upgrade%20%26%20remove%20dev-new%20cluster%20(portera%2C%20tkb)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.98019801980198%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-12%2011%3A38%3A36%22%2C%22id%22%3A%22b229fb18-a9f9-4ec2-9d0b-c2c51d9a9725%22%2C%22title%22%3A%22Prometheus%20%26%20Grafana%20latest%20version%20installations.%20(Developer%20dashboard%2C%20Pipeline%20Integrations)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.772277227722772%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-06-08%2001%3A11%3A17%22%2C%22id%22%3A%2260fa907f-22ad-42c2-82dc-d59538a6e58a%22%2C%22title%22%3A%22Nexus%2FGCR%20disk%20cleanup%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.227722772277228%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-04%2021%3A08%3A54%22%2C%22id%22%3A%22bb7d1a7b-1ae3-4f9a-8acc-02e5c3fcd5fd%22%2C%22title%22%3A%22Auth%20reevaluation%20(Active%20Directory%20Groups)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.3366336633663365%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-06-30%2008%3A47%3A31%22%2C%22id%22%3A%2275d2904f-5f97-451e-b73e-559537e79ece%22%2C%22title%22%3A%22Jenkis%20Upgrade%20(portera%2C%20tkb)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.138613861386139%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-12%2003%3A48%3A07%22%2C%22id%22%3A%2208e9c236-6add-4939-a58c-41f90354f399%22%2C%22title%22%3A%22Evaluation%20of%20hosting%20codes%20on%20%C4%B0%C5%9Fnet%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.693069306930693%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-05-23%2002%3A36%3A49%22%2C%22id%22%3A%22eeafd68c-c6a3-49a8-86de-79c49544859b%22%2C%22title%22%3A%22Providing%20devops%20tools%20availability%20tracking%20dashboards%22%2C%22description%22%3A%22%22%2C%22duration%22%3A8.811881188118813%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Consultancy%22%2C%22color%22%3A%7B%22lane%22%3A%22%238eb021%22%2C%22bar%22%3A%22%23aac459%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-07%2012%3A21%3A23%22%2C%22id%22%3A%22d522f595-e7e4-4e18-9142-fb4c71067c03%22%2C%22title%22%3A%22QPay%22%2C%22description%22%3A%22%22%2C%22duration%22%3A12.801980198019802%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-09%2005%3A56%3A26%22%2C%22id%22%3A%22b7a0b555-de20-4c9f-9dd3-aea4e5d3e283%22%2C%22title%22%3A%22%C4%B0%C5%9F%20Faktoring%22%2C%22description%22%3A%22%22%2C%22duration%22%3A12.712871287128714%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2008%3A33%3A16%22%2C%22id%22%3A%22d1ba58ae-a40d-48b7-bb6a-631b25ac3c0b%22%2C%22title%22%3A%22%C4%B0%C5%9FBank%20%2F%20Google%20%2F%20Huawei%20Partnetship%22%2C%22description%22%3A%22%22%2C%22duration%22%3A13.495049504950495%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%5D%2C%22markers%22%3A%5B%7B%22title%22%3A%22Pointer%22%2C%22markerDate%22%3A%222023-04-14%2012%3A07%3A07%22%7D%5D%7D Roadmap%20Planner 0c146dd06642c560f9c81714e40839c8 Q3 d0eb600b-d782-42cf-8101-021f990443f0~~~~~782c3011-6367-491f-a7cd-86de2b76d117 true %7B%22title%22%3A%22Roadmap%20Planner%22%2C%22timeline%22%3A%7B%22startDate%22%3A%222023-05-01%2000%3A00%3A00%22%2C%22endDate%22%3A%222023-07-31%2000%3A00%3A00%22%2C%22displayOption%22%3A%22WEEK%22%7D%2C%22lanes%22%3A%5B%7B%22title%22%3A%22devops.softtech%22%2C%22color%22%3A%7B%22lane%22%3A%22%23f6c342%22%2C%22bar%22%3A%22%23fadb8e%22%2C%22text%22%3A%22%23594300%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Checkmarx%20baseline%22%2C%22description%22%3A%22This%20is%20the%20first%20bar.%22%2C%22startDate%22%3A%222023-03-11%2016%3A23%3A45%22%2C%22duration%22%3A4.772277227722772%2C%22rowIndex%22%3A0%2C%22id%22%3A%22d0eb600b-d782-42cf-8101-021f990443f0%22%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%2C%7B%22title%22%3A%22Checkmarx%20Upgrade%22%2C%22description%22%3A%22This%20is%20the%20second%20bar.%22%2C%22startDate%22%3A%222023-02-25%2011%3A04%3A23%22%2C%22duration%22%3A2.6435643564356437%2C%22rowIndex%22%3A1%2C%22id%22%3A%2241c32136-9eb9-4813-a94d-87410a8fbc23%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-04-09%2012%3A21%3A23%22%2C%22id%22%3A%2216a7fbb9-86b3-456c-b563-34cbc272f3eb%22%2C%22title%22%3A%22Checkmarx%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.0792079207920793%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-04-16%2002%3A22%3A34%22%2C%22id%22%3A%222b982a2b-c899-4d00-8732-f1f0b5e29aef%22%2C%22title%22%3A%22Provide%20API%20for%20Backstage%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.118811881188119%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-04-01%2018%3A03%3A33%22%2C%22id%22%3A%22782c3011-6367-491f-a7cd-86de2b76d117%22%2C%22title%22%3A%22NexusIQ%20paketlerin%20derlenmesi%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.118811881188119%2C%22pageLink%22%3A%7B%22id%22%3A%2280480967%22%2C%22spaceKey%22%3A%22SDO%22%2C%22title%22%3A%22DevOps%20Softtech%20Panel%22%2C%22type%22%3A%22page%22%2C%22wikiLink%22%3A%22%5BDevOps%20Softtech%20Panel%5D%22%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-01%2000%3A00%3A00%22%2C%22id%22%3A%222b024a9b-910e-4039-9259-5ae08aff0615%22%2C%22title%22%3A%22NexusIQ%20Isbank%20Release%20Integraiton%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.9405940594059405%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-07-10%2023%3A17%3A13%22%2C%22id%22%3A%22c0686373-ef14-47d2-bbde-b0727580773d%22%2C%22title%22%3A%22Backstage%20API%20Integration%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.6930693069306932%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-20%2006%3A24%3A57%22%2C%22id%22%3A%226b9053fb-1ece-4c21-952c-24158efc29f7%22%2C%22title%22%3A%22Checkmarx%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.6138613861386135%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-19%2010%3A27%3A19%22%2C%22id%22%3A%22e78a806c-f66e-4a9c-81d9-73439ef69430%22%2C%22title%22%3A%22NexusIQ%20developer%20panel%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.762376237623762%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-07-04%2010%3A55%3A50%22%2C%22id%22%3A%228d72ff59-b93a-4d80-939a-a27bda1240df%22%2C%22title%22%3A%22NexusIQ%20Exception%20Management%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.801980198019802%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22%C4%B0%C5%9Fnet%20Anthos%22%2C%22color%22%3A%7B%22lane%22%3A%22%23d04437%22%2C%22bar%22%3A%22%23dc7369%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2021%3A51%3A40%22%2C%22id%22%3A%223d0ee80c-5f03-4fcd-9bbf-d16295fcefe5%22%2C%22title%22%3A%22HA%20Cluster%20component%20upgrade%20(Smartfarm%2C%20Emtia)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.5841584158415842%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-30%2019%3A14%3A51%22%2C%22id%22%3A%2237e48b57-0fb3-46be-9173-491b6e7ce544%22%2C%22title%22%3A%22Rollout%20Tests%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.5148514851485149%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-13%2000%3A57%3A01%22%2C%22id%22%3A%2221805466-6803-477c-b6d8-69c385d2ef6d%22%2C%22title%22%3A%22Disaster%20Tests%22%2C%22description%22%3A%22%22%2C%22duration%22%3A1.7227722772277227%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-28%2000%3A14%3A15%22%2C%22id%22%3A%226be75c62-3df5-40ea-904e-5d395eeecbd4%22%2C%22title%22%3A%22SQL%20Runner%20(ldap%2C%20audit%20logs)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.376237623762377%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-07%2005%3A42%3A10%22%2C%22id%22%3A%22e155f422-42ba-44c1-bde8-6cfc9f57e4fa%22%2C%22title%22%3A%22Migrate%20projets%20and%20Remove%20old%20Clusters%20(1.4%2C%201.11)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.475247524752476%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-01%2001%3A39%3A48%22%2C%22id%22%3A%22f714dba0-4278-4c7a-af04-b195dd81920e%22%2C%22title%22%3A%22License%20Agreement%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.00990099009901%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-05-04%2017%3A49%3A18%22%2C%22id%22%3A%223243c4be-bc1c-460c-ba45-fab818e64f06%22%2C%22title%22%3A%22SmartFarm%2C%20Emtia%2C%20HR360%2C%20Pob%20Maintenance%20%26%20Support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A13.297029702970297%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-06-09%2023%3A45%3A44%22%2C%22id%22%3A%2262b6a28e-af47-4107-b8d4-9542ca4598c0%22%2C%22title%22%3A%22VMWare%20Upgrade%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.732673267326733%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Cloud%22%2C%22color%22%3A%7B%22lane%22%3A%22%233b7fc4%22%2C%22bar%22%3A%22%236c9fd3%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2003%3A33%3A51%22%2C%22id%22%3A%229b7ff55d-de80-4e6b-9a19-5b2abc2e9be6%22%2C%22title%22%3A%22GKE%20version%20upgrade%20%26%20remove%20dev-new%20cluster%20(portera%2C%20tkb)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.98019801980198%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-06-12%2011%3A38%3A36%22%2C%22id%22%3A%22b229fb18-a9f9-4ec2-9d0b-c2c51d9a9725%22%2C%22title%22%3A%22Prometheus%20%26%20Grafana%20latest%20version%20installations.%20(Developer%20dashboard%2C%20Pipeline%20Integrations)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.772277227722772%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-06-08%2001%3A11%3A17%22%2C%22id%22%3A%2260fa907f-22ad-42c2-82dc-d59538a6e58a%22%2C%22title%22%3A%22Nexus%2FGCR%20disk%20cleanup%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.227722772277228%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-04%2021%3A08%3A54%22%2C%22id%22%3A%22bb7d1a7b-1ae3-4f9a-8acc-02e5c3fcd5fd%22%2C%22title%22%3A%22Auth%20reevaluation%20(Active%20Directory%20Groups)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.3366336633663365%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-06-30%2008%3A47%3A31%22%2C%22id%22%3A%2275d2904f-5f97-451e-b73e-559537e79ece%22%2C%22title%22%3A%22Jenkis%20Upgrade%20(portera%2C%20tkb)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.138613861386139%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-12%2003%3A48%3A07%22%2C%22id%22%3A%2208e9c236-6add-4939-a58c-41f90354f399%22%2C%22title%22%3A%22Evaluation%20of%20hosting%20codes%20on%20%C4%B0%C5%9Fnet%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.693069306930693%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-05-23%2002%3A36%3A49%22%2C%22id%22%3A%22eeafd68c-c6a3-49a8-86de-79c49544859b%22%2C%22title%22%3A%22Providing%20devops%20tools%20availability%20tracking%20dashboards%22%2C%22description%22%3A%22%22%2C%22duration%22%3A8.811881188118813%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Consultancy%22%2C%22color%22%3A%7B%22lane%22%3A%22%238eb021%22%2C%22bar%22%3A%22%23aac459%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-05-07%2012%3A21%3A23%22%2C%22id%22%3A%22d522f595-e7e4-4e18-9142-fb4c71067c03%22%2C%22title%22%3A%22QPay%22%2C%22description%22%3A%22%22%2C%22duration%22%3A12.801980198019802%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-05-09%2005%3A56%3A26%22%2C%22id%22%3A%22b7a0b555-de20-4c9f-9dd3-aea4e5d3e283%22%2C%22title%22%3A%22%C4%B0%C5%9F%20Faktoring%22%2C%22description%22%3A%22%22%2C%22duration%22%3A12.712871287128714%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-05-03%2008%3A33%3A16%22%2C%22id%22%3A%22d1ba58ae-a40d-48b7-bb6a-631b25ac3c0b%22%2C%22title%22%3A%22%C4%B0%C5%9FBank%20%2F%20Google%20%2F%20Huawei%20Partnetship%22%2C%22description%22%3A%22%22%2C%22duration%22%3A13.495049504950495%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%5D%2C%22markers%22%3A%5B%7B%22title%22%3A%22Pointer%22%2C%22markerDate%22%3A%222023-04-14%2012%3A07%3A07%22%7D%5D%7D Roadmap%20Planner 0c146dd06642c560f9c81714e40839c8 Q4 true %7B%22title%22%3A%22Roadmap%20Planner%22%2C%22timeline%22%3A%7B%22startDate%22%3A%222023-10-12%2000%3A00%3A00%22%2C%22endDate%22%3A%222024-01-31%2000%3A00%3A00%22%2C%22displayOption%22%3A%22WEEK%22%7D%2C%22lanes%22%3A%5B%7B%22title%22%3A%22devops.softtech%22%2C%22color%22%3A%7B%22lane%22%3A%22%23f6c342%22%2C%22bar%22%3A%22%23fadb8e%22%2C%22text%22%3A%22%23594300%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Transfer%20operational%20work%20to%20L2%22%2C%22description%22%3A%22This%20is%20the%20first%20bar.%22%2C%22startDate%22%3A%222023-10-01%2000%3A00%3A00%22%2C%22duration%22%3A9.118811881188119%2C%22rowIndex%22%3A0%2C%22id%22%3A%2251853ada-92c5-4c12-93a6-19dbd7ecb8f1%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22title%22%3A%22Nexus%20IQ%20roll%20out%20support%22%2C%22description%22%3A%22This%20is%20the%20second%20bar.%22%2C%22startDate%22%3A%222023-10-17%2014%3A15%3A26%22%2C%22duration%22%3A5.267326732673268%2C%22rowIndex%22%3A1%2C%22id%22%3A%22688060f4-245e-45eb-be44-0eed3a9f0943%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-10-15%2019%3A00%3A35%22%2C%22id%22%3A%22da66cde7-c8d8-4ae2-822a-52ce19a588b3%22%2C%22title%22%3A%22Checkmarx%20Reliability%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.0693069306930694%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-12-26%2017%3A35%3A02%22%2C%22id%22%3A%2281cdf0a3-ce20-41e2-90e6-5d0a888fdc73%22%2C%22title%22%3A%22Checkmarx%20Baseline%20%26%20Fullscan%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.762376237623762%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-11-22%2005%3A13%3A39%22%2C%22id%22%3A%22520e708d-9654-4493-89b2-0a6f8f628b69%22%2C%22title%22%3A%22devops.softtech%20development%20%26%20integrations%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.742574257425742%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A4%2C%22startDate%22%3A%222023-11-09%2007%3A50%3A29%22%2C%22id%22%3A%2222db56ce-18ae-4b51-9d1b-41eedf1ac9e1%22%2C%22title%22%3A%22Scoretfs.isbank%20support%22%2C%22description%22%3A%22%22%2C%22duration%22%3A11.089108910891088%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Lane%202%22%2C%22color%22%3A%7B%22lane%22%3A%22%233b7fc4%22%2C%22bar%22%3A%22%236c9fd3%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22title%22%3A%22Vector%20DB%22%2C%22description%22%3A%22This%20is%20the%20third%20bar.%22%2C%22startDate%22%3A%222023-10-02%2001%3A39%3A48%22%2C%22duration%22%3A17.97029702970297%2C%22rowIndex%22%3A0%2C%22id%22%3A%22e2e17977-b3eb-4cdb-a1ba-8a6899b209cc%22%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-10-26%2006%3A10%3A41%22%2C%22id%22%3A%227efbc564-c821-41f4-a00d-589df6e07c2b%22%2C%22title%22%3A%22PostgreSQL%20maintenance%20%26%20upgrade%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.237623762376238%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-12-05%2002%3A36%3A49%22%2C%22id%22%3A%221cc26fc1-ec2b-4b53-a447-e42b55d42cd0%22%2C%22title%22%3A%22MySQL%20maintenance%20%26%20upgrade%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.01980198019802%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222024-01-03%2008%3A33%3A16%22%2C%22id%22%3A%22057e1ca5-3420-4fd7-816f-5a966b8f276d%22%2C%22title%22%3A%22Audit%20logs%20collection%20%26%20visibility%22%2C%22description%22%3A%22%22%2C%22duration%22%3A4.653465346534653%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22Platform%22%2C%22color%22%3A%7B%22lane%22%3A%22%23d04437%22%2C%22bar%22%3A%22%23dc7369%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-10-16%2014%3A58%3A13%22%2C%22id%22%3A%22eee14dc2-8407-4933-b712-86bd182dc200%22%2C%22title%22%3A%22Zabbix%20-%20increase%20monitoring%20coverage%22%2C%22description%22%3A%22%22%2C%22duration%22%3A6.079207920792079%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-12-28%2021%3A08%3A54%22%2C%22id%22%3A%22167aaf8c-803c-4d11-88fb-f1d1628efb6c%22%2C%22title%22%3A%22Close%20old%20anthos%20cluster%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.396039603960396%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-10-28%2023%3A02%3A58%22%2C%22id%22%3A%22cc9fad7b-f22c-4a74-95c1-0cf1b0c5530c%22%2C%22title%22%3A%22Disk%2FResource%20cleaning%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.0297029702970297%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-10-16%2009%3A58%3A48%22%2C%22id%22%3A%229bd7bf9f-dc6d-444b-b88a-25dd8c0ea49d%22%2C%22title%22%3A%22Github%20upgrade%22%2C%22description%22%3A%22%22%2C%22duration%22%3A5.455445544554456%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A2%2C%22startDate%22%3A%222023-11-19%2019%3A00%3A35%22%2C%22id%22%3A%226108b41a-8911-4858-a5ac-a27113151954%22%2C%22title%22%3A%22GCR%20-%3E%20Harbor%20%20transformation%22%2C%22description%22%3A%22%22%2C%22duration%22%3A7.752475247524752%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A1%2C%22startDate%22%3A%222023-11-29%2020%3A11%3A52%22%2C%22id%22%3A%22a9cb7f67-773a-4e3d-8753-09938a21b854%22%2C%22title%22%3A%22Regular%20maintenance%20work%20(certificate%2C%20ELK%2C%20prometheous%2C%20grafana)%22%2C%22description%22%3A%22%22%2C%22duration%22%3A9.584158415841584%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A3%2C%22startDate%22%3A%222023-11-26%2004%3A02%3A22%22%2C%22id%22%3A%2223af2ad8-81b3-4493-86a8-a91f2f190ecd%22%2C%22title%22%3A%22containerd%20transformation%22%2C%22description%22%3A%22%22%2C%22duration%22%3A2.910891089108911%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A4%2C%22startDate%22%3A%222023-10-12%2021%3A08%3A54%22%2C%22id%22%3A%22bd8e3309-2a95-4c56-bba7-e5cef9bd4a61%22%2C%22title%22%3A%22Mocha%20Deployment%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.4257425742574257%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A4%2C%22startDate%22%3A%222023-11-07%2020%3A54%3A39%22%2C%22id%22%3A%221c74914f-1efa-4b43-8492-8b63c696e5d0%22%2C%22title%22%3A%22SonarQube%20New%20setup%20for%20Softtech%20Projects%22%2C%22description%22%3A%22%22%2C%22duration%22%3A3.405940594059406%2C%22pageLink%22%3A%7B%7D%7D%2C%7B%22rowIndex%22%3A5%2C%22startDate%22%3A%222023-11-23%2002%3A51%3A05%22%2C%22id%22%3A%220035d8da-e15c-4e5a-90d5-26f6c1b911ad%22%2C%22title%22%3A%22Cost%20Dashboard%22%2C%22description%22%3A%22%22%2C%22duration%22%3A9.96039603960396%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%2C%7B%22title%22%3A%22New%20Lane%22%2C%22color%22%3A%7B%22lane%22%3A%22%238eb021%22%2C%22bar%22%3A%22%23aac459%22%2C%22text%22%3A%22%23ffffff%22%2C%22count%22%3A1%7D%2C%22bars%22%3A%5B%7B%22rowIndex%22%3A0%2C%22startDate%22%3A%222023-10-16%2008%3A19%3A00%22%2C%22id%22%3A%221042f762-4171-48cd-98db-602f7d2df1e6%22%2C%22title%22%3A%22Pob%2CProemtia%2CSmartfarm%2CQPay%2C%C4%B0%C5%9FFaktoring%22%2C%22description%22%3A%22%22%2C%22duration%22%3A15.94059405940594%2C%22pageLink%22%3A%7B%7D%7D%5D%7D%5D%2C%22markers%22%3A%5B%7B%22title%22%3A%22Marker%201%22%2C%22markerDate%22%3A%222023-10-15%2000%3A00%3A00%22%7D%5D%7D Roadmap%20Planner 151dd43b77d7b9edfc631067bf7ba8ed","{'title': 'Roadmap 2023', 'id': '77727411', 'source': 'https://wiki.softtech.com.tr/display/SDO/Roadmap+2023'}"
"1- NexusIQ Taraması : Banka Projeleri : Score TFS pipeline üzerinden yapılıyor. NexusIQ taraması XlReleaseden bir adımda, ScoreTfs de başlatılan bir build başlatılıyor. Bu sekilde tetikleniyor ( https://scoretfs.isbank/ISBANK/SOFTWARE_QA/_build ) Softtech Rally Projeleri Jenkins pipeline üzerinde tanımlanan job ile schedule ediliyor, http://10.80.36.112:8080/ , paketlerin repo server : https://gitlab.rally.softtech/ NexusIQ tarama server: https://nexusiq.softtech:8443 Çalışma saatleri: 05:00, 2- NexusIQ sürüm kesme akışı Sürüm kontrolleri Banka için geliştirilen uygulamalar için uygulanacak olup, “ Ocak” ve “ Mart” kontrolleri olacak şekilde parçalı bir devreye alım yapılacaktır. Bu parçalı devreye alım ile ekiplerin Nexus IQ Server ürününün tespit ettiği açıklıklara ilişkin aksiyonlarını tamamlaması amaçlanmıştır. Ocak ayı itibari ile devreye aldığımız kontrollerde aşağıdaki kontroller baz alınacaktır. Yeni oluşan ve ilk kez taramadan geçen bir pakette  CVSS skoru “7 ve üzeri” raporlanmış “Kritik” önem dereceli bulgu sayısının “0” olması gerekecektir. Tarama sonucunda, pakette tespit edilen CVSS skoru “ 7 ve üzeri” raporlanmış “Kritik” önem dereceli bulgu sayısının üzerinde bir bulgu artışı var ise sürüm geçişine izin verilmeyecektir. (Örneğin, tarama sonucunda 10 adet Kritik seviyeli bulgu raporlanmışsa, tarama sonrası bu sonucun 10’un üstüne çıkmaması gerekecektir.) Mart ayı itibari ile Ocak ayında devreye aldığımız kontrollere ek olarak  aşağıdaki kontrol eklenecektir. Tarama sonucunda CVSS skoru “ 7 ve üzeri” raporlanmış “Kritik” önem dereceli bulgu sayısı toplamının “ 0” olması gerekecektir.","{'title': 'Nexus IQ Entegrasyonu', 'id': '77727505', 'source': 'https://wiki.softtech.com.tr/display/SDO/Nexus+IQ+Entegrasyonu'}"
"Gitlab.rally.softtech üzerinde bulunan projeleri Checkmarx'ta ki tarama sonuçları,  DevopsSofttech uygulaması tarafından plateau devops pipeline'a gönderiliyor. ( http://devopsutil.util.rally.softtech/checkmarx ) Checkmarx sonuçlarına göre hattı kırılıyor yada kırılmıyor, bunun kontrolu  hat ekibinin geliştirdiği GCP üzerinde çalışan bir API uygulaması.","{'title': 'Rally Projeleri Checkmarx sürüm kırma akışı', 'id': '79462695', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=79462695'}"
,"{'title': 'Smart Farm Projesi', 'id': '80478962', 'source': 'https://wiki.softtech.com.tr/display/SDO/Smart+Farm+Projesi'}"
Info Emtia Projesi Sunucu Listesi No Sunucu Adı Ortam IP İşlemci Bellek (GB) Disk (GB) Açıklama Durum beta-admin-host02 Beta 10.224.1.102 4 8 40 Admin Node Aktif beta-admin-host11 Beta 10.224.1.111 4 8 40 Admin Node Aktif beta-admin-host12 Beta 10.224.1.112 4 8 40 Admin Node Aktif beta-emtia-worker01 Beta 10.224.7.101 8 48 100 Worker Node Aktif beta-emtia-worker02 Beta 10.224.7.102 8 48 100 Worker Node Aktif beta-emtia-worker03 Beta 10.224.7.103 8 48 100 Worker Node Aktif EMTIA-POSTGRE01-BETA Beta 10.224.7.51 2 8 50 PostgreSQL DB sunucusu Aktif EMTIA-POSTGRE02-BETA Beta 10.224.7.52 2 8 50 PostgreSQL DB sunucucu Aktif EMTIA-POSTGREBCKP-BETA Beta 10.224.7.59 2 8 50 PostgreSQL Backup sunucusu Aktif EMTIA-POSTGREMGMT01-BETA Beta 10.224.7.56 2 8 50 PostgreSQL Management Sunucusu Aktif EMTIA-POSTGREMGMT02-BETA Beta 10.224.7.57 2 8 50 PostgreSQL Management Sunucusu Aktif EMTIA-POSTGREMGMT03-BETA Beta 10.224.7.58 2 8 50 PostgreSQL Management Sunucusu Aktif srv-emtia-beta-nfs-server-01 Beta 10.224.7.15 2 8 50 NFS sunucusu Aktif beta-emtia-data-plateau-01 Beta 10.224.7.20 2 8 50 Data Plateau sunucusu Aktif srv-beta-emtia-jmeter Beta 10.224.7.16 2 8 50 Jmeter Sunucusu Aktif prod-admin-host13 Prod 10.224.2.113 4 8 40 Admin Node Aktif prod-admin-host14 Prod 10.224.2.114 4 8 40 Admin Node Aktif prod-admin-host15 Prod 10.224.2.115 4 8 40 Admin Node Aktif prod-emtia-worker01 Prod 10.224.8.101 8 48 100 Worker Node Aktif prod-emtia-worker02 Prod 10.224.8.102 8 48 100 Worker Node Aktif prod-emtia-worker03 Prod 10.224.8.103 8 48 100 Worker Node Aktif EMTIA-POSTGRE01-PROD Prod 10.224.8.51 4 16 300 PostgreSQL DB sunucusu Aktif EMTIA-POSTGRE02-PROD Prod 10.224.8.52 4 16 300 PostgreSQL DB sunucusu Aktif EMTIA-POSTGREMGMT01-PROD Prod 10.224.8.56 2 4 50 PostgreSQL Management Sunucusu Aktif EMTIA-POSTGREMGMT02-PROD Prod 10.224.8.57 2 4 50 PostgreSQL Management Sunucusu Aktif EMTIA-POSTGREMGMT03-PROD Prod 10.224.8.58 2 4 50 PostgreSQL Management Sunucusu Aktif EMTIA-POSTGREBCKP-PROD Prod 10.224.8.59 2 8 100 PostgreSQL Backup sunucusu Aktif srv-emtia-nfs-server-prod-01 Prod 10.224.8.15 2 8 100 NFS sunucusu Aktif emtia-data-plateau-prod-01 Prod 10.224.8.20 8 32 200 Data Plateau sunucusu Aktif,"{'title': 'Emtia Projesi', 'id': '80478964', 'source': 'https://wiki.softtech.com.tr/display/SDO/Emtia+Projesi'}"
"Sunucu Kurulum Gereksinimleri Ubuntu Jammy 22.04 LTS 2 vCPU 16GB Memory 20 GB Disk Docker Kurulumu Docker engine kurulum için aşağıdaki linkteki yönergeler takip edilerek kurulum tamamlanır. Docker engine versiyonu için “ latest” tercih edilerek devam edilir. https://docs.docker.com/engine/install/ubuntu/ Set up the repository HTTPS üzerinden repository kullanmak için aşağıdaki komutlar çalıştırılır. # sudo apt-get update # sudo apt-get install \ ca-certificates \ curl \ gnupg \ lsb-release Docker official key eklenir ve repository kurulumu tamamlanır. # sudo mkdir -p /etc/apt/keyrings # curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg # echo \ ""deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) stable"" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Docker Engine Kurulumu # sudo apt-get update # sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin Docker engine kurulumunu test etmek için aşağıdaki komut çalıştırılır. # sudo docker run hello-world MSSQL Kurulumu Ubuntu üzerinde docker container olarak çalışacak mssql kurulum için aşağıdaki link üzerindeki image kullanılarak kurulum yapılır. Docker engine için Microsoft SQL Server container image versiyonu “ 2022-latest” tercih edilir. https://hub.docker.com/_/microsoft-mssql-server Container MSSQL Server ayağa kaldırma Container data volumelerinin sunucu dizinine mount edilmesi için sunucuda dizin oluşturulur. # mkdir -p /data/mssql # sudo docker run --rm -it mcr.microsoft.com/mssql/server:2022-latest id mssql # chown 10001 /data/mssql # docker run --restart always --name $container-name -e ""ACCEPT_EULA=Y"" -e ""MSSQL_SA_PASSWORD=hPvcgtmuD8YDDvaz"" -p 1433:1433 -v /data/ mssql:/var/opt/mssql/data -d mcr.microsoft.com/mssql/server:2022-latest # docker ps -a # docker logs -f $container-name # docker exec -it $container-name sh MSSQL Server DB, User, Password Oluşturma MSSQL Workbench üzerinden “Fiba” isimli DB create edilir. Sonrasında user, password ve yetkilendirmeler için aşağıdaki komutlar çalıştırılır. USE fiba; DECLARE @sessionId INT; DECLARE @statement NVARCHAR(200); SELECT @sessionId=session_id FROM sys.dm_exec_sessions WHERE login_name = N'notificationuser'; SET @statement = 'KILL ' + CAST(@sessionId AS NVARCHAR(20)); EXEC sp_executesql @statement; IF EXISTS (SELECT * FROM dbo.syslogins WHERE name = N'notificationuser') BEGIN DROP LOGIN notificationuser END; CREATE LOGIN notificationuser WITH PASSWORD = '2queOcQTMKzBeEpZ', CHECK_POLICY=OFF; IF EXISTS(SELECT * FROM sys.database_principals WHERE name = 'notificationuser') BEGIN DROP USER notificationuser END; CREATE USER notificationuser FOR LOGIN notificationuser; IF NOT EXISTS ( SELECT  * FROM    sys.schemas WHERE   name = N'notification' ) EXEC('CREATE SCHEMA [notification];'); EXEC sp_addrolemember N'db_owner', N'notificationuser'; GRANT SELECT, INSERT, UPDATE, DELETE ON SCHEMA :: [notification] TO notificationuser; ALTER USER notificationuser WITH DEFAULT_SCHEMA = [notification]; GRANT CREATE TABLE TO [notificationuser]; Migration ve Notification Service’in ayağa kaldırılması Container içerisindeki logların sunucuya mount edilmesi için sunucuda aşağıdaki dizin oluşturulur. # mkdir -p /data/notification/logs Docker-compose.yaml’ın bulunduğu dizine iletilen gcr-io'dan image pull edilebilmesi için key dosyası(gcr-io.json) ile erişim kontrol edilir. # docker login -u _json_key -p ""$(cat gcr-io.json)"" https://gcr.io Docker-compose.yaml içerisindeki environmentlar için .env dosya içeriği için gerekli bilgiler girilir ve bu 2 dosya aynı dizine taşınarak config check edilir. # docker compose config Docker-compose.yaml dosyasının bulunduğu dizin üzerinde aşağıdaki komutlarla initial db çalışması ve notification container servisin ayağa kalkması sağlanır. # docker compose up # docker ps 250 250","{'title': 'Plateau Notification Kurulumu', 'id': '80479002', 'source': 'https://wiki.softtech.com.tr/display/SDO/Plateau+Notification+Kurulumu'}"
,"{'title': 'DevOps Araçları', 'id': '80479276', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80479276'}"
"Word dökümanına bağlantı üzerinden erişilebilir : GİRİŞ Bu belge, PostgreSQL veri tabanı sunucu ortamları için kurulması istenen cluster yapısı için gerekli olan bileşenleri ve kurulum adımlarını ele alır. PostgreSQL cluster kurulumu için geliştirilmiş birden fazla araç bulunmaktadır. Bknz: PAF, Repmgr, Pg_auto_failover, BDR (EDB), Patroni. Bu belgede, Patroni ve gerekli bileşenleri ile PostgreSQL cluster kurulumu ele alınmaktadır. PostgreSQL cluster kurulumunda Patroni'nin tercih edilmesinin sebebi; open source bir çözüm olması, güçlü bir topluluk desteğine sahip olması, cluster ortamını tutarlı, doğru bir şekilde yönetmesi ve esnek cluster yönetimini sağlıyor olmasıdır. Patroni öncesinde yaşanan bazı sorunlar; olmaması gerektiği halde failover durumlarının tetiklenmesi ( false – failover ) veya olması gerektiği halde failover durumunun tetiklenmemesi dolayısıyla split-brain sorununun tam olarak çözülemiyor olması, cluster yönetimini yapan araçların performans sorunları, fail olan sunucuların cluster'a yeniden otomatik olarak eklenememesi dolayısıyla cluster'ın esnek bir şekilde ölçeklendirilememesi ( yeni nodeların kolayca eklenememesi ) gibi sorunlar örnek verilebilir. KURULUM GEREKSİNİMLERİ Cluster kurulumu Patroni ile yapılmaktadır. Patroni, PostgreSQL'in cluster yönetiminin otomatikleştirilmesi için geliştirilmiş bir template 'tir. Patroni, tüm PostgreSQL süreçlerini ""sadece ""yönetmekten sorumludur ve hiç bir şekilde PostgreSQL'in internal yapısına müdahelede bulunmaz. Konfigürasyon değişikliklerinin tüm nodelarda doğru bir şekilde uygulanması, nodelar arası latency takibinin yapılması, failover – switchover süreçlerinin yerine getirilmesi, yeni nodeların clustera dahil edilmesi veya belirli nodeların clusterdan dışarı çıkartılması gibi temel unsurlardan sorumludur. Patroni, tüm bu süreci yönetirken split-brain (aynı anda 1'den fazla node'un write işlemlerini kabul etmesi durumu) denilen senaryodan kaçınmak için bazı consensus araçlarına güvenir. Bu araçlar, ETCD, Consul, Zookeeper olabilir. Bu tür araçlar, RAFT protokolü denilen bir consensus algoritması kullanılırlar. Dolayısıyla Patroni, bu araçlar üzerinden dolaylı olarak RAFT protokolünü kullanır. Hangi PostgreSQL node'u write işlemlerine açık olacak? Kaç tane replikasyon node'u mevcut? Nodeların sağlık durumları nedirl? Her bir node'un IP adresi nedir? Cluster'a eklenen yeni bir node var mı? gibi soruların cevabını Patroni, consensus araçları üzerinden bulur. RAFT Protokolü Patroni'nin çalışma yapısını anlamak için, bu protokolün çalışma yapısını anlamak oldukça önem arz eder. Çünkü, Patroni tüm süreci bu protokole güvenerek yürütmektedir. RAFT protokolü için, Split-Brain durumuna önlem olarak geliştirilmiş bir consensus algoritmasıdır diyebiliriz. Bu algoritma, 1'den fazla node'un olduğu bir ortamda bir t anında her zaman sadece bir node 'un yazma işlemine açık olmasını garanti altına alabilmek için bir oylama yönetimi kullanır. Oylama sonucuna göre ortamdaki üyelerden bir tanesi lider olarak seçilir ve geriye kalan tüm üyeler follower durumunda olup, belirli zaman aralıklarında lideri takip ederler. Başlangıçta tüm üyeler – liderliğe aday – denilen bir pozisyonda konumlanırlar. Bu aşamada tüm üyeler bir birlerinden oy talep ederler. Oylama sürecinin sonunda en çok oy alan node, kendisini lider olarak ilan eder ve liderlik kilidini ele aldığını ortamdaki diğer tüm üyelere bildirerek diğer üyelerin kendisini takip etmelerini sağlar. İşte bu liderlik seçiminin yapılabilmesi için oy çokluğu gerekmektedir. Bu duruma quorum denilir. Quorum, yeter sayıda oy demektir. Diğer bir deyişle consensus'un sağlanmasıdır. Liderin seçilebilmesi için ortamdaki toplam üyelerin en az yarısından 1 fazlası sağlıklı, aktif, faal durumda olması gerekmektedir. Aksi taktirde, yeter sayıda oy çokluğu sağlanamayacağı için lider seçimi yapılamaz ve clusterın düşmesi gerçekleşir. N = clusterdaki toplam üye sayısı ise, aktif durumda olması istenen üye sayısı minimum (N / 2) + 1'dir. Dolayısıyla, oy çokluğunun sağlanabilmesi için üye sayısı tek hanelerde bir sayıda olması ve en az 3 adet olması gerekmektedir. Örneğin; 3, 5, 7, 9 gibi. RAFT protokolü için, split-brain'i önlemek için demokrasinin kullanıldığı, hakim olduğu bir protocoldür diyebiliriz. KURULUM AYRINTILARI Patroni, cluster yönetimini gerçekleştirebilmesi için aşağıdaki araçlara ihtiyaç duymaktadır: 1. HAProxy - Proxy & TCP Load Balance 2. KeepAlived – Floating IP 3. ETCD / Consul / Zookeeper – Distributed Configuration Store Yukarıdaki bu 3 araçların olduğu ortamları biz yönetim ortamı olarak tanımlıyoruz. Bu araçlar için, management ortamlarımız diyebiliriz. 4. Patroni & PostgreSQL – Template & DB Server 5. PgBouncer – Connection Pooling 6. PgBackRest – Backup & Restore Yukarıdaki araçlardan PgBouncer, connection pooling aracı olarak; PgBackRest ise veri tabanı backup / restore yapısı için gerekmektedir. Bu araçlar, Patroni'nin çalışması için zorunlu olan araçlar değildir. Sağlıklı bir cluster ortam kurulumu için gereklidir. SİSTEM İHTİYAÇLARI Kurulum yapılacak olan sunucuların tüm işletim sistemi dağıtımları, versiyonları aynı sevide olmalıdır. Debian ve RedHat tabanlı bir dağıtımda olan bir işletim sistemi kullanılabilir. Debian için Ubuntu , RedHat için Rocky Linux tercih edilebilir. Yönetim makineleri için minimum kaynak ihtiyacı: CPU: 2x RAM: 4 GB DISK: 50 GB Veri Tabanı makineleri için minimum kaynak ihtiyacı: ( İhtiyaca göre değiştirilebilir ) CPU: 4x RAM: 8 GB DISK: 100 GB Yedekleme makinesi için minimum kaynak ihtiyacı: ( İhtiyaca göre değiştirilebilir ) CPU: 4x RAM: 8 GB DISK: 100 GB ORTAM ÇİZİMLERİ BİLEŞENLERİN KURULUMLARI Patroni'nin, süreçlerini yürütebilmesi için DCS araçlarına ihtiyaç duyduğundan bahsetmiştik. Kurulacak cluster'ın temelini oluşturacak ortam bu araçlardır. Dolayısıyla, kuruluma bu araçlardan başlanılabilir. Burada tercih ettiğimiz araç ETCD ve bu aşamada Ubuntu dağıtımı üzerinde ETCD 'nin kurulumundan devam edeceğiz. Aşağıdaki kurulum işlemleri tüm management makineleri üzerinde ayrı ayrı gerçekleştirilir. [ root@sunucu ~]# sudo apt update -y [ root@sunucu ~]# sudo apt install vim wget curl [ root@sunucu ~]# export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest|grep tag_name | cut -d '""' -f 4) [ root@sunucu ~]# wget https://github.com/etcd-io/etcd/releases/download/${RELEASE}/etcd-${RELEASE}-linux-amd64.tar.gz [ root@sunucu ~]# tar xvf etcd-${RELEASE}-linux-amd64.tar.gz [ root@sunucu ~]# cd etcd-${RELEASE}-linux-amd64 [ root@sunucu ~]# sudo mv etcd etcdctl etcdutl /usr/local/bin [ root@sunucu ~]# sudo mkdir -p /var/lib/etcd/ [ root@sunucu ~]# sudo mkdir /etc/etcd [ root@sunucu ~]# sudo groupadd --system etcd [ root@sunucu ~]# sudo useradd -s /sbin/nologin --system -g etcd etcd [ root@sunucu ~]# sudo chown -R etcd:etcd /var/lib/etcd/ ETCD kurulumunun ardından bu makineler üzerinde HAProxy ve KeepAlived kurulumları gerçekleştirilir: [ root@sunucu ~] # sudo apt install haproxy keepalived -y Tüm management makinelerinde HAProxy, KeepAlived, ETCD kurulumları tamamlandıktan sonra bir sonraki aşama olan, tüm veri tabanı sunucularında PgBouncer , Patroni , PostgreSQL ve PgBackRest kurulumları gerçekleştirilir. Patroni, Python ile yazıldığı için öncelikli olarak Python ve gerekli bağımlılıkları yüklenir ardından Patroni kurulumu yapılır: [ root@sunucu ~]# sudo apt-get install python3-psycopg2 -y [ root@sunucu ~]# sudo apt-get install python3-pip -y [ root@sunucu ~]# pip3 install psycopg2-binary [ root@sunucu ~]# pip3 install patroni[etcd] Python, Patroni kurulumlarından sonra PostgreSQL , PgBouncer ve PgBackRest kurulumu yapılır: [ root@sunucu ~]# sudo sh -c 'echo ""deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list' [ root@sunucu ~]# wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - [ root@sunucu ~]# sudo apt-get update [ root@sunucu ~]# sudo apt-get -y install pgbackrest pgbouncer postgresql postgresql-contrib [ root@sunucu ~]# systemctl stop postgresql.service [ root@sunucu ~]# systemctl disable postgresql.service [ root@sunucu ~]# rm -rf /var/lib/postgresql/ installed_version /* Yukarıdaki kurulumlar tamamlandıktan sonra Patroni'nin, PostgreSQL ile beraber gelen bazı binaryleri kullanabilmesi için binary dizininde symlink tanımlaması yapılmalı ve Patroni'nin kullanacağı servis dosyasının hazırlanması gerekmektedir: NOT: Aşağıdaki komuttaki installed_version ifadesi, kurulu olan PostgreSQL'in versiyonunu ifade etmektedir. Örn; /usr/lib/postgresql/15/bin/ * [ root@sunucu ~]# ln -s /usr/lib/postgresql/ {installed_version} /bin/* /usr/sbin/ Yukarıdaki komut ile symlink tanımlaması yapıldıktan sonra, Patroni'nin servis dosyası oluşturulur: [ root@sunucu ~]# vi /etc/systemd/system/patroni.service Oluşturulan Patroni servis dosyasının içerisine aşağıdaki gibi servis tanımları girilir: [Unit] Description=Runners to orchestrate a high-availability PostgreSQL After=syslog.target network.target [Service] Type=simple User=postgres Group=postgres ExecStart=/usr/local/bin/patroni /etc/patroni/patroni.yml KillMode=process TimeoutSec=30 Restart=no [Install] WantedBy=multi-user.target Servis tanımları kayıt edilip ilgili ekrandan çıkıldıktan sonra, Patroni'nin dizinleri oluşturulur ve yetki atamaları yapılır: [ root@sunucu ~]# mkdir /etc/patroni [ root@sunucu ~]# touch /etc/patroni/patroni.yml [ root@sunucu ~]# chown -R postgres:postgres /etc/patroni [ root@sunucu ~]# chmod -R 700 /etc/patroni [ root@sunucu ~]# mkdir -p /var/log/patroni [ root@sunucu ~]# chown -R postgres:postgres /var/log/patroni [ root@sunucu ~]# chmod -R 700 /var/log/patroni Yedek sunucusu üzerinde de PostgreSQL ve PgBackRest kurulumları yapılır: [ root@sunucu ~]# sudo sh -c 'echo ""deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list' [ root@sunucu ~]# wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add – [ root@sunucu ~]# sudo apt-get update [ root@sunucu ~]# sudo apt-get -y install pgbackrest postgresql postgresql-contrib KONFİGÜRASYONLARIN TANIMLANMASI Tüm management makinelerinde ETCD konfigürasyonları sırasıyla ayrı ayrı yapılır: 1. Management Makinesi Üzerinde [ root@sunucu ~]# export IP_1= { 1. MANAGEMENT SUNUCU IP ADRESİ } export IP_2= { 2. MANAGEMENT SUNUCU IP ADRESİ } export IP_3= { 3. MANAGEMENT SUNUCU IP ADRESİ } [ root@sunucu ~]# cat > /etc/systemd/system/etcd.service <<EOF [Unit] Description=etcd Documentation= https://github.com/coreos/etcd After=network.target Wants=network-online.target [Service] Type=notify Restart=always RestartSec=5s LimitNOFILE=40000 TimeoutStartSec=0 ExecStart=etcd --name etcd-1 \ --data-dir /var/lib/etcd \ --listen-client-urls http://${IP_1}:2379, http://localhost:2379 \ --advertise-client-urls http://${IP_1}:2379 \ --listen-peer-urls http://${IP_1}:2380 \ --initial-advertise-peer-urls http://${IP_1}:2380 \ --initial-cluster etcd-1=http://${IP_1}:2380,etcd-2=http://${IP_2}:2380,etcd-3=http://${IP_3}:2380 \ --initial-cluster-token etcd-cluster-token \ --initial-cluster-state new  \ --enable-v2=true [Install] WantedBy=multi-user.target EOF 2. Management Makinesi Üzerinde [ root@sunucu ~]# export IP_1= { 1. MANAGEMENT SUNUCU IP ADRESİ } export IP_2= { 2. MANAGEMENT SUNUCU IP ADRESİ } export IP_3= { 3. MANAGEMENT SUNUCU IP ADRESİ } [ root@sunucu ~]#  cat > /etc/systemd/system/etcd.service <<EOF [Unit] Description=etcd Documentation= https://github.com/coreos/etcd After=network.target Wants=network-online.target [Service] Type=notify Restart=always RestartSec=5s LimitNOFILE=40000 TimeoutStartSec=0 ExecStart=etcd --name etcd-2 \ --data-dir /var/lib/etcd \ --listen-client-urls http://${IP_2}:2379, http://localhost:2379 \ --advertise-client-urls http://${IP_2}:2379 \ --listen-peer-urls http://${IP_2}:2380 \ --initial-advertise-peer-urls http://${IP_2}:2380 \ --initial-cluster etcd-1=http://${IP_1}:2380,etcd-2=http://${IP_2}:2380,etcd-3=http://${IP_3}:2380 \ --initial-cluster-token etcd-cluster-token \ --initial-cluster-state new \ --enable-v2=true [Install] WantedBy=multi-user.target EOF 3. Management Makinesi Üzerinde [ root@sunucu ~]# export IP_1= { 1. MANAGEMENT SUNUCU IP ADRESİ } export IP_2= { 2. MANAGEMENT SUNUCU IP ADRESİ } export IP_3= { 3. MANAGEMENT SUNUCU IP ADRESİ } [ root@sunucu ~]# cat > /etc/systemd/system/etcd.service <<EOF [Unit] Description=etcd Documentation= https://github.com/coreos/etcd After=network.target Wants=network-online.target [Service] Type=notify Restart=always RestartSec=5s LimitNOFILE=40000 TimeoutStartSec=0 ExecStart=etcd --name etcd-3 \ --data-dir /var/lib/etcd \ --listen-client-urls http://${IP_3}:2379, http://localhost:2379 \ --advertise-client-urls http://${IP_3}:2379 \ --listen-peer-urls http://${IP_3}:2380 \ --initial-advertise-peer-urls http://${IP_3}:2380 \ --initial-cluster etcd-1=http://${IP_1}:2380,etcd-2=http://${IP_2}:2380,etcd-3=http://${IP_3}:2380 \ --initial-cluster-token etcd-cluster-token \ --initial-cluster-state new \ --enable-v2=true [Install] WantedBy=multi-user.target EOF Tüm management makinelerinde ETCD konfigürasyonları tamamlandıktan sonra tüm sunucularda etcd servisleri enabled edilip, çalıştırılır ve etcd cluster’ının doğru bir şekilde yapılandırıldığının kontrolü sağlanır. Tüm management makinelerindeki etcd servisleri başlatılır : [ root@sunucu ~]#  sudo systemctl enable etcd.service [ root@sunucu ~]#  sudo systemctl start etcd.service Tüm makinelerde etcd servisi çalıştıktan sonra, cluster kontrolü yapılır. Kontrol sonucunda 1 tane lider ve 2 tane follower sunucu olması gerekmektedir: [ root@sunucu ~]# export IP_1= { 1. MANAGEMENT SUNUCU IP ADRESİ } export IP_2= { 2. MANAGEMENT SUNUCU IP ADRESİ } export IP_3= { 3. MANAGEMENT SUNUCU IP ADRESİ } export ETCDCTL_API=3 ENDPOINTS=$IP_1:2379,$IP_2:2379,$IP_3:2379 etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status Tüm management makinelerinde ETCD konfigürasyonları tamamlandıktan sonra yine aynı sunucular üzerinde HAProxy ayarlamaları yapılır: 1. Management Makinesi Üzerinde vi /etc/haproxy/haproxy.cfg komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global maxconn                 100000 log /dev/log           local0 log /dev/log           local1 notice chroot                     /var/lib/haproxy stats socket           /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout        30s user                        haproxy group                     haproxy daemon defaults mode                      tcp log                          global retries                     2 timeout queue       5s timeout connect    5s timeout client        60m timeout server       60m timeout check        15s listen stats mode                        http bind { 1. MANAGEMENT SUNUCU IP ADRESİ } :7000 stats                         enable stats                         uri / listen                               master bind { VIP IP ADRESİ } :5432 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /master http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ }:6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas bind { VIP IP ADRESİ } :5433 maxconn                   10000 option                        tcplog option                        httpchk OPTIONS /replica balance                      roundrobin http-check                 expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ }:6432 check port 8008 listen                               replicas_sync bind { VIP IP ADRESİ } :5434 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /sync balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ }:6432 check port 8008 listen                               replicas_async bind { VIP IP ADRESİ } :5435 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /async balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ }:6432 check port 8008 2. Management Makinesi Üzerinde vi /etc/haproxy/haproxy.cfg komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global maxconn                 100000 log /dev/log           local0 log /dev/log           local1 notice chroot                     /var/lib/haproxy stats socket           /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout        30s user                        haproxy group                     haproxy daemon defaults mode                       tcp log                            global retries                      2 timeout queue       5s timeout connect    5s timeout client        60m timeout server       60m timeout check        15s listen stats mode                        http bind { 2. MANAGEMENT SUNUCU IP ADRESİ }:7000 stats                         enable stats                         uri / listen                               master bind { VIP IP ADRESİ } :5432 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /master http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas bind { VIP IP ADRESİ } :5433 maxconn                   10000 option                        tcplog option                        httpchk OPTIONS /replica balance                      roundrobin http-check                 expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas_sync bind { VIP IP ADRESİ } :5434 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /sync balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas_async bind { VIP IP ADRESİ } :5435 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /async balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 3. Management Makinesi Üzerinde vi /etc/haproxy/haproxy.cfg komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global maxconn                 100000 log /dev/log           local0 log /dev/log           local1 notice chroot                     /var/lib/haproxy stats socket           /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout        30s user                        haproxy group                     haproxy daemon defaults mode                       tcp log                            global retries                      2 timeout queue       5s timeout connect    5s timeout client        60m timeout server       60m timeout check        15s listen stats mode                        http bind { 3. MANAGEMENT SUNUCU IP ADRESİ } :7000 stats                         enable stats                         uri / listen                               master bind { VIP IP ADRESİ } :5432 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /master http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas bind { VIP IP ADRESİ } :5433 maxconn                   10000 option                        tcplog option                        httpchk OPTIONS /replica balance                      roundrobin http-check                 expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas_sync bind { VIP IP ADRESİ } :5434 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /sync balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 listen                               replicas_async bind { VIP IP ADRESİ } :5435 maxconn                    10000 option                         tcplog option                         httpchk OPTIONS /async balance                       roundrobin http-check                  expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server { 1. DB MAKİNESİ HOST NAME } { 1. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 server { 2. DB MAKİNESİ HOST NAME } { 2. DB MAKİNESİ IP ADRESİ } :6432 check port 8008 HAProxy ile ilgili tüm ayarlamalar tamamlandıktan sonra tüm management sunucuları üzerinde KeepAlived ayalamaları yapılır: 1. Management Makinesi Üzerinde vi /etc/keepalived/keepalived.conf komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script "" /usr/bin/killall -0 haproxy "" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id         40 priority 100 advert_int                2 state MASTER virtual_ipaddress { { VIP IP ADRESİ } } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } 2. Management Makinesi Üzerinde vi /etc/keepalived/keepalived.conf komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script "" /usr/bin/killall -0 haproxy "" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id         40 priority 101 advert_int                2 state BACKUP virtual_ipaddress { { VIP IP ADRESİ } } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } 3. Management Makinesi Üzerinde vi /etc/keepalived/keepalived.conf komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script "" /usr/bin/killall -0 haproxy "" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id         40 priority 102 advert_int                2 state BACKUP virtual_ipaddress { { VIP IP ADRESİ } } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } Tüm management makinelerinde kurulum ve konfigürasyon tamamlandıktan sonra tüm veri tabanı sunucularında gerekli ayarlamalar yapılmalıdır. Bunun için ilk olarak patroni konfigürasyonu yapılacak olup akabinde pgbouncer ve pgbackrest ayarlamaları yapılacaktır. 1. Veri Tabanı Makinesi Üzerinde vi /etc/patroni/patroni.yml komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: --- scope: postgres-cluster name: { DB – CLUSTER -NAME } namespace: /service/ restapi: listen: { 1. DB MAKİNESİ IP ADRESİ } :8008 connect_address: { 1. DB MAKİNESİ IP ADRESİ } :8008 etcd: hosts: { 1. MANAGEMENT MAKİNESİ IP ADRESİ } :2379, { 2. MANAGEMENT MAKİNESİ IP ADRESİ } :2379, { 3. MANAGEMENT MAKİNESİ IP ADRESİ } :2379 bootstrap: method: initdb dcs: ttl: 30 loop_wait: 10 retry_timeout: 10 maximum_lag_on_failover: 1048576 master_start_timeout: 300 synchronous_mode: true synchronous_mode_strict: true synchronous_node_count: 1 postgresql: use_pg_rewind: true use_slots: true parameters: max_connections: 500 superuser_reserved_connections: 5 password_encryption: scram-sha-256 max_locks_per_transaction: 64 max_prepared_transactions: 0 huge_pages: try shared_buffers: 512MB work_mem: 128MB maintenance_work_mem: 256MB effective_cache_size: 4GB checkpoint_timeout: 15min checkpoint_completion_target: 0.9 min_wal_size: 2GB max_wal_size: 8GB wal_buffers: 32MB default_statistics_target: 1000 seq_page_cost: 1 random_page_cost: 4 effective_io_concurrency: 2 synchronous_commit: on autovacuum: on autovacuum_max_workers: 5 autovacuum_vacuum_scale_factor: 0.01 autovacuum_analyze_scale_factor: 0.01 autovacuum_vacuum_cost_limit: 500 autovacuum_vacuum_cost_delay: 2 autovacuum_naptime: 1s max_files_per_process: 4096 archive_mode: on archive_timeout: 1800s archive_command: cd . wal_level: replica wal_keep_size: 2GB max_wal_senders: 10 max_replication_slots: 10 hot_standby: on wal_log_hints: on wal_compression: on shared_preload_libraries: pg_stat_statements,auto_explain pg_stat_statements.max: 10000 pg_stat_statements.track: all pg_stat_statements.track_utility: false pg_stat_statements.save: true auto_explain.log_min_duration: 10s auto_explain.log_analyze: true auto_explain.log_buffers: true auto_explain.log_timing: false auto_explain.log_triggers: true auto_explain.log_verbose: true auto_explain.log_nested_statements: true auto_explain.sample_rate: 0.01 track_io_timing: on log_lock_waits: on log_temp_files: 0 track_activities: on track_counts: on track_functions: all log_checkpoints: on logging_collector: on log_truncate_on_rotation: on log_rotation_age: 1d log_rotation_size: 0 log_line_prefix: '%t [%p-%l] %r %q%u@%d ' log_filename: 'postgresql-%a.log' log_directory: /var/log/postgresql hot_standby_feedback: on max_standby_streaming_delay: 30s wal_receiver_status_interval: 10s idle_in_transaction_session_timeout: 10min jit: off max_worker_processes: 24 max_parallel_workers: 8 max_parallel_workers_per_gather: 2 max_parallel_maintenance_workers: 2 initdb:  # List options to be passed on to initdb - encoding: UTF8 - locale: en_US.UTF-8 - data-checksums  pg_hba:  # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host replication replicator { 1. DB MAKİNESİ IP ADRESİ } /0  md5 - host replication replicator { 2. DB MAKİNESİ IP ADRESİ } /0  md5 - host all all 0.0.0.0/0 md5 postgresql: listen: { 1. DB MAKİNESİ IP ADRESİ } ,127.0.0.1:5432 connect_address: { 1. DB MAKİNESİ IP ADRESİ } :5432 use_unix_socket: true data_dir: /var/lib/postgresql/ {installed_version} /main/ bin_dir: /usr/lib/postgresql/ {installed_version} /bin/ pgpass: /var/lib/postgresql/.pgpass_patroni authentication: replication: username: replicator password: 1qaz2wsx superuser: username: postgres password: 1qaz2wsx parameters: unix_socket_directories: /var/run/postgresql remove_data_directory_on_rewind_failure: true remove_data_directory_on_diverged_timelines: true create_replica_methods: - basebackup basebackup: max-rate: '100M' checkpoint: 'fast' watchdog: mode: off  # Allowed values: off, automatic, required device: /dev/watchdog safety_margin: 5 tags: nofailover: false noloadbalance: false clonefrom: false nosync: false 2. Veri Tabanı Makinesi Üzerinde vi /etc/patroni/patroni.yml komutu ile konfigürasyon dosyasının içerisine aşağıdaki konfigler düzenlenerek yazılır: --- scope: postgres-cluster name: { DB – CLUSTER -NAME } namespace: /service/ restapi: listen: { 2. DB MAKİNESİ IP ADRESİ } :8008 connect_address: { 2. DB MAKİNESİ IP ADRESİ } :8008 etcd: hosts: { 1. MANAGEMENT MAKİNESİ IP ADRESİ } :2379, { 2. MANAGEMENT MAKİNESİ IP ADRESİ } :2379, { 3. MANAGEMENT MAKİNESİ IP ADRESİ } :2379 bootstrap: method: initdb dcs: ttl: 30 loop_wait: 10 retry_timeout: 10 maximum_lag_on_failover: 1048576 master_start_timeout: 300 synchronous_mode: true synchronous_mode_strict: true synchronous_node_count: 1 postgresql: use_pg_rewind: true use_slots: true parameters: max_connections: 500 superuser_reserved_connections: 5 password_encryption: scram-sha-256 max_locks_per_transaction: 64 max_prepared_transactions: 0 huge_pages: try shared_buffers: 512MB work_mem: 128MB maintenance_work_mem: 256MB effective_cache_size: 4GB checkpoint_timeout: 15min checkpoint_completion_target: 0.9 min_wal_size: 2GB max_wal_size: 8GB wal_buffers: 32MB default_statistics_target: 1000 seq_page_cost: 1 random_page_cost: 4 effective_io_concurrency: 2 synchronous_commit: on autovacuum: on autovacuum_max_workers: 5 autovacuum_vacuum_scale_factor: 0.01 autovacuum_analyze_scale_factor: 0.01 autovacuum_vacuum_cost_limit: 500 autovacuum_vacuum_cost_delay: 2 autovacuum_naptime: 1s max_files_per_process: 4096 archive_mode: on archive_timeout: 1800s archive_command: cd . wal_level: replica wal_keep_size: 2GB max_wal_senders: 10 max_replication_slots: 10 hot_standby: on wal_log_hints: on wal_compression: on shared_preload_libraries: pg_stat_statements,auto_explain pg_stat_statements.max: 10000 pg_stat_statements.track: all pg_stat_statements.track_utility: false pg_stat_statements.save: true auto_explain.log_min_duration: 10s auto_explain.log_analyze: true auto_explain.log_buffers: true auto_explain.log_timing: false auto_explain.log_triggers: true auto_explain.log_verbose: true auto_explain.log_nested_statements: true auto_explain.sample_rate: 0.01 track_io_timing: on log_lock_waits: on log_temp_files: 0 track_activities: on track_counts: on track_functions: all log_checkpoints: on logging_collector: on log_truncate_on_rotation: on log_rotation_age: 1d log_rotation_size: 0 log_line_prefix: '%t [%p-%l] %r %q%u@%d ' log_filename: 'postgresql-%a.log' log_directory: /var/log/postgresql hot_standby_feedback: on max_standby_streaming_delay: 30s wal_receiver_status_interval: 10s idle_in_transaction_session_timeout: 10min jit: off max_worker_processes: 24 max_parallel_workers: 8 max_parallel_workers_per_gather: 2 max_parallel_maintenance_workers: 2 initdb:  # List options to be passed on to initdb - encoding: UTF8 - locale: en_US.UTF-8 - data-checksums  pg_hba:  # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host replication replicator { 1. DB MAKİNESİ IP ADRESİ } /0  md5 - host replication replicator { 2. DB MAKİNESİ IP ADRESİ } /0  md5 - host all all 0.0.0.0/0 md5 postgresql: listen: { 2. DB MAKİNESİ IP ADRESİ } ,127.0.0.1:5432 connect_address: { 2. DB MAKİNESİ IP ADRESİ } :5432 use_unix_socket: true data_dir: /var/lib/postgresql/ {installed_version} /main/ bin_dir: /usr/lib/postgresql/ {installed_version} /bin/ pgpass: /var/lib/postgresql/.pgpass_patroni authentication: replication: username: replicator password: 1qaz2wsx superuser: username: postgres password: 1qaz2wsx parameters: unix_socket_directories: /var/run/postgresql remove_data_directory_on_rewind_failure: true remove_data_directory_on_diverged_timelines: true create_replica_methods: - basebackup basebackup: max-rate: '100M' checkpoint: 'fast' watchdog: mode: off  # Allowed values: off, automatic, required device: /dev/watchdog safety_margin: 5 tags: nofailover: false noloadbalance: false clonefrom: false nosync: false Patroni konfigürasyonu tamamlandıktan sonra, tüm veri tabanı sunucularında üzerinde PgBouncer konfigürasyonu yapılır: 1. Veri Tabanı Makinesi Üzerinde sudo vi /etc/pgbouncer/pgbouncer.ini komutu ile pgbouncer konfigürasyon dosyası aşağıdaki gibi düzenlenir: [databases] postgres = host=127.0.0.1 port=5432 dbname=postgres * = host=127.0.0.1 port=5432 [pgbouncer] logfile = /var/log/pgbouncer/pgbouncer.log pidfile = /var/run/pgbouncer/pgbouncer.pid listen_addr = { 1. DB MAKİNESİ IP ADRESİ } listen_port = 6432 unix_socket_dir = /var/run/postgresql auth_type = scram-sha-256 auth_file = /etc/pgbouncer/userlist.txt admin_users = postgres ignore_startup_parameters = extra_float_digits,geqo pool_mode = session server_reset_query = DISCARD ALL max_client_conn = 10000 default_pool_size = 20 reserve_pool_size = 1 reserve_pool_timeout = 1 max_db_connections = 1000 pkt_buf = 8192 listen_backlog = 4096 log_connections = 0 log_disconnections = 0 Veri tabanı sunucusuna bağlanacak kullanıcıların listesinin tutulduğu dosya, vi /etc/pgbouncer/userlist.txt komutu ile oluşturulur ve içerisine aşağıdaki sorgu sonucunda gelen sonuçlar eklenir: select usename, passwd from pg_shadow order by 1; Örnek Girdi: ""user_name"" ""SCRAM-SHA-256$4096:VkACutTA+OAe/AuNfSlStg==$hd/hOrZ4cItfcBwLLom5SeHUf+Q/LVFTGV9cYTDSssI=:u60vroBUuPDD28ykHeCApJ4="" 2. Veri Tabanı Makinesi Üzerinde sudo vi /etc/pgbouncer/pgbouncer.ini komutu ile pgbouncer konfigürasyon dosyası aşağıdaki gibi düzenlenir: [databases] postgres = host=127.0.0.1 port=5432 dbname=postgres * = host=127.0.0.1 port=5432 [pgbouncer] logfile = /var/log/pgbouncer/pgbouncer.log pidfile = /var/run/pgbouncer/pgbouncer.pid listen_addr = { 2. DB MAKİNESİ IP ADRESİ } listen_port = 6432 unix_socket_dir = /var/run/postgresql auth_type = scram-sha-256 auth_file = /etc/pgbouncer/userlist.txt admin_users = postgres ignore_startup_parameters = extra_float_digits,geqo pool_mode = session server_reset_query = DISCARD ALL max_client_conn = 10000 default_pool_size = 20 reserve_pool_size = 1 reserve_pool_timeout = 1 max_db_connections = 1000 pkt_buf = 8192 listen_backlog = 4096 log_connections = 0 log_disconnections = 0 Veri tabanı sunucusuna bağlanacak kullanıcıların listesinin tutulduğu dosya, vi /etc/pgbouncer/userlist.txt komutu ile oluşturulur ve içerisine aşağıdaki sorgu sonucunda gelen sonuçlar eklenir: select usename, passwd from pg_shadow order by 1; Örnek Girdi: ""user_name"" ""SCRAM-SHA-256$4096:VkACutTA+OAe/AuNfSlStg==$hd/hOrZ4cItfcBwLLom5SeHUf+Q/LVFTGV9cYTDSssI=:u60vroBUuPDD28ykHeCApJ4="" PgBouncer konfigürasyonu tamamlandıktan sonra, tüm veri tabanı sunucuları üzerinde PgBackRest konfigürasyonu yapılır: vi /etc/pgbackrest/pgbackrest.conf komutu ile pgbouncer konfigürasyon dosyası tüm veri tabanı sunucularında aşağıdaki gibi düzenlenir: [ STANZA-NAME ] pg1-path=/var/lib/postgresql/ {installed_version} /main [global] repo1-host= { BACKUP MAKİNESİ IP ADRESİ } repo1-host-user=postgres Tüm konfigürasyonlar tamamlandıktan sonra, tüm veri tabanı sunucularında Patroni ve PgBouncer servisleri enable & start edilir: [ root @ sunucu ~]# sudo systemctl enable patroni.service [ root @ sunucu ~]# sudo systemctl start  patroni.service [ root @ sunucu ~]# sudo systemctl enable pgbouncer.service [ root @ sunucu ~]# sudo systemctl start  pgbouncer.service patronictl -c /etc/patroni/patroni.yml list komutu ile lider ve replica sunucularının listesi gözlemlenir. [ root @ sunucu ~]# sudo patronictl -c /etc/patroni/patroni.yml list Tüm veri tabanı sunucularında kurulumlar ve konfigürasyonlar tamamlandıktan sonra, management makineleri üzerinde de HAProxy ve Keepalived servisleri başlatılır: [ root@sunucu ~]#  sudo  systemctl enable haproxy.service [ root@sunucu ~]#  sudo  systemctl start haproxy.service [ root@sunucu ~]#  sudo  systemctl enable keepalived.service [ root@sunucu ~]#  sudo  systemctl start keepalived.service Management makineleri üzerinde VIP adresinin ataması gerçekleştirilmiş mi kontrolü aşağıdaki komut ile yapılabilir: [ root@sunucu ~]#   ip --brief add BACKUP SERVER KURULUMU VE YAPILANDIRILMASI Aşağıdaki komutlar ile backup sunucusu üzerinde PgBackRest ve PostgreSQL kurulumları yapılır. [ root@sunucu ~]# sudo sh -c 'echo ""deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main"" > /etc/apt/sources.list.d/pgdg.list' [ root@sunucu ~]# sudo  wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - [ root@sunucu ~]# sudo apt-get update [ root@sunucu ~]# sudo apt-get -y install postgresql [ root@sunucu ~]# sudo apt-get -y install pgbackrest Backup işlemlerini başlatabilmek için öncelikle primary veri tabanı sunucusu üzerinde sudo su – postgres komutu ile postgres kullanıcısına geçiş yapılarak patronictl edit-config komutu ile PostgreSQL parametrelerinden archive_mode: on olarak, archive_command: pgbackrest --stanza=stanza_name archive-push %p olarak ayarlanmalıdır. İlgili ayarlamalar yapıldıktan sonra ayarların etkin olabilmesi için cluster restart edilmelidir çünkü archive_mode parametreleri PostgreSQL’in restart edilmesini gerektirmektedir. Restart işlemi için, patronictl -c /etc/patroni/patroni.yml restart <CLUSTER_NAME> komutu ile veri tabanı servisleri restart edilir. Bu işlemlerin ardından tüm veri tabanı sunucularındaki postgres kullanıcıları ile backup sunucusundaki postgres kullanıcısı arasında parolasız ssh ayarlanmalıdır. Tüm Veri Tabanı Sunucuları Üzerinde [ root@sunucu ~]#       sudo su – postgres [ postgres@sunucu ~]#   ssh-keygen -t rsa -b 4096 [ postgres@sunucu ~]#   touch authorized_keys [ postgres@sunucu ~]#   chown 600 authorized_keys cat .ssh/id_rsa.pub komutunun çıktısı backup sunucusundaki postgres kullanıcısına ait authorized_keys dosyasının içerisine yapıştırılır. BACKUP Sunucusu Üzerinde [ root@sunucu ~]#      sudo su – postgres [ postgres@sunucu ~]#  ssh-keygen -t rsa -b 4096 [ postgres@sunucu ~]#  touch authorized_keys [ postgres@sunucu ~]#  chown 600 authorized_keys cat .ssh/id_rsa.pub komutunun çıktısı veri tabanı sunucularındaki postgres kullanıcısına ait authorized_keys dosyasının içerisine yapıştırılır. Parolasız ssh işleminin sunucular arasında sorunsuz çalıştığı teyit edilmelidir. Bunun için hem backup sucusundan tüm veri tabanı sunucularına hem de tüm veri tabanı sunucularından backup sunucusuna postgres kullanıcısı için ayrı ayrı parolasız ssh denemeleri yapılmalı ve her hangi bir sorun olmadığı gözlemlenmelidir. Örnek ssh denemesi: ssh postgres@db-1 -> Bu işlem, backup sunucusu üzerinde yapıldığı zaman parola sorulmadan db-1 isimli veri tabanı sunucusuna ssh yapılabildiği görülmelidir. Postgres kullanıcıları arasında parolasız ssh tanımlamaları yapıldıktan sonra backup sunucusunda konfigürasyon tanımlaları yapılmalıdır. Bunun için, sudo vi /etc/pgbackrest/pgbackrest.conf komutu ile konfigürasyon dosyası aşağıdaki şekilde düzenlenir: [stanza_name] pg1-host= { 1. DB MAKİNESİ IP ADRESİ } pg1-user=postgres pg1-path=/var/lib/postgresql/15/main pg2-host= { 2. DB MAKİNESİ IP ADRESİ } pg2-path=/var/lib/postgresql/15/main pg2-user=postgres [global] backup-standby=y process-max=3 compress=y repo1-path=/var/lib/pgbackrest repo1-retention-full=2 start-fast=y Tüm konfigürasyonlar tamamlandıktan sonra, belirlediğimiz stanza için katalog oluşturulması gerekmektedir. Postgres kullanıcısına geçiş yapılarak katalog oluşturulur. Bunun için aşağıdaki komutlar çalıştırılır: [ root@sunucu ~]#       sudo su – postgres [ postgres@sunucu ~]#   pgbackrest --stanza=stanza_name stanza-create pgbackrest --stanza=stanza_name stanza-create -> yukarıda da yer alan bu komut ile pgbackrest, yedek dizini içerisine ilgili stanza’ya ait katalog yapısını oluşturur. pgbackrest --stanza=stanza_name check komutu ile oluşturulan stanzaya ilişkin doğruluk kontrolü yapılır. Stanza doğru oluşturulmamış ise, ekrana bu durumla ilgili hata basılır. Eğer her şey doğru yapılmış ise stanza’nın doğru yapılandırıldığına dair status (ok) olarak ekrana çıktı basılır. pgbackrest --stanza=stanza_name --type=full backup komutu ile backup alımı artık yapılabilir. pgbackrest --stanza=stanza_name info komutu ile alınan backuplara ilişkin bilgi çıktısı üretilir. ANSIBLE İLE CLUSTER KURULUMU Bu aşamaya kadar manuel kurulum ve konfigürasyonları ele alındı. Bu aşamada ise, tüm bu kurulum sürecinin daha önceden yazılmış olan ansible playbookları ile gerekli ayarlamaları yapılarak otomatik olarak nasıl gerçekleştirilebileceği gösterilmektedir. Ansible ile kurulum yapılabilmesi için Ansible sunucusunun kurulumun yapılacağı tüm sunuculara parolasız ssh yapabildiğinden emin olunulmalıdır. Ardından kurulum süreci başlatılabilir. GitHub üzerinde, Ansible ile PostgreSQL HA kurulumu için open source olarak paylaşılmış bir proje bulunmaktadır. Ansible sunucusuna bu proje aşağıdaki komutla klonlanarak çekilir. git clone https://github.com/vitabaks/postgresql_cluster Klonlanan projedeki dosyalardan inventory dosyası açılarak ilgli management ve veri tabanı sunucularının IP adresleri bu dosyaya yazılır. Ve bu dosyanın son bölümünde ansible’ın, kurulumun yapılacağı sunuculara yapacağı ssh bağlantısında kullanacağı root kullanıcısı ve parolası girilmelidir. Eğer sıfır bir cluster kurulumu yapılıyorsa postgresql_exists ifadesi false olarak ayarlanmalıdır. Eğer mevcut bir cluster ortamı için düzenleme yapılıyorsa, bu ifade true olarak set edilmelidir. Bu işlemleri yapabilmek için group_vars dizine içerisinde master ve replica dosyalarında postgresql_exists: false/true olarak ayarlanabilir. Ardından vars dizini içerisine geçilir. Burada kuracağımız clustera ait kritik ayarlar bulunmaktadır. Bu dizin içerisinde main.yaml dosyası bulunur. Bu dosya bir editör aracılığı ile açılarak gerekli düzenlemeler yapılmalıdır. Açılan bu dosyanın en üstünde cluster vip ifadesi bulunmaktadır. Buraya hiçbir makinede kullanılmayan boş bir sanal IP adresi girilmesi gerekmektedir. Bu IP adresi, clustera ait floting ip olarak kullanılacak. Olası fail durumlarında HAProxy keepalived'yi tetikleyerek bu sanal IP’yi bir sonraki yedek makineye çekebilecek. Tüm veri tabanı bağlantı işlemleri bu IP üzerinden gerçekleşip arka tarafta failover yapısı olacaktır. main.yaml dosyası içerisinde tüm ihtiyaca yönelik olarak gerekli düzenlemeler yapılmalıdır. Tüm ayarlamalar yapıldıktan sonra değişiklikler kayıt edilir ve çıkılır. Cluster kurulumunu başlatmak için, ansible-playbook deploy_pgcluster.yaml komutu ile kurulum süreci başlatılır ve kurulum sonucu gözlemlenir. PRODUCTION KONFİGÜRASYONU 1.  MANAGEMET SUNUCUSU ÜZERİNDE HAPROXY KONFİGÜRASYONU global maxconn 100000 log /dev/log    local0 log /dev/log    local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon defaults mode               tcp log                global retries            2 timeout queue      5s timeout connect    5s timeout client     60m timeout server     60m timeout check      15s listen stats mode http bind 10.224.6.66:7000 stats enable stats uri / listen master bind 10.224.6.60:5432 maxconn 10000 option tcplog option httpchk OPTIONS /master http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas bind 10.224.6.60:5433 maxconn 10000 option tcplog option httpchk OPTIONS /replica balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_sync bind 10.224.6.60:5434 maxconn 10000 option tcplog option httpchk OPTIONS /sync balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_async bind 10.224.6.60:5435 maxconn 10000 option tcplog option httpchk OPTIONS /async balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 1.  MANAGEMET SUNUCUSU ÜZERİNDE KEEPALIVED KONFİGÜRASYONU global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script ""/usr/libexec/keepalived/haproxy_check.sh"" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id 60 priority  100 advert_int 2 state  BACKUP virtual_ipaddress { 10.224.6.60 } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } 1.  MANAGEMET SUNUCUSU ÜZERİNDE ETCD KONFİGÜRASYONU ETCD_NAME=""smartfarm-postgremgmt01-prod"" ETCD_LISTEN_CLIENT_URLS="" http://10.224.6.66:2379,http://127.0.0.1:2379 "" ETCD_ADVERTISE_CLIENT_URLS="" http://10.224.6.66:2379 "" ETCD_LISTEN_PEER_URLS="" http://10.224.6.66:2380 "" ETCD_INITIAL_ADVERTISE_PEER_URLS="" http://10.224.6.66:2380 "" ETCD_INITIAL_CLUSTER_TOKEN=""etcd-smartfarm-postgres-cluster"" ETCD_INITIAL_CLUSTER=""smartfarm-postgremgmt01-prod= http://10.224.6.66:2380,smartfarm-postgremgmt02-prod=http://10.224.6.67:2380,smartfarm-postgremgmt03-prod=http://10.224.6.68:2380 "" ETCD_INITIAL_CLUSTER_STATE=""new"" ETCD_DATA_DIR=""/var/lib/etcd"" ETCD_ELECTION_TIMEOUT=""5000"" ETCD_HEARTBEAT_INTERVAL=""1000"" ETCD_INITIAL_ELECTION_TICK_ADVANCE=""false"" ETCD_ENABLE_V2=""true"" 2.  MANAGEMET SUNUCUSU ÜZERİNDE HAPROXY KONFİGÜRASYONU global maxconn 100000 log /dev/log    local0 log /dev/log    local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon defaults mode               tcp log                global retries            2 timeout queue      5s timeout connect    5s timeout client     60m timeout server     60m timeout check      15s listen stats mode http bind 10.224.6.67:7000 stats enable stats uri / listen master bind 10.224.6.60:5432 maxconn 10000 option tcplog option httpchk OPTIONS /master http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas bind 10.224.6.60:5433 maxconn 10000 option tcplog option httpchk OPTIONS /replica balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_sync bind 10.224.6.60:5434 maxconn 10000 option tcplog option httpchk OPTIONS /sync balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_async bind 10.224.6.60:5435 maxconn 10000 option tcplog option httpchk OPTIONS /async balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 2.  MANAGEMET SUNUCUSU ÜZERİNDE KEEPALIVED KONFİGÜRASYONU global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script ""/usr/libexec/keepalived/haproxy_check.sh"" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id 60 priority  100 advert_int 2 state  BACKUP virtual_ipaddress { 10.224.6.60 } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } 2.  MANAGEMET SUNUCUSU ÜZERİNDE ETCD KONFİGÜRASYONU ETCD_NAME=""smartfarm-postgremgmt02-prod"" ETCD_LISTEN_CLIENT_URLS="" http://10.224.6.67:2379,http://127.0.0.1:2379 "" ETCD_ADVERTISE_CLIENT_URLS="" http://10.224.6.67:2379 "" ETCD_LISTEN_PEER_URLS="" http://10.224.6.67:2380 "" ETCD_INITIAL_ADVERTISE_PEER_URLS="" http://10.224.6.67:2380 "" ETCD_INITIAL_CLUSTER_TOKEN=""etcd-smartfarm-postgres-cluster"" ETCD_INITIAL_CLUSTER=""smartfarm-postgremgmt01-prod= http://10.224.6.66:2380,smartfarm-postgremgmt02-prod=http://10.224.6.67:2380,smartfarm-postgremgmt03-prod=http://10.224.6.68:2380 "" ETCD_INITIAL_CLUSTER_STATE=""new"" ETCD_DATA_DIR=""/var/lib/etcd"" ETCD_ELECTION_TIMEOUT=""5000"" ETCD_HEARTBEAT_INTERVAL=""1000"" ETCD_INITIAL_ELECTION_TICK_ADVANCE=""false"" ETCD_ENABLE_V2=""true"" 3.  MANAGEMET SUNUCUSU ÜZERİNDE HAPROXY KONFİGÜRASYONU global maxconn 100000 log /dev/log    local0 log /dev/log    local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon defaults mode               tcp log                global retries            2 timeout queue      5s timeout connect    5s timeout client     60m timeout server     60m timeout check      15s listen stats mode http bind 10.224.6.68:7000 stats enable stats uri / listen master bind 10.224.6.60:5432 maxconn 10000 option tcplog option httpchk OPTIONS /master http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas bind 10.224.6.60:5433 maxconn 10000 option tcplog option httpchk OPTIONS /replica balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_sync bind 10.224.6.60:5434 maxconn 10000 option tcplog option httpchk OPTIONS /sync balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 listen replicas_async bind 10.224.6.60:5435 maxconn 10000 option tcplog option httpchk OPTIONS /async balance roundrobin http-check expect status 200 default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions server smartfarm-postgresql01-prod 10.224.6.61:6432 check port 8008 server smartfarm-postgresql02-prod 10.224.6.62:6432 check port 8008 3.  MANAGEMET SUNUCUSU ÜZERİNDE KEEPALIVED KONFİGÜRASYONU global_defs { router_id ocp_vrrp enable_script_security script_user root } vrrp_script haproxy_check { script ""/usr/libexec/keepalived/haproxy_check.sh"" interval 2 weight 2 } vrrp_instance VI_1 { interface ens160 virtual_router_id 60 priority  100 advert_int 2 state  BACKUP virtual_ipaddress { 10.224.6.60 } track_script { haproxy_check } authentication { auth_type PASS auth_pass 1ce24b6e } } 3.  MANAGEMET SUNUCUSU ÜZERİNDE ETCD KONFİGÜRASYONU ETCD_NAME=""smartfarm-postgremgmt03-prod"" ETCD_LISTEN_CLIENT_URLS="" http://10.224.6.68:2379,http://127.0.0.1:2379 "" ETCD_ADVERTISE_CLIENT_URLS="" http://10.224.6.68:2379 "" ETCD_LISTEN_PEER_URLS="" http://10.224.6.68:2380 "" ETCD_INITIAL_ADVERTISE_PEER_URLS="" http://10.224.6.68:2380 "" ETCD_INITIAL_CLUSTER_TOKEN=""etcd-smartfarm-postgres-cluster"" ETCD_INITIAL_CLUSTER=""smartfarm-postgremgmt01-prod= http://10.224.6.66:2380,smartfarm-postgremgmt02-prod=http://10.224.6.67:2380,smartfarm-postgremgmt03-prod=http://10.224.6.68:2380 "" ETCD_INITIAL_CLUSTER_STATE=""new"" ETCD_DATA_DIR=""/var/lib/etcd"" ETCD_ELECTION_TIMEOUT=""5000"" ETCD_HEARTBEAT_INTERVAL=""1000"" ETCD_INITIAL_ELECTION_TICK_ADVANCE=""false"" ETCD_ENABLE_V2=""true"" 1.  VERİ TABANI SUNUCUSU ÜZERİNDE PGBOUNCER KONFİGÜRASYONU [databases] postgres = host=127.0.0.1 port=5432 dbname=postgres * = host=127.0.0.1 port=5432 [pgbouncer] logfile = /var/log/pgbouncer/pgbouncer.log pidfile = /var/run/pgbouncer/pgbouncer.pid listen_addr = 10.224.6.61 listen_port = 6432 unix_socket_dir = /var/run/postgresql auth_type = scram-sha-256 auth_file = /etc/pgbouncer/userlist.txt admin_users = postgres ignore_startup_parameters = extra_float_digits,geqo,search_path pool_mode = session server_reset_query = DISCARD ALL max_client_conn = 10000 default_pool_size = 20 reserve_pool_size = 1 reserve_pool_timeout = 1 max_db_connections = 1000 pkt_buf = 8192 listen_backlog = 4096 log_connections = 0 log_disconnections = 0 # Documentation https://pgbouncer.github.io/config.html 1.  VERİ TABANI SUNUCUSU ÜZERİNDE PATRONI KONFİGÜRASYONU --- scope: smartfarm-postgres-cluster name: smartfarm-postgresql01-prod namespace: /service/ log: level: INFO traceback_level: ERROR format: '%(asctime)s %(levelname)s: %(message)s' dateformat: '' max_queue_size: 1000 dir: /var/log/patroni file_num: 4 file_size: 25000000 loggers: patroni.postmaster: WARNING urllib3: WARNING restapi: listen: 10.224.6.61:8008 connect_address: 10.224.6.61:8008 #  certfile: /etc/ssl/certs/ssl-cert-snakeoil.pem #  keyfile: /etc/ssl/private/ssl-cert-snakeoil.key #  authentication: #    username: username #    password: password etcd: hosts: 10.224.6.66:2379,10.224.6.67:2379,10.224.6.68:2379 bootstrap: method: initdb dcs: ttl: 30 loop_wait: 10 retry_timeout: 10 maximum_lag_on_failover: 1048576 master_start_timeout: 300 synchronous_mode: true synchronous_mode_strict: false synchronous_node_count: 1 postgresql: use_pg_rewind: true use_slots: true parameters: max_connections: 1000 superuser_reserved_connections: 10 password_encryption: scram-sha-256 max_locks_per_transaction: 64 max_prepared_transactions: 0 huge_pages: try shared_buffers: 512MB work_mem: 128MB maintenance_work_mem: 256MB effective_cache_size: 4GB checkpoint_timeout: 15min checkpoint_completion_target: 0.9 min_wal_size: 2GB max_wal_size: 8GB wal_buffers: 32MB default_statistics_target: 1000 seq_page_cost: 1 random_page_cost: 4 effective_io_concurrency: 2 synchronous_commit: on autovacuum: on autovacuum_max_workers: 5 autovacuum_vacuum_scale_factor: 0.01 autovacuum_analyze_scale_factor: 0.01 autovacuum_vacuum_cost_limit: 500 autovacuum_vacuum_cost_delay: 2 autovacuum_naptime: 1s max_files_per_process: 4096 archive_mode: on archive_timeout: 1800s archive_command: cd . wal_level: replica wal_keep_size: 2GB max_wal_senders: 10 max_replication_slots: 10 hot_standby: on wal_log_hints: on wal_compression: on shared_preload_libraries: pg_stat_statements,auto_explain pg_stat_statements.max: 10000 pg_stat_statements.track: all pg_stat_statements.track_utility: false pg_stat_statements.save: true auto_explain.log_min_duration: 10s auto_explain.log_analyze: true auto_explain.log_buffers: true auto_explain.log_timing: false auto_explain.log_triggers: true auto_explain.log_verbose: true auto_explain.log_nested_statements: true auto_explain.sample_rate: 0.01 track_io_timing: on log_lock_waits: on log_temp_files: 0 track_activities: on track_counts: on track_functions: all log_checkpoints: on logging_collector: on log_truncate_on_rotation: on log_rotation_age: 1d log_rotation_size: 0 log_line_prefix: '%t [%p-%l] %r %q%u@%d ' log_filename: 'postgresql-%a.log' log_directory: /var/log/postgresql hot_standby_feedback: on max_standby_streaming_delay: 30s wal_receiver_status_interval: 10s idle_in_transaction_session_timeout: 10min jit: off max_worker_processes: 24 max_parallel_workers: 8 max_parallel_workers_per_gather: 2 max_parallel_maintenance_workers: 2 initdb:  # List options to be passed on to initdb - encoding: UTF8 - locale: en_US.UTF-8 - data-checksums pg_hba:  # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host all all 0.0.0.0/0 md5 postgresql: listen: 10.224.6.61,127.0.0.1:5432 connect_address: 10.224.6.61:5432 use_unix_socket: true data_dir: /var/lib/postgresql/15/main bin_dir: /usr/lib/postgresql/15/bin config_dir: /etc/postgresql/15/main pgpass: /var/lib/postgresql/.pgpass_patroni authentication: replication: username: replicator password: ******* superuser: username: postgres password: ****** #    rewind:  # Has no effect on postgres 10 and lower #      username: rewind_user #      password: rewind_password parameters: unix_socket_directories: /var/run/postgresql stats_temp_directory: /var/lib/pgsql_stats_tmp remove_data_directory_on_rewind_failure: false remove_data_directory_on_diverged_timelines: false #  callbacks: #    on_start: #    on_stop: #    on_restart: #    on_reload: #    on_role_change: create_replica_methods: - basebackup basebackup: max-rate: '100M' checkpoint: 'fast' watchdog: mode: off  # Allowed values: off, automatic, required device: /dev/watchdog safety_margin: 5 tags: nofailover: false noloadbalance: false clonefrom: false nosync: false # specify a node to replicate from (cascading replication) #  replicatefrom: (node name) 1.  VERİ TABANI SUNUCUSU ÜZERİNDE PGBACKREST KONFİGÜRASYONU [smartfarm-backups] pg1-path=/var/lib/postgresql/15/main [global] repo1-host=10.224.6.69 repo1-host-user=postgres 2.  VERİ TABANI SUNUCUSU ÜZERİNDE PGBOUNCER KONFİGÜRASYONU [databases] postgres = host=127.0.0.1 port=5432 dbname=postgres * = host=127.0.0.1 port=5432 [pgbouncer] logfile = /var/log/pgbouncer/pgbouncer.log pidfile = /var/run/pgbouncer/pgbouncer.pid listen_addr = 10.224.6.62 listen_port = 6432 unix_socket_dir = /var/run/postgresql auth_type = scram-sha-256 auth_file = /etc/pgbouncer/userlist.txt admin_users = postgres ignore_startup_parameters = extra_float_digits,geqo,search_path pool_mode = session server_reset_query = DISCARD ALL max_client_conn = 10000 default_pool_size = 20 reserve_pool_size = 1 reserve_pool_timeout = 1 max_db_connections = 1000 pkt_buf = 8192 listen_backlog = 4096 log_connections = 0 log_disconnections = 0 # Documentation https://pgbouncer.github.io/config.html 2.  VERİ TABANI SUNUCUSU ÜZERİNDE PATRONI KONFİGÜRASYONU --- scope: smartfarm-postgres-cluster name: smartfarm-postgresql02-prod namespace: /service/ log: level: INFO traceback_level: ERROR format: '%(asctime)s %(levelname)s: %(message)s' dateformat: '' max_queue_size: 1000 dir: /var/log/patroni file_num: 4 file_size: 25000000 loggers: patroni.postmaster: WARNING urllib3: WARNING restapi: listen: 10.224.6.62:8008 connect_address: 10.224.6.62:8008 #  certfile: /etc/ssl/certs/ssl-cert-snakeoil.pem #  keyfile: /etc/ssl/private/ssl-cert-snakeoil.key #  authentication: #    username: username #    password: password etcd: hosts: 10.224.6.66:2379,10.224.6.67:2379,10.224.6.68:2379 bootstrap: method: initdb dcs: ttl: 30 loop_wait: 10 retry_timeout: 10 maximum_lag_on_failover: 1048576 master_start_timeout: 300 synchronous_mode: true synchronous_mode_strict: false synchronous_node_count: 1 postgresql: use_pg_rewind: true use_slots: true parameters: max_connections: 1000 superuser_reserved_connections: 10 password_encryption: scram-sha-256 max_locks_per_transaction: 64 max_prepared_transactions: 0 huge_pages: try shared_buffers: 512MB work_mem: 128MB maintenance_work_mem: 256MB effective_cache_size: 4GB checkpoint_timeout: 15min checkpoint_completion_target: 0.9 min_wal_size: 2GB max_wal_size: 8GB wal_buffers: 32MB default_statistics_target: 1000 seq_page_cost: 1 random_page_cost: 4 effective_io_concurrency: 2 synchronous_commit: on autovacuum: on autovacuum_max_workers: 5 autovacuum_vacuum_scale_factor: 0.01 autovacuum_analyze_scale_factor: 0.01 autovacuum_vacuum_cost_limit: 500 autovacuum_vacuum_cost_delay: 2 autovacuum_naptime: 1s max_files_per_process: 4096 archive_mode: on archive_timeout: 1800s archive_command: cd . wal_level: replica wal_keep_size: 2GB max_wal_senders: 10 max_replication_slots: 10 hot_standby: on wal_log_hints: on wal_compression: on shared_preload_libraries: pg_stat_statements,auto_explain pg_stat_statements.max: 10000 pg_stat_statements.track: all pg_stat_statements.track_utility: false pg_stat_statements.save: true auto_explain.log_min_duration: 10s auto_explain.log_analyze: true auto_explain.log_buffers: true auto_explain.log_timing: false auto_explain.log_triggers: true auto_explain.log_verbose: true auto_explain.log_nested_statements: true auto_explain.sample_rate: 0.01 track_io_timing: on log_lock_waits: on log_temp_files: 0 track_activities: on track_counts: on track_functions: all log_checkpoints: on logging_collector: on log_truncate_on_rotation: on log_rotation_age: 1d log_rotation_size: 0 log_line_prefix: '%t [%p-%l] %r %q%u@%d ' log_filename: 'postgresql-%a.log' log_directory: /var/log/postgresql hot_standby_feedback: on max_standby_streaming_delay: 30s wal_receiver_status_interval: 10s idle_in_transaction_session_timeout: 10min jit: off max_worker_processes: 24 max_parallel_workers: 8 max_parallel_workers_per_gather: 2 max_parallel_maintenance_workers: 2 initdb:  # List options to be passed on to initdb - encoding: UTF8 - locale: en_US.UTF-8 - data-checksums pg_hba:  # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host all all 0.0.0.0/0 md5 postgresql: listen: 10.224.6.62,127.0.0.1:5432 connect_address: 10.224.6.62:5432 use_unix_socket: true data_dir: /var/lib/postgresql/15/main bin_dir: /usr/lib/postgresql/15/bin config_dir: /etc/postgresql/15/main pgpass: /var/lib/postgresql/.pgpass_patroni authentication: replication: username: replicator password: ******* superuser: username: postgres password: ******* #    rewind:  # Has no effect on postgres 10 and lower #      username: rewind_user #      password: rewind_password parameters: unix_socket_directories: /var/run/postgresql stats_temp_directory: /var/lib/pgsql_stats_tmp remove_data_directory_on_rewind_failure: false remove_data_directory_on_diverged_timelines: false #  callbacks: #    on_start: #    on_stop: #    on_restart: #    on_reload: #    on_role_change: create_replica_methods: - basebackup basebackup: max-rate: '100M' checkpoint: 'fast' watchdog: mode: off  # Allowed values: off, automatic, required device: /dev/watchdog safety_margin: 5 tags: nofailover: false noloadbalance: false clonefrom: false nosync: false # specify a node to replicate from (cascading replication) #  replicatefrom: (node name) 2.  VERİ TABANI SUNUCUSU ÜZERİNDE PGBACKREST KONFİGÜRASYONU [smartfarm-backups] pg1-path=/var/lib/postgresql/15/main [global] repo1-host=10.224.6.69 repo1-host-user=postgres BACKUP SUNUCUSU ÜZERİNDE PGBACKREST KONFİGÜRASYONU [smartfarm-backups] pg1-host=10.224.6.61 pg1-user=postgres pg1-path=/var/lib/postgresql/15/main pg2-host=10.224.6.62 pg2-path=/var/lib/postgresql/15/main pg2-user=postgres [global] backup-standby=y process-max=3 compress=y repo1-path=/var/lib/pgbackrest repo1-retention-full=2 start-fast=y REFERANSLAR https://patroni.readthedocs.io/en/latest/ https://github.com/zalando/patroni https://www.postgresql.org/ https://pgbackrest.org/ https://www.pgbouncer.org/ http://www.haproxy.org/ http://www.etcd.io/ https://keepalived.readthedocs.io/en/latest/introduction.html https://github.com/vitabaks/postgresql_clusterPOSTGRESQL CLUSTER KURULUM BELGESİ.docximage-2023-1-29_16-50-20.pngimage-2023-1-29_16-40-18.pngworddav7fbfb6c785504945dce49ed1515e7e9b.pngworddav3a1871a03214859f6b02859a2c313fee.pngworddavd3328bd5c76e539e171a2b760ebc3430.pngworddav8d503ccd4dcc17de89e7597b956a87f2.pngworddav3c14cf262307c537051a0e33e4413394.pngworddavd98b1eebbd480cd4639bf8e4b764b2a9.png","{'title': 'HA Mimaride PostgreSQL Kurulumu', 'id': '80479281', 'source': 'https://wiki.softtech.com.tr/display/SDO/HA+Mimaride+PostgreSQL+Kurulumu'}"
"DevopSofttech TSL, banka tarafından yönetiliyor. Eğer uygulama ssl hatası verirse, sertifika update için Checkmarx SSL için , NexusIQ, DevopsSofttech - ScoreTFS'e erişebiliyor olması lazım,  banka izin veriyor.  EMRAH DURAN, Jenkins, nexusiq [Jenkins]","{'title': 'Entegrasyonlar izinleri', 'id': '80480478', 'source': 'https://wiki.softtech.com.tr/display/SDO/Entegrasyonlar+izinleri'}"
,"{'title': 'Platform & Tools', 'id': '80480913', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80480913'}"
"grey solid Sık Sorulan Sorular Loglara nasıl Bakabilirim? Lorlara erişim için çalışma yaptığınız ortamda ilgili link'e gitmeniz gerekiyor. Aşağıdaki link'e tıklayarak araç listerine erişebilirsiniz? Araç Listesi Tools DevOps Araç Açıklama Link Login Yöntemi VPN Statüs Jenkins Derleme, DEV & INT kurulumları https://jenkins.softtech/ Softtech LDAP PulseSecure Gitlab https://gitlab.softtech Deprecated. Anthos Cluster1 Araç Açıklama Link GCloud Azure","{'title': 'Platform Guidelines', 'id': '80480916', 'source': 'https://wiki.softtech.com.tr/display/SDO/Platform+Guidelines'}"
Alert Oluşması Durumunda Yapılması Gerekenler small Zabbix Aracı Alertler kontrol edilir. Sistem kaynak kullanım dashboard'u güncellenir. Kibana loglarında ilgili hata aratılır. devop.ssofttech ilili ekran üzerinden paket bilgilerine erişilir. DevOps paltform ekibine ulaşılarak konu aktarılır.,"{'title': 'Uygulama Destek Guidelines', 'id': '80480933', 'source': 'https://wiki.softtech.com.tr/display/SDO/Uygulama+Destek+Guidelines'}"
,"{'title': 'Platform & Tools Team', 'id': '80480939', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80480939'}"
,"{'title': 'Kontrol Noktaları', 'id': '80480942', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80480942'}"
,"{'title': 'Zabbix Dashboard', 'id': '80480944', 'source': 'https://wiki.softtech.com.tr/display/SDO/Zabbix+Dashboard'}"
,"{'title': 'ELK', 'id': '80480946', 'source': 'https://wiki.softtech.com.tr/display/SDO/ELK'}"
,"{'title': 'Guidelines', 'id': '80480950', 'source': 'https://wiki.softtech.com.tr/display/SDO/Guidelines'}"
,"{'title': 'Monitoring', 'id': '80480952', 'source': 'https://wiki.softtech.com.tr/display/SDO/Monitoring'}"
,"{'title': 'Middleware', 'id': '80480954', 'source': 'https://wiki.softtech.com.tr/display/SDO/Middleware'}"
,"{'title': 'Platform Components', 'id': '80480965', 'source': 'https://wiki.softtech.com.tr/display/SDO/Platform+Components'}"
"Quarter Başlık Tasks Detaylar Kazanımlar 290 complete Devops-Softtech  nexus-iq background job' larında performans iyileştirmesi için code refactor edildi. 291 complete False-Positive talebi ile çözülemeyen yüksek sayıda bulgu içeren UK0668 nolu C++ projesinde 4 bin adet olan ""Improper Null termination"" bulgu için  Checkmarx Audit aracından faydalanarak  kalıcı olarak kapatılması için mimarla çalışma yapıldı. 292 complete Mobile Hub 3rd Party paketin checkmarx taraması yapılması sağlandı. 283 complete ISPP paketi ve Mobil-hub, third-party paketlerinin checkmarx taraması için destek verildi. 284 complete Checkmarx ve Nexus IQ taramaları konusunda gelen soru ve taleplere destek olundu. 285 complete Endpoint ekibi ile Checkmarx Audit ile query düzenleme konusunda çalışma yapıldı. 286 complete Promptbox dockerize etme ve kuruluma destek verildi. 278 complete Nexus.rally.softtech repository manager üzerinden 07/11/2023 tarihinde silinen reponun geri getirilmesi için 07/11/2023 tarihli snapshot'tan disk dönüldü ve snapsahottan dönüldüğü ana kadar olan verilerin erişimi için de yeni nexus instance oluşturuldu. Hat ekibinin yeni instance ortamındaki işlemleri tamamlandıktan sonra bu instance kaldırılacak. 275 complete Nexus IQ Server kullanıcı deneyimi iyleştirme maddeleri ve bug-fix ler yapıldı. 276 complete Prometeia, 3rd party paketi için firma gözetiminde Kaynak kodların Checkmarx taraması konusunda destek verildi. 272 complete Checkmarx operasyonel işlere destek verildi. Paketler sadece birim bazlı görüntülene bildiği için, ekiplerin paket görüntüleme sorunları, yetki ve sürüm çıkışı gibi destek konularını içermektedir. 269 complete Checkmarx sunucu down oldugu için schedule job'ların duruldurulmasından dolayı, planlı sürüm çıkışı olan ekiblere yoğun destek sağlandı. 270 complete Checkmarx uygulamasının haftasonu sunucularda yapılan güncellemelerden dolayı Down olmasının, Pazartesi günü fark edilmesinin ardından,  Manager ve Engine sunucuların da yüklenen dotnet web hosting yeni versiyonları silinip, 6.0.5 versiyonu yüklenmesi yapıldı. Daha önce de bu sorun  bir çok kez tekrar etmiş olup bu şekilde manuel bir çözümle devam edilmekteydi. Ancak bu kez  CxWebClient  sayfası açılmasına rağmen, Engin’ler de problem devam ediyordu, sorunun Checkmarx ekibin desteğiyle ActiveMQ kaynaklı oldugu tespit edilip silinip yeniden kurulması ile sorun çözüldü. 263 complete Nexus Iq akışında yapılan çalışmalar devreye alındı. 264 complete Twislock bulguları için Bilgi güvenliği ekibine destek verildi. Yeni talepler ve Netspark ürünü için toplantı planlandı. 265 complete Checkmarx ve Nexus Iq için Mail ve Jira üzerinden gelen taleplere destek verildi. 266 complete Checkmarx veri saklama politikası devreye alındı.  Disk kapasite probleminin önüne geçilmiş oldu. 257 complete Checkmarx  data retention mode aktif etmeden önce, scan history'sinin bütünlüğünün korunmasını sağlama çalışması. Bu sayede sunucu lisansı expire olmadan disk kapasite artırımı sağlanmış olacak. 258 complete Banka Hat ekibine hazırlanan python scripti ile NexusIQ banka AzureDevops hattına entegre etmiş olduk.  Önümüzdeki pazartesi (28 Ağus), ilk olarak tüm Java projeleri için devreye alınması planlandı, çalışma takip ediliyor olacak. 252 complete DevopsSofttech üzerinden Checkmarx için İstisna onay mekanızması devreye alındı. 253 complete Nexus IQ testleri banka hattında başarılı şekilde yapıldı, 254 complete DevopsSoftech operasyonlarına, sürüm çıkarken alınan hatalara ve manuel tarama isteklerine destek verildi. 243 complete Dockerfile'ı olan 14 projede Nexus IQ taraması başarılı olanlar ve olmayanlar için kontroller sağlandı. Elif Kılıçoğlu Saygıner'e bilgi verildi. 244 complete Banka hattın üzerinde Nexus IQ taramasının yapılmasını sağlayacak Python scripti hazırlandı. 14'unde skor ekibiyle tesleri yapılacak. 245 complete DevopsSofttech web üzerinden Checkmarx taramalarının gerektiğinde istisna talebi girilmesi için ekran geliştirmesi tamamladı, 15'ine kadar devreye alım yapılacak. 246 complete Paketlerin sürüm çıkışında yaşanan problemleri çözmeleri için inceleme ve destek verildi. 247 complete Yeni karne dönemi için NexusIQ baseline alınması 248 complete Grafana dashboard için sql scriptlerinin hazırlanması desteği 234 complete DevopsSofttech'te yapılan Nexus IQ taramalarının , Banka hattına adım olarak eklenebilmesi için  XLR dan hata/uyarı  vermeden geçebilmesi için hazırlanacak scriptin,  ilk adımı olan local ortamda environment ve connection çalışması yapıldı. 235 complete High bulgusu 26 bin civarında olan ACI02244  nolu paketin, Framework tarafında çözülmüş olmasından dolayı hepsinin FalsePositive olarak işaretlenmesi ekrandan time-out a sebebiyet vermekteydi,  bug fix yapıldı. 236 complete Devops Softtech checkmarx taramalarında içinde source code içermeyen paketlerin istisna tanımlaması çalışması için ekran geliştirmeleri yapıldı,  backend devam ediyor. 227 complete DevopsSofttech karne raporu için Elif'lere destek verildi. 228 complete NexusIQ taramalarının seri hatta alınması için son yapılan değerlendirme toplantısında, NexusIQ server a erişim çalışması için destek verilecek 229 complete Checkmarx Sunucusunda yapılan updateler den kaynaklı çıkan tarama yapılamaması ve  sistem yavaşlığı gibi problemler çözüldü 230 complete Paketlerin sürüm çıkışında yaşanan problemleri çözmeleri için inceleme ve destek verildi. 221 complete NexusIQ Taramasında hata alan 3 proje için Elif Kılıçoğlu Saygıner'e destek verildi. Hata nedeni bulundu ve pipeline'lar tetiklenerek taranma işlemi gerçekleştirildi. 222 complete Taranamayan Java 17 projeleri için pipeline çalışması yapıldı. 216 complete Devops Softtech Chechmarx Baz Değerler sayfasında database üzerinden istenilen custom raporlara destek verildi. 217 complete Chechmarx için database üzerinden istenilen custom raporlara destek verildi. 218 complete Devops Softtech Nexus Baz değerler sayafası son yapılan issue raporlar sayfasıyla güncellendi ve kapatılan issue'ların anlık durumu raporlanması sağlandı. 212 complete Yazılım paketlerinin güvenlik taramalarını direkt DevopsSofttech database'i üzerinden Grafana ile takip edilmesi için kurulum çalışması. 210 complete DevopsSoftech portal kullanımında kullanıcı problemlerine destek verildi. 208 complete DevopsSofttech NexusIQ issue raporlarında minor değişiklikler yapıldı ayrıca Directorate based issued summary ekranı yapıldı. 206 complete NexusIQ baseline alma yontemi değişti, karne için bulguları kapatması yeterli olan bulgular hesaplanacak, yeni bulgular mevcut baseline da olmayacak, bu doğrultuda 2 adet rapor geliştirmesi DevopsSofttech'te tamamlandı. 201 complete Microsoft low-code uygulaması Azure Power Portal ile chatbot geliştirme eğitimi. 202 complete Generative-AI konusunda yetkinlik artırma çalışmaları yapıldı. (Langchain kütüphanesiyle  Azure OpenAI'nin ""Davinci-003"" modeli ile belge içinde search uygulaması ) 203 complete Checkmarx taraması devops softtech portal üstünden yapılamayan third party projlerin <ENTRAPEER> ve farklı branch yapısından dolayı Bireysel İnternet Bankacılığ paketlerinin tarama analiz süreçlerine destek verildi. 204 complete DevopsSofttech Checkmarx hata alan taramalara sorun tespit ve bilgilendirme konusunda destek verildi. 195 complete Sürüm kontrol ekibi ve banka güvenlik ekibiyle, NexusIQ server taraması yapmak için Softtech tarafında konuşlandırılan mevcuttaki bizim oluşturdugumuz paralel hattın paketlerin tamamını Build edemediği için sürüm hattına eklenmesi gerekliliği konusunda yapılan bir dizi toplantı yapıldı, nihayetinde bu analiz adımının sürüm hattında doğrudan eklenmesine karar verildi ancak burada sürümün minimum etkilemesi için  yeni çözümler için çalışma bizim tarafımızdan yapılacak. 196 complete Checkmarx taramaları ve DevopsSofttech için kullanıcılara erişim, tarama, rapor alma konularında operasyonel destek verildi. 191 complete Nexus IQ sürüm kesme akışında trend değeri hesaplama için backend geliştirmeleri yapıldı, SQL db Baseline tablosu ve ekran geliştirmesi tamamlandı,  akış geliştirmesi devam ediyor. 192 complete Devops Softtech FalsePositive talep giriş ekranında job ile geç gelen sonucların,  ekrandan refresh butonu ile hızlı getirlmesi sağlandı. 186 complete Devops Softtech backend mail service geliştirmeleri ile softtech projelerinin Checkmarx ve NexusIQ tarama sonuçlarının backstage uygulamasında gösterilmesi sağlandı. 187 complete FalsePositive talep girişi yaparken sorun yaşayanlara ve gelen taleplere destek verildi. 188 complete DevopsSofttech üzerinde yeni baseline ekranında, karne hedefi olmayan birimler filtrelendi ve bazı düzeltmelerle performans iyileştirmesi yapıldı ekranda. 181 complete NexusIQ Taramalarının component değişimi takibini yapabilmek için  snapshot yapısı ve static baseline yapısı oluşturuldu, ekran hazırlandı. Ekran düzeltmeri yapılacak. Her paketin component değişimi tutulmuş olacak. 182 complete Checkmarx fullscan Taramaları için baseline siteye koyuldu. 178 complete Paralel Pipeline dan yapılan NexusIQ taramasında build edilemeyen DotNET paketlerindeki sorun inceleme çalışması yapıldı. Banka güvenlşik ekipleriyle yapılan toplantıda neden yapılamadığı hakkında bilgi verildi. Sürüm ekibine bilgilendirme notu hazırlandı. 175 complete NexusIQ da tarama başlatılamaması sorunun, işbank sunucusunda yapılan Java 17 versiyon güncellemesinden kaynaklı olduğu  tespit edildi ve sorun giderildi. SuccessFactory de henuz olmayan Genomda birimleri için Devops.Softtech Login esnasında directoryship mappinge göre kullanıcıya paket atama yapılacak. 171 complete Java paketleri NexusIQ taraması esnasında proje adından dolayı  build hatası almaktadır, bu bilgiyi genomda Repo bilgisinden cekmek daha sağlıklı olacaktır. checkmarkx ve nexusIQ da UYGDSTEK olan paketlerin taramalara dahil edilmemesi. Inceleme  &  geliştirme  yapılacak. Nexus IQ Server Softtech projeleri için anlık tarama başlat akışının ve build raporunun hazırlanması sağlanacak. (Softtech projeleri sürümü etkilemediği için paralelde takip ediyor olacağız) Nexus IQ Server ve Checkmarx raporlarına paketlerin DMZ de olup olmadığı bilgisi eklenecek. Bu bilgi genomun bize döndüğü veride “full name” ve “domain” bilgilerinde tutuluyor. Bu kısmı kontrol edip raporlarımıza ve ekranlarımıza ekleyeceğiz. İlerleyen dönemlerde olgunlaştıkça sürüm kontrol noktası da tasarlanabilir. (Banka Bilgi Güvenliği Ekibi’nin bu doğrultuda talebi mevcut.) (Bu çalışmaya Mayıs ayında başlayabileceğiz. Ne kadar süreceği konusunda henüz net bir tahmin yapamıyoruz. ) 156 complete NexusIQ Java ve .NET  paketlerinin pipeline parameterlerinin düzenlemesi, DevopsSofttechten yeni pipeline a tetiklendi. 157 complete Nexus IQ Server sürüm kontrolleri kapsamında aşağıda akış  geliştirildi. - Checkmarx akışına benzer akış tasarımı yapılacak. (Kırmızı Kart, Sarı Kart, Yeşil Kart) - Her sürüm geçişinde %10 bulgu azaltılacak. Azaltılmazsa bir sürüm izin verilecek sonrasında kesimi sağlanacak. (Bu açıdan Genom’a giden rakamları ve sürüm kesim notlarında düzeltme yapıldı. ) - Banka Bilgi Güvenliği Ekibi 9 ve üzeri puanlanan bulgular ile ilgili ek bir kontrol noktası da eklememizi istiyor. Bu geliştirmenin daha sonra ilave edilmesi gerekecek. 160 complete Checkmarx fullscan taramalarının bitimine mütakip baseline olması için yeni preset ile yapılan fullscan tarama sonuçları iletildi. 147 complete Checkmarx baseline için tüm pakertleri OWASP TOP 10 - 2021 preset ile FULL scan yapan partitionlar halinde bölüp taramaya sokan job yazıldı, taramalar devam ediyor. 162 complete Nexus IQ Server bulguları için 2 snapshot tablosunun hazırlanması. Aşağıdaki kriterler göz önünde bulundurularak hazırlanacak ve günlük periyotta tutulacaktır. (Önümüzdeki hafta sonuna kadar tamamlanması planlanıyor)  -CVSS skoru>7 olan bulgular için hazırlanacak tablo  -CVSS skoru<7 olan bulgular için hazırlanacak olan tablo. ( Snapshot tablosunda bulunması gereken verileri zaten belirlemiştik. Bize iletilen rapor formatındaki veri baz alınacaktır. ) 140 complete devops.softtech paket build hataları, component hataları inceleme & destek pom.xml dosyasında versiyon ile NexusServerdaki bileşen versiyonu eşleşmiyor, ACI11775, commons-beanutils : commons-beanutils : 1.9.3 141 complete checkmarx baseline incelenmesi 142 complete Java build hatası alan paketlerin incelenmesi Q1 DevOps Softtech 136 complete Genomdan Java paketlerinin çekilmesi Q1 DevOps Softtech 134 complete Sürüm ekibi ve banka güvenlik ekibiyle, NexusIQ taraması build hatası toplantısı. Q1 DevOps Softtech 131 complete Success Factory joblarının incelenmesi Q1 DevOps Softtech 132 complete Ekiplerin genomdan ve devops softtechten paket bilgilerinin , yetkileri ve müdürülük konusunda destek. Q1 DevOps Softtech 127 complete MySQL DevopsSoftech databaselerinin backup alınması, local de restore edilmesi. Q1 DevOps Softtech 128 complete NexusIQ build hatası toplantsı öncesi sürüm ekiplerinin kontrolü için failed olan Java ve .NET build örneklerin hazırlanması Q1 DevOps Softtech 124 complete NexusIQ taranamayan paketler için BT ekipleriyle görüşme yapılacak. Toplantı yapıldı ve ikinci toplantıya kadar yapılacak aksiyonlar planlandı. Q1 DevOps Softtech 110 complete checkmarx Taramasından çıkarılması talep edilen Rally paketleri muaf tutulmuştur. Q1 DevOps Softtech 117 complete Devops Softtech de tarama yapmak isteyen ama paket  görüntüleme yetkisi olmayan kullanıcılar için destek olundu. Q1 DevOps Softtech 118 complete False+ den dolayı hata alan kullanılara mail üzerinden destek verildi. Q1 DevOps Softtech 119 complete NexusIQ false positive' i etkileyen, checkmarx upgrade sonrası oluşan WA ile çözülen sorun kalıcı olarak fixlendi. Q1 DevOps Softtech 120 complete Checkmarx manual tarama için destek verildi. Q1 DevOps Softtech 121 complete Bilgi Güvenliği Ekibin kullandığı GetLastScanComponentByApplicationId servisinde , component bilgisi gelemyen kayıtların anlamlı hata vermesi sağlandı. Q1 DevOps Softtech 122 complete Mimarlar için paketere görüntüleme yetkisi verildi. Q1 DevOps Softtech 106 complete İşbank kodu olan kullanıcıların paketlere erişmesinde sorun çıkaran login ekranında bugfix yapıldı. Q1 DevOps Softtech 107 complete Paket görüntüleyememe sorununun kontrol edilmesi. Q1 DevOps Softtech 108 complete Paketlerin müdrülük bilgisinin boş gelmesi, jobların kontrol edilmesi . Q1 DevOps Softtech 102 complete Devops Softtech Checkmarx sonuçlarını görüntüleme sayfasında Banka Score ekibine ait bir paket görüntülenemiyor. Banka projeleri için erişim yetki yapısının incelenmesi ve gerekli düzenlemenin yapılması gerekiyor. Devops Softtech - Checkmarx Development Guideline - Softtech DevOps - Confluence Q1 Checkmarx upgrade 2 complete Checkmarx ürünü için yeni versiyon geçişinin gerçekleştirilmesi Q1 Chackmarx Baseline 49 complete Checkmarx ürünü için yeni versiyon geçişinin gerçekleştirilmesi Q1 NexusIQ 98 complete NexusIQ severity değeri 7 ve üzeri olanların devops.softtech panelinde kırılımlarının detaylı olarak gösterilmesi Q1 Checkmarx 100 complete Tüm ürünler için versiyon güncellemesi sonrasında Checkmarx fullscan yapılarak baseline alınması NexusIQ 166 complete Java projelerinin toplu taraması NexusIQ 167 complete Dotnet projelerinin toplu taraması","{'title': 'DevOps Softtech Panel', 'id': '80480967', 'source': 'https://wiki.softtech.com.tr/display/SDO/DevOps+Softtech+Panel'}"
"Quarter Başlık Tasks Detaylar Kazanımlar Q3 Cost Dashboard 60 complete Devops hattında softtech-cost-center label'ını otomatik set edebilmek için city tarafından ürün sahibi bilgisini almak istiyoruz. Bunun için API isteğinde bulunuldu. Q3 Cost Dashboard 58 complete Büşra Kılıç hat tarafında softtech-product-name label'ının set edildiği kısımda bir düzeltme yaptığını belirtti. Bunun google cost dashboard'a yansıması kontrol edildi. Q3 Cost Dashboard 56 complete Erdem Bey'e Google Cost dashboard sunumu yapıldı. Feedbackler alındı ve iyileştirmeler için aksiyonlar oluşturuldu. Q2 Cost Dashboard 54 complete Google Cost Dashboard için gelen geri bildirimlere göre iyileştirmeler yapıldı ve dashboard'daki mimariyi açıklayan bir sunum hazırlandı. Q2 Cost Dashboard 52 complete GKE resources labelları üzerinde çalışma yapıldı ve cost dashboard üzerinde düzenlemelere devam edildi. Q2 Cost Dashboard 49 complete VM instance oluştururken vm'e verilen label'ın boot diskine otomatik yansıması için cloud function ve pub-sub yapısı kullanılarak yapılabileceği bulundu. Denemeler yapıldı ancak bu yapıda da ekstra maliyet çıkacağı ön görüldü ve vazgeçildi. Q2 Cost Dashboard 50 complete Bigquery'den gelen Billing export datasında google tarafından güncellemeler yapılmış. Bu güncellemeler incelendi. Q2 Cost Dashboard 44 complete Softtech-rally projesi altında resource-cost-center label'ı olmayan vm disklerine label eklendi. Bu işlemi yapan bir bash scripti de hazırlandı. Q2 Cost Dashboard 45 complete VM instance oluştururken vm'e verilen label'ın boot diskine yansımadığı görülmüştü. Bu durumu nasıl otomatize edebiliriz diye araştırma yapıldı. VM oluştururken startup script kullanılabileceği bulundu ancak bu uygulama güvenlik zafiyeti nedeniyle bize uygun olmadığı anlaşıldı. Farklı bir yöntem için araştırmalar devam ediyor. Q2 Cost Dashboard 46 incomplete GCP tarafında resource yaratırken label kullanımı için policy'lerin oluşturulmaya başlanması Q2 Cost Dashboard 39 complete Google tarafı BigQuery ve Billing Dashboard arasindaki cost farkı için timezone farkından dolayı olduğunu paylaşmışlardı. Bu durumu düzeltmek için looker studio da inceleme yapıldı. Looker uzerinde usage_start_time a bagli bir field daha yaratip timezone değiştirilmesi denendi ancak başarılı olamadı. bigquery tarafındaki sorgularda düzenlenmesi denenecek. (DATE degil, SELECT DATE(usage_start_time, ""America/Los_Angeles"") seklinde) Q2 Cost Dashboard 40 complete Label kullanımı için policy oluşturulması hakkında araştırmalar yapıldı. https://www.doit.com/automatically-label-google-cloud-compute-engine-instances-and-disks-upon-creation/ https://support.tools/post/opa-gatekeeper-require-labels/ https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller Q2 Cost Dashboard 36 complete Project cost center'ların gösterildiği sayfada bir projeye dahil olmayan masraflar 'Non-Project' adı altında  ve detayı ile gözükecek şekilde ayarlandı. Q2 Cost Dashboard 30 complete Resource-cost-center label'ı eksik olan resource'ların belirlenmesi çalışması yapıldı. Q2 Cost Dashboard 27 complete Dashboard'a bigquery ile data bağlanmasının sağlanması üzerine çalışıldı. Q2 Cost Dashboard 28 complete Taha ile toplantı yapıldı. Dashboard'daki sorunlar üzerine görüşüldü. Q2 Cost Dashboard 23 complete Eklenen yeni labelların dashboard'a yansıması ile dashboard üzerinde yeni düzenlemeler yapıldı. Hakan Bey'e sunum yapıldı. Q2 Cost Dashboard 16 complete Lookerstudio'da cost dashboard üzerinde costs+discount=actual cost hesabı yapılarak maliyetlerimiz hesaplanıyordu. Ancak bu formül doğru çalışmamaya başladı. Bunun için incelemeler yapılması ve çözümünün sağlanması Q2 Cost Dashboard 17 complete Label yapısında Baransel ile yeni bir planlama üzerine çalışması. GCP'de project-cost-center ve resource-cost-center adlarında yeni labellar oluşturulması ve testlerinin yapılarak dashboard üzerine yansıtılması. Q2 Cost Dashboard 18 complete Hat tarafında labellarda yapılan düzenlemeler merge edildi. Cost Dashboard'a yansıması kontrol edildi. Yeni labellara göre dashboard'da düzenlemeler yapıldı. Q1 Cost Dashboard 12 complete Hat tarafından Büşra ile softtech-cost-center label yapısında yapılan düzenlemeler üzerine çalışma yapılması Q1 Cost Dashboard 10 complete Cost Dashboard'un ekip içi sunumu yapıldı. Q1 Cost Dashboard 2 complete Dashboard üzerinde Total Cost - Discount = Actual Cost şeklinde gösteriminin sağlanması Q1 Cost Dashboard 6 complete Dashboard üzerinde undefined ve null şeklinde gözüken değerler var. Bu değerlerin anlamlandırılması gerekiyor null gözüken değerler anlamlandırıldı undefined gözüken değerler için hat ekibine bilgi verildi. Q1 Cost Dashboard 7 complete Cost table'a percentage column eklenebilir mi? Araştırması yapılıyor Yeni bir field eklenerek orada bir formül ile hesaplama yapılabileceği görüldü. Ancak satır satır okunduğu için data, tablonun total değeri alınamadı. Yeni bir field eklendi actualcost olarak ve bu değerde cost-discount toplanarak actualcost hesaplandı ve tablolarda artık bu değer gösteriliyor Ayrıca costusd diye de bir field eklendi, burada da actualcost'un usd değeri hesaplandı ve bu column tablolara eklendi. Q1 Cost Dashboard 8 incomplete Board üzerinde softtech-cost-center label'ına sahip olanlar ve olmayanları göstermek için iki tablo yapılıp gerekli filtreler uygulanacak. Taha'ya soruldu, dönüş bekleniyor. label'a sahip olanları filtreleyebildik ancak label'ı olmayanları filtrelerken hatalı sonuç dönüyor. Kontrol ediliyor. Cost Labelling 20 complete GCP üzerindeki projelerin ve shared kaynakların ilk etiketlemesinin yapılması","{'title': 'Cost Dashboard', 'id': '80480969', 'source': 'https://wiki.softtech.com.tr/display/SDO/Cost+Dashboard'}"
,"{'title': 'Anthos OnPrem Platform', 'id': '80480971', 'source': 'https://wiki.softtech.com.tr/display/SDO/Anthos+OnPrem+Platform'}"
,"{'title': 'Google Cloud', 'id': '80480973', 'source': 'https://wiki.softtech.com.tr/display/SDO/Google+Cloud'}"
,"{'title': 'Epics', 'id': '80481163', 'source': 'https://wiki.softtech.com.tr/display/SDO/Epics'}"
,"{'title': 'Supported Tools and Versions Guideline', 'id': '80481297', 'source': 'https://wiki.softtech.com.tr/display/SDO/Supported+Tools+and+Versions+Guideline'}"
,"{'title': 'Duyurular', 'id': '80481304', 'source': 'https://wiki.softtech.com.tr/display/SDO/Duyurular'}"
"Quarter Başlık Tasks Detaylar Kazanımlar Zabbix Envanter 16 complete Zabbix üzerinde monitor edilen uygulama ve sunucuların envanter listesi çıkarıldı. Yeni ortamların zabbix üzerinden izlenmesi için gerekli tanımların yapılması devam ediyor. Q2 Uygulama Destek 14 complete Anthos prod cluster üzerindeki 360survey uygulama deploy için destek verildi. Q1 Zabbix Guidelines 12 complete Zabbix uygulaması için guideline hazırlanması Q1 Jira Service Desk 9 incomplete Jira Service Desk üzerinden Platform ekibine gelecek talepler için akışın belirlenmesi. Gerekli yetkilendirme ve tanımların yapılması. Yapılan çalışmaların Klavuz dokümana yansıtılması Service Desk Guide - Softtech DevOps - Confluence Q1 Zabbix Yetki 10 complete Zabbix'te Uygulama destek ekibine hangi yetkilerin verileceğinin belirlenmesi ve dokümante edilmesi. Şeyma'ya bu yetkilerin tanımlanması. Uygulama Destek Ekibi - Softtech DevOps - Confluence Q1 CheckMarx Upgrade Zabbix Dashboard Oluşturma 3 complete Zabbix üzerinde Emtia projesi için PROD ortamında dashboard iskeletinin oluşturulması http://192.168.104.224:8080/ Q1 Zabbix Dashboard Analiz 4 incomplete Emtia PROD dashboard için izlenmesi gereken ve alert oluşması gereken konular için analiz gerçekleştir. Q1 Zabbix Dashboard Klavuz 6 incomplete Emtia PROD dashboard kullanım Klavuzunun hazırlanması. (Yetkiler, İçerdiği bilgiler, Alertler vb. ) Emtia Dashboard Prod Ortmaı - Softtech DevOps - Confluence","{'title': 'Prod Uygulama Destek', 'id': '80481820', 'source': 'https://wiki.softtech.com.tr/display/SDO/Prod+Uygulama+Destek'}"
,"{'title': 'Emtia Prod Ortamı Dashboard', 'id': '80481829', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80481829'}"
,"{'title': 'Zabbix yetkiler', 'id': '80481833', 'source': 'https://wiki.softtech.com.tr/display/SDO/Zabbix+yetkiler'}"
,"{'title': 'Zabbix', 'id': '80481835', 'source': 'https://wiki.softtech.com.tr/display/SDO/Zabbix'}"
,"{'title': 'Uygulama Destek Ekibi', 'id': '80481839', 'source': 'https://wiki.softtech.com.tr/display/SDO/Uygulama+Destek+Ekibi'}"
,"{'title': 'Tasks', 'id': '80481843', 'source': 'https://wiki.softtech.com.tr/display/SDO/Tasks'}"
"Başlık Tasks Detaylar Kazanımlar IK 38 complete Ekip içi değerlendirmeyi tamamla. ROADMAP Hat ekibinin ayrı olmasının gerekçeleri ve PLT efor azaltılması için yapılabilecek çalışmalar ROADMAP Bürke'den gelen yorumların ROADMAP'e yansıtılması. Conversational AI, data plateau & studio.onplateu uygulamalarının yeni hatta taşınması. SUNUM 33 complete devops hizmet sunumunun hazırlanması ve sunulması TEFTİŞ 30 complete Bulut yazılım gider yönetimi teftiş çalışmalarına destek verilmesi ile bu konuda cevap yazılması gerekiyor. AWS 28 complete Sertifika alamayanlar için taahütnameye gerek var mı? CHECKMARX 26 complete Checkmarx(Endpoint) ekibiyle toplantı organize et. Disk arttırmak için lisans yükseltmek gerekiyor? Maliyet ne kadar? FATURA 24 complete Yeliz Elmas'tan gelen İşnet VMWare fatura mail'ine cevap yaz. Masraf formu doldurulacak JIRA 22 complete Ticari bankacılığa DevOps anlatılacak JIRA 19 complete Jira desk üzerinde Uygulama destek için yetkilendirme ve akışın netleştirilmesi BANKA 20 complete SonarQube ve NexusIQ için paralel hat işletme maliyeti konusunda banka ile görüşülmesi gerekiyor. Bunların ana hatta aktarılması gerekiyor. PLT 3 complete Ömer için Jira Yetkisinin Alınması . PLT atamaları için yetki alınması gerekiyor. SUNUM 4 complete Plateau Ekosistem Çalışması sunum . DEĞERLENDİRME IK ekip içi değeerlendirme yapılması BULGU 14 complete Checkmarx kod gözden geçirme konusunda açılan bulgu için değerlendirme yapılacak ile birlikte bu konuda çalışma yapılacak.","{'title': 'Melih', 'id': '80481845', 'source': 'https://wiki.softtech.com.tr/display/SDO/Melih'}"
,"{'title': 'Kılavuzlar', 'id': '80481938', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80481938'}"
,"{'title': 'Toplantı Notları', 'id': '80481940', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80481940'}"
c891d19e-4f95-45b7-8c92-1fddee7a5e23 com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint Create meeting note Incomplete tasks from meetings SDO 10 space:SDO incomplete meeting-notes All meeting notes c891d19e-4f95-45b7-8c92-1fddee7a5e23 com.atlassian.confluence.plugins.confluence-business-blueprints:meeting-notes-blueprint meeting-notes Plan your meetings and share notes and actions with your team. Meeting notes Create meeting note meeting-notes,"{'title': 'Meeting notes', 'id': '80482176', 'source': 'https://wiki.softtech.com.tr/display/SDO/Meeting+notes'}"
Date Kararlar Azure Github için Asenkron Geolocation replika kurulmayacak. Günlük olarak VM Backup'ı alınarak Azure'da saklanacak. Azure github os imajı sadece github versiyon güncellemelerinde alınacak. Github güncelleme işlermi bir sonraki versiyon çıktığında gerçekleştirilecek. Action items 1 complete Github için Azure'da bir VM backup alınması için aksiyon alınacak. Burda çalışan sistemin snapshot'ı alınacak. 5 complete Github maintenance ve policy için bir guideline hazırlanacak. Github Maintenance And Policies - Softtech DevOps - Confluence 6 complete Github actions aktif hale getirilecek. 7 complete Microsoft ile yapılacak bir sonraki toplantının daveti ve gündemi paylaşılacak Duygu Silivri'nin tuttuğu toplantı notları aşağıdadır. 150,"{'title': '2023-02-22 Microsoft Ortak Çalışma', 'id': '80482297', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=80482297'}"
"Amaç Banka üst yönetiminin talebi üzerine Plateau Core kodlarının günlük olarak Banka Scoretfs aracına senkronize edilmesine karar verilmiştir. Plateau Core paketlerinin listesi aşağıdaki mail ile temin edilmiştir. İçerik Repo Listesi true Senkronize Edilen Repolar Paket Repository rally_rally-commons rally / rubix / rally-commons · GitLab rally_metadata rally / rubix / metadata · GitLab rally_scheduler rally / rubix / scheduler · GitLab rally_eventstore rally / rubix / eventstore · GitLab rally_reporting rally / rubix / reporting · GitLab rally_basic https://gitlab.rally.softtech/rally/basic rally_sentinel rally / rubix / sentinel · GitLab Apigateway rally / rubix / apigateway · GitLab IAMUsers Plateau Security / Proxy Plateau IAM Services / iamusers · GitLab IAMAuthorization Plateau Security / Proxy Plateau IAM Services / iamauthorization · GitLab Identity Provider Plateau IAM Providers · GitLab Plateau Security / Plateau IAM - Identity Provider · GitLab rally_config rally / rubix / config · GitLab Mimari true Mimari Diyagram Bu projede https://scoretfs.isbank/ISBANK/Platform_PLATEAU ve Gitlab.rally.softtech makinelerine erişim için http://10.80.36.112:8080/ ip'li Jenkins makinesi kullanılmıştır. IP:PORT → IP:PORT erişimi için maximo açılmıştır. İki yıl süreli izin alınmıştır. ScoreTFS true ScoreTFS Hedef proje: https://scoretfs.isbank/ISBANK/Platform_PLATEAU Kod senkronizasyonu için st_devops_ent kullanılmılştır. Gerekli yetkiler Can Taçoğlu tarafından verilmiştir. ( st_devops_ent referans link) Jenkins true Jenkins Bu proje için Jenkins üzerinde reposync_plateau-security, reposync_plateau-security_proxy-plateau-service, reposync_rally, reposync_rally_rubix adlı 4 job oluşturulmuştur. Bu jobların farkı gitlab source pathlerinin farklı olmasıdır. Job üzerinde scoretfs.isbank'a erişim için scoretfs.isbank'da st_devops_ent kullanıcısı için KOD(Okuma, Yazma, Yönetme) yetkilerine sahip bir Personel Access Token oluşturulmuştur. Jenkins PAT oluşturma Oluşturduğumuz PAT Secret kısmına yapıştırılır. Ve bir ID verilip Ok'a basılır. Sonrasında Binding kısmından Pat'ımız pipeline eklenir. Bu proje için Jenkins Windows makineden çalıştığı için Powershell script'i yazılmıştır. Powershell Script'i Kodda görüldüğü üzere Scoretfs.isbank tarafına 443 üncü porttan, gitlab.rally.softtech'e 22 inci porttan erişim sağlanmıştır. Script'in çalışabilmesi için jenkins agent'ın çalıştığı makinanın terminalinde aşağıdaki komutun çalıştırılarak ssl doğrulamasının kapatılması gerekmektedir. powershell Referans Dokümanlar true Paydaşlar Kişi Rol İletişim Bilgisi Can Taçoğlu Banka Score Ekibi ScoreTFS sorumlusu mail Olgu Okur Üretim Takım Lideri, Yeni Nesil Teknolojiler Orta Katman Uygulamaları (Plateau Core). Caner Akkaya Üretim Takım Lideri, Yeni Nesil Teknolojiler Orta Katman Uygulamaları (Plateau Security). true Bakımimage-2023-2-23_16-21-32.pngimage-2023-2-23_15-49-59.pngScreenshot 2023-02-23 at 11.07.02 AM.pngScreenshot 2023-02-23 at 11.03.29 AM.png","{'title': 'Plateau Core Kod Senkronizasyonu', 'id': '80482344', 'source': 'https://wiki.softtech.com.tr/display/SDO/Plateau+Core+Kod+Senkronizasyonu'}"
Banka Projeleri: Tarama Parametreleri: Default branch : Develop Softtech Projeleri: Tarama Parametreleri: Default Branch: Master,"{'title': 'Checkmarx Entegrasyonu', 'id': '84836355', 'source': 'https://wiki.softtech.com.tr/display/SDO/Checkmarx+Entegrasyonu'}"
,"{'title': 'Devops Softtech Paneli', 'id': '84836624', 'source': 'https://wiki.softtech.com.tr/display/SDO/Devops+Softtech+Paneli'}"
Mimari Integrations Software Components,"{'title': 'Devops Softtech Development Guideline', 'id': '84836627', 'source': 'https://wiki.softtech.com.tr/display/SDO/Devops+Softtech+Development+Guideline'}"
,"{'title': 'Test', 'id': '84836630', 'source': 'https://wiki.softtech.com.tr/display/SDO/Test'}"
"Başlık Tasks Detaylar Kazanımlar 2 complete Zabbix.softtech 'in kapatılması işinin takip edilmesi CodeCoverage Bankada Code Coverae ölçümü, 6 incomplete devops.softtech ile sonar.softtech'e entegrasyon 7 incomplete Bankada Code Coverage raporlarının oluşumunun artırılmasına ilişkin çalışmalar - ekiplere yönelik test yazımına ilişkin kılavuz ve talepler 8 incomplete Banka Pipeline ile entegrasyon çalışmaları","{'title': 'Sinan', 'id': '84836748', 'source': 'https://wiki.softtech.com.tr/display/SDO/Sinan'}"
Quarter Başlık Tasks Detaylar Kazanımlar Sertifika Güncelleme 42 complete https://gitlab.rally.softtech uygulamasının sertifikası güncellendi. Q2 Güvenlik Açığı 40 complete wiki.softtech.com.tr adresi üzerindeki güvenlik taramalarında subdomain header üzerinde eksik çıkan Strict-Transport-Security (HSTS) parametrenin oluşturduğu güvenlik açığının giderilmesi için çalışma yapıldı. 38 complete GCR.io bileşenlerine yazılımcı bilgisayarlarından erişilebilmesi için çözümler araştırılıdı ve Hat ekibi ile beraber değerlendirmesi yapılarak gcloud ile docker config oluşturularak erişim sağlanması yönünde yöntem belirlendi. Q1 Sertifika 36 complete 2pm.softtech.com.tr adresin ssl sertifikasının güncellenmesi Q1 GÜVENLİK 34 complete Openssh güvenlik açıklarının giderilmesi,"{'title': 'Security Enhancement', 'id': '84836924', 'source': 'https://wiki.softtech.com.tr/display/SDO/Security+Enhancement'}"
"Quarter Başlık Tasks Detaylar Kazanımlar city 114 complete city.softtech.com.tr adresine erişim için iap entegrasyonunun doğru bir şekilde çalışması için destek verilmeye devam ediliyor. Uygulamanın kod tarafında düzenlemeler yapılıyor ardından load balancer ve iap ayarları ile tekrar test ediliyor. Jenkins 111 complete Hat ekibi ile beraber Jenkins plugin reposu gitlab ortamından githuba taşındı ve github'daki reponunun okunması için Jenkins'te konfigürasyon değişiklikleri yapıldı. Promptbox 112 complete Promptbox uygulaması ayağa kaldırıldı.( https://promptbox.rally.softtech/ ) uygulama üzerinde saml entegrasyonu olmadığı için login sistemi şuan çalışmıyor. Onun için ekiplerle görüşülüp planlama yapılacak. Promptbox 108 complete promptbox uygulaması için gcp üzerinde sunucu kurulumu gerçekleştirildi. promptbox.rally.softtech dns kaydı yapıldı. DNS sertifikası security ekibinden talep edildi Sonarqube 106 complete sonarqube.rally.softtech ortamın gelen taramalarda ""413 Request Entity Too Large"" hatası alındığı belirtildi. Bunun için inceleme yapıldı. Nginx tarafında client_max_body_size 300M ayarlanarak sorunun çözüldüğü görüldü. OpenShift Artemis 104 complete Redhat ekibinden Erkan Ercan ile Openshift Artemis üzerindeki monitoring gereksinimleri konuşuldu ve defaultta gelen Prometheus,Grafana uygulamaları konfigürasyonla enable edildi. plateau city 102 complete city.softtech.com.tr için load balancer tanımı yapılmıştı. City ekibi ile de testler gerçekleştirildi. Invalid Host Header hatası alınıyordu. Bu hatanın giderilmesi için proje tarafında kontroller sağlandı ve hata giderildi. Ardından 3000 portuna ve 7007 portuna yönlendirme yapmamız istendi. 3000 portu için yaptığımız konfigurasyon başarılı çalıştı ancak 7007 portu için sorun yaşıyoruz. Bunun için google tarafına bir case açıp o tarafa danışacağız. Vector DB SaaS Proje Çalışması 100 complete Vektör DB SaaS Proje Çalışması kapsamında araştırmalara devam edildi. plateau city 98 complete City.softtech.com.tr için Load Balancer tanımı yapıldı. DNS için sertifika talebi yapıldı. Q4 Yetkilendirme 95 complete Artemis Openshift ortamındaki servislerin yük testinin test otomasyon sunucusu üzerinden sağlanması için gerekli yetkiler tanımlandı. Q4 Yetkilendirme 96 complete Plateau güvenlik ekibi ile çalışan DefineX firma çalışanlarının gcr.io daki imagelara erişim problemi giderildi. Q4 Vector Database 92 complete GCP üzerinde vector database kurulumları için compute instance açıldı ve Milvus kurulumu yapıldı. Q4 Monitoring & Alert 90 complete GCP utils cluster üzerinde çalışan eski versiyonlu prometheus, grafana ve alartmanager uygulamaları upgrade edildi. Q3 Monitoring & Alert 88 complete GCP üzerinde jenkins-vm'deki disk kullanımı için alert oluşturuldu. VM üzerindeki kullanılabilir disk boyutu %10 altına düştüğünde alert oluşturacak şekilde ayarlandı. Test edildi, başarılı alert oluştuğu görüldü. Q3 Erişim Problemi 86 complete jenkins.rally.softtech uygulamasından yeni kurulan sonarqube.rally.softtech uygulamasına erişim sorunu olduğu belirtildi. Bunun için firewall kuralı oluşturuldu ve test edildi. Erişimin sağlandığı görüldü. Q3 Upgrade 84 complete GKE Utils cluster üzerindeki docker image'lı gitlab ve nexus node pool'ları containerd image'lı gke versiyona upgrade edildi. Q3 Upgrade 81 complete GKE Utils cluster'da bulunan docker image'lı node pool'ların containerd image'lı node'lara migrate edilmesi için containerd image'lı yeni node poollar oluşturuldu. Yeni node poollara geçiş için Hat ekibi ile beraber çalışma planlanmaktadır. Q3 Otomasyon 82 complete GCP'de Tag'i olmayan imageların jenkins üzerinden job ile otomatik silinmesi için Hat ekibine aktarım yapıldı. Q3 Yetkilendirme 78 complete Google Speech modeli için veri yapay zeka ekibine gerekli yetkilendirmeler sağlandı. Q3 Disk Optimization 74 complete GCP softtech-rally projesi üzerinde tag'i olmayan imageların silinmesi ile ilgili çalışma yapıldı. Toplamda 52839 image silindi. Image'ların silinmesi ile aylık baz fiyat olarak 321$ 'lık kazanç sağlaması beklenmektedir. Q3 Disk Optimization 75 complete GCP softtech-rally projesinde attach olmayan 6 aydan eski ve 6 aydır kullanılmayan disklerin snapshotı alındıktan sonra bu diskler silinerek disk optimization çalışması yapıldı. 13.3 TB'lık disk silinerek aylık baz fiyat olarak 544,84$ kazanç sağlaması beklenmektedir. Q3 Disk Optimization 76 complete GCP softtech-rally projesinde 1 yıldan eski snapshotlar silindi. 19.6 TB'lık snapshotların silinmesi ile aylık baz fiyat olarak 983$ kazanç sağlaması beklenmektedir. Q3 Sunum 70 complete GCP üzerindeki Figopara Artifact Repository kaynaklarına erişim için oluşturulan service account ve paketleri indirmeleri için gerekli konfig adımları dökümante edilerek müşteri ile paylaşıldı. Q3 Yetkilendirme 67 complete GCP üzerinde figopara dosya paylaşımı için artifact registry oluşturulup gerekli yetkilendirmeler sağlandı. Q2 Cluster Optimization 68 complete GCP softtech-rally projesi üzerindeki eski hat için kullanılan dev-new kubernetes cluster ortamı kaldırıldı Q2 Yetkilendirme 64 complete GCP softtech-rally projesi üzerindeki plateaux cluster için yetki isteyen kişilere yetkiler sağlandı. Q2 Yetkilendirme 62 complete Openshift artemis ortamında yetki talep eden kullanıcılar için yetkilendirmeler yapıldı Q2 Cluster Optimization 60 complete GCP softtech-rally projesi üzerindeki dev-new clustera ait node poollar sıfırlandı. Ağustos ayı sonunda cluster'ın tamamen silinmesi için plan yapıldı. Q2 Yetkilendirme 58 complete gcr.io developer yetkilendirmeleri için dış kaynaklar ile ilgili bir bug var, geçici olarak bir kaç dış kaynak developer'a elle yetki verildi Q2 SonarQube Kurulumu 56 complete Yeni kurulan SonarQube üzerinde Plugins ve Quality Profiles kontrolleri sağlandı. Hat ekibine teslim edilmek üzere planlama yapıldı. Q2 SonarQube Kurulumu 54 complete SonarQube Kurulumu gerçekleştirildi. Q2 Snapshot Optimization 44 complete GCP üzerindeki 1 yıldan eski snapshotların silinmesi çalışması tamamlandı. Toplamda aylık $4611 kadar kazanç sağlandı. Q2 Disk Optimization 32 complete GCP üzerinde 1 yıldan eski disklerin silinmesi, 6 aydan eski ve 6 aydır kullanılmayan diskler için de snaphot alınarak ilgili diskler silinmesi çalışması tamamlandı. Toplamda aylık $1638 kadar kazanç sağlandı Q2 Disk Optimization 26 complete GCP Platform üzerinde son 1 yıldan eski disk snapshotlarının listesi çıkarıldı ve silinmesi için çalışma planlandı. Q2 Disk Optimization 24 complete GCP'de silinecek disklerin ve snapshot'ı alınacak disklerin listesi çıkarıldı ve 31.05.2023 tarihinde alınacak bu işlemler için duyuru çıkıldı. Q2 Update 22 complete GCP Ünlüco arasındaki S2S VPN bağlantı için GCP'de tanımlanan VPN tunel bilgileri güncellendi ve key Ünlüco ile paylaşıldı. Q2 Disk Optimization 20 complete GCP plateaux cluster üzerinde prometheus meticleri için kullanılan pvc'lerin diskleri artırıldı. Q2 DB Kurulumu 18 complete Mimari coe ekibi için GCloud üzerinde mimari-coe-analiz2 sunucusunda PostgreSQL kurulumu yapıldı, talep edilen service_db isimli db bu ortama taşındı. Q2 Sertifika Güncelleme 16 complete Jenkins.rally.softtech sertifikası güncellendi. Q2 Sunucu Talebi 14 complete Mimari coe ekibine backstage uygulaması için GCP softtech-rally projesi altında prod sunucu kurulumu yapıldı. Q1 Upgrade 12 complete Playground ve utils cluster üzerindeki node poolların versiyonları 1.23.24-gke.1800 versiyonuna upgrade edildi. Q1 Upgrade 6 complete GCP platform üzerinde Ubuntu 18.04 base image kullanan VM sunucularının listesinin çıkartılması ve ekibe dahil olan sunucularla ilgili gerekli aksiyonların planlanması Q1 Upgrade 4 complete Softtech-rally utils GKE cluster'ın Certificate Authority(CA)'si 29 Nisan 2023'de expire olacak ve gerekli credential rotasyonun uygulanarak CA güncellemesi yapıldı. Q1 Upgrade 2 complete GCP Plateaux cluster üzerindeki Grafana, Prometheus ve Alertmanager uygulamaların 40.5.0 helm chart versiyonuna upgrade edilmesi 9 complete GCP Plateaux cluster için Spot instance geçişinin yapılması. 10 complete GCP kullanılmayan/uzun süredir erişilmeyen instance'ların kapalı duruma getirilmesi 28 complete GCP gcr.io imajlarının saklama yöntemi konusunda ön çalışmalar  (artifact repository ya da alternatif yöntemler konusunda araştırma ve toplantılar yapıldı. 30 complete GCP chat cluster üzerinde chat.softtech.com.tr adresinin arkasında çalışacak şekilde data-plateau ekibinin AI uygulamasının deploy edilmesi. Maliyet Azaltma 34 complete GCR üzerinde duran imajların kullanılmayanlarının tespit edilip archive amaçlı daha ucuz bir bucket'a aktarılması için POC olarak Harbor kurulumu yapıldı. Bu yapıyı devreye aldığımızda aylık 2000 dolar civarı tasarruf hedefliyoruz. Maliyet Azaltma 38 complete GCR üzerindeki container imajlarını taşımak için gcr-archive.rally.softtech isminde repo kurulumu tamamlandı ve ilk taşıma (2019'dan eskiler) başlatıldı. Hat ekibinin doğrulaması sonrası CAB'te karar ile taşı&sil operasyonu günlük job'lar ile yapılacak. Altyapı Destekleri 39 complete Temmuz sonunda kapatılacak eski hat cluster için disk genişletme ve performans testlerin destek verildi Altyapı Desteği 40 complete City (Backstage) için tasarım/altyapı desteği verildi. Altyapı Desteği 45 complete City (backstage) ssl proxy ve backstage ayarları konusunda destek verildi. Altyapı Desteği 49 complete Google Cloud İşfaktoring arasındaki Site2Site VPN, İşnet tarafıyla ortak çalışma ile güncellendi. Altyapı Desteği 50 complete Faktoring uygulaması Int ortamı İşfaktoring servisleri arasındaki network/firewall incelemesi yapıldı, İşnet ile birlikte devam eden bir seri inceleme devam ediyor. Eski Hat Desteği 52 complete Temmuz sonunda kapanacak olan eski cluster'daki manual disk/snapshot işlemleri için destek verildi.","{'title': 'Google Cloud Epic', 'id': '84836927', 'source': 'https://wiki.softtech.com.tr/display/SDO/Google+Cloud+Epic'}"
"Quarter Başlık Tasks Detaylar Kazanımlar Github Enterprise 64 complete Github Enterprise replica hatası için açılan case sonucu önerilen yöntemler uygulanarak incelemeler devam ediyor. Milvus Vector Database 59 complete Azure üzerinde Veri Bilimi ve Yapay Zeka ekibi için Milvus veri tabanının GUI olan ATTU kurulumu yapıldı. SaaS Vector Database 60 complete Vektör DB SaaS proje çalışması için yapılan araştırmalarla ilgili toplantı yapıldı. Bussiness Model oluşturulması konusu ile çalışmalara devam edildi. Github Enterprise 56 complete Github Enterprise prod uygulaması 3.8.10 versiyonundan 3.10.2 versiyonuna upgrade edildi. Upgrade sonrası replica sunucusundaki servisler ayağa kalkmadı. Bunun için Github'a case açıldı ve supportun çözüm adımları uygulanarak servisler ayağa kaldırıldı. Sorunun kalıcı çözümü için support incelemeye devam ediyor. Github Enterprise 54 complete Github Prod ortamında yaşanan internete çıkış probleminin çözümü için yeni replika sunucular oluşturulup internete erişimi test edildi. Yeni Azure sunucularında internete başarılı olarak çıkış sağlandı. Ama image ile kurulan githubda ssh agent'i ile ilgili sorun yaşandığı için devreye alınmadı. Microsoft ile konuyu inceledik, paralelde dönüş bekleniyor. Github Enterprise 52 complete Github Enterprise prod ortamında 3.8.2 versiyonundan 3.8.10 versiyonuna upgrade edildi. Upgrade sonrası git replikasyon işlemleri sırasında bazı repolar için hata alındı ve Github'a case açıldı. Case sonucu önerilen işlemler uygulandığında ilgili hata giderildi. Github Enterprise 50 complete Github enterprise test ortamında 3.10.2 versiyona upgrade edilirken alınan hata çözüldü ve upgrade sağlandı. Vector Database SaaS Hizmeti 48 complete Vektör DB SaaS Proje Çalışması için planlama toplantısı yapıldı. Toplantı sonucu olarak, piyasa ve rakip analizi çalışması sonrasında yeniden bri araya gelinerek çalışmaya devam edilmesi kararı alındı. Github Enterprise 43 complete Studio ekibinin github-test ortamındaki studio organizasyonunda gerekli actionsları çalıştırması için runner agent kurulumu sağlandı. Github Enterprise 44 complete Security ekibi için oluşturulan github runner sunucusu üzerinde actionsların çalışması için gerekli docker engine kurulumu sağlandı ve gcr.io repolarına erişim için GCP'de service account oluşturuldu. Github Enterprise 45 complete İsnet üzerindeki Mocha sunucularının Softtech Azure üzerindeki Github uygulamasına erişimi için S2S VPN tanımları tanımlandı. Github Enterprise 46 complete Mocha ekibi kodlarını Github'ta kendi organizasyonları üzerinde yönetimini sağlayabilmeleri için Softtech-Mocha isimli bir organizasyon oluşturuldu. Milvus Vector Database 38 complete Azure üzerinde vector veri tabanı resource group oluşturularak Veri Bilimi ve Yapay Zeka ekibi için Milvus veri tabanı kuruldu. Ekibe erişim verildi. Github Enterprise 34 complete Github enterprise test ortamı 3.8.2 versiyonundan 3.10.2 versiyonuna upgrade edilirken hata alındı. Hata ile ilgili github'a case açıldı. Github Enterprise 35 complete Securtiy ekibinin github runner üzerinde çalıştıracağı actionlar ve maliyetin ilgili ekibe yansıtılması için security ekibe özel azure üzerinde yeni bir runner sunucu kurulumu sağlandı. Github Enterprise 36 complete Github runner üzerinde çalışan actionların checkout adımlarında alınan local issuer certifacate hatası Microsoft ile incelenerek environment tanımı eklenmesi ile çözüm sağlandı. Github Enterprise 30 complete github-test.rally.softtech uygulamasının sertifika güncellenmesi yapıldı. Github Enterprise 28 complete Github Enterprise prod primary sunucu için kaynak artırımı yapıldı ve böylelikle github actions'larda yaşanan sorun çözüldü. MSSQL Server 22 complete Azure Arch servisine ilişkin çalışma çıktıları incelendi. Microsoft ile çalışmanın edip etmeyeceği ile ilgili yol haritası belirlenecek. Github Enterprise 19 complete Github sunucular üzerindeki güvenlik açıkların giderilmesi ile ilgili Github tarafına case açıldı ve Github versiyon güncellemeleri ile ilgili açıkların giderileceği şeklinde çözüm sağlanacağı belirtildi. Github Enterprise 20 complete Github enterprise prod uygulaması 3.8.2 versiyonuna upgrade edildi. MSSQL Server 16 complete GCP ortamındaki TKB SQL Server sunucusunun Azure Arch servisi ile takip edilmesi yönünde Microsoft ile ortak çalışma yapıldı. Github Enterprise 14 complete Github enterprise audit loglara erişim için gerekli token için site admin yetkili devopsekibi kullanıcının basic authentication özelliği aktif edilerek kullanıcı üzerinde token oluşturulup Şaban Ulutaş ile paylaşılacak Github Enterprise 12 complete Github test ortamı 3.8.2 versiyonuna güncellendi. Github.com 10 complete Co-pilot demosu için Github Cloud üzerinde organizasyon oluşturuldu ve Co-pilot yetkilendirme yapısı hazırlandı. Q1 Github Enterprise 4 complete Prod Github enterprise uygulamasının 3.8.0 versiyona upgrade edildi. MSSQL Server 8 complete GCP ortamındaki TKB SQL Server sunucusunun Azure Arch servisi ile takip edilmesi yönünde Microsoft ile çalışma planlandı Q1 MSSQL Server 2 complete Platform ekibimizin yönetiminde olan MSSQL serverların belirlenmesi ve bunların hani ortamda olduğunun bir liste olarak hazırlanması. On-Prem mssql serverlar için (ekte4ki mailde yer alan URL'ler için)outbound network yetkisi istenecek. Liste bu nedenle hazırlanıyor. Azure NAT Gateway 6 incomplete Azure OPlateau netowrk'ten internet erişimi için nat gateway tanımı yapılması DataPlateau 24 complete Data Plateau ürününün banka Openshift ortamına nasıl kurulacağına dair toplantı yapıldı & Tüm ekip Github Enterprise 26 complete Sertifika güncellemesi yapıldı.","{'title': 'Azure Cloud Epic', 'id': '84836930', 'source': 'https://wiki.softtech.com.tr/display/SDO/Azure+Cloud+Epic'}"
"Quarter Başlık Tasks Detaylar Kazanımlar Onboarding 421 complete Onboarding ünlüco ortamındaki jitsi sunucularının sertifikaları güncellenerek erişim sorunu giderildi. Emtia 422 complete Proemtia prod ortamı sürüm çıkışları yapıldı. Mocha DB Sunucusu 418 complete Mocha aracına ait MongoDB veri tabanı backup yapısı oluşturuldu. Restore testleri için otomasyon yapılacak. Mocha DB Sunucusu 416 complete Mocha'nın MongoDB veri tabanlarının backup alımları için backup sunucusu kuruldu. Onboarding 410 complete Onboarding Prod ortamı için zipkin servisi kurulumu yapıldı. Onboarding 411 complete Onboarding Prod ortamı için veritabanı, onboarding servisi ve document servisi için kaynak arttırımları yapıldı. Ayrıca veritabanındaki artan disk kullanımını düzeltmek için audit logları yedeklenip temizlendi. Emtia 412 complete Proemtia prod ortamı sürüm çıkışları yapıldı. Tekcep 413 complete Tekcep UAT ortamında Core ile haberleşme sorunu incelendi, ekibe destek verildi Güvenlik 414 complete Gitlab.softtech sertifikası yenilendi. Emtia 400 complete Proemtia prod ortamı sürüm çıkışları yapıldı. Onboarding 401 complete Onboarding prod uygulumasının veri tabanı sunucusunda kaynak arttırımı yapıldı. Güvenlik 402 complete jenkins.softtech uygulaması güncellenerek güvenlik açıklıkları giderildi. QCloud 403 complete QCloud sym servisinin yeni prod ortamında bağlanacağı Mongodb makinesi oluşturuldu. QCloud 404 complete QCloud bileşenlerinin yeni anthos cluster'ına kurulumu için gerekli deployment dosyaları hazırlandı. Smartfarm 394 complete Smartfarm ürün ekibinden gelen data düzeltme, silme, backup destek talepleri karşılandı. PoB 386 complete PoB uygulamasının Prod ortamının yeni anthos cluster'ında sorunsuz çalışır hale gelmesi için ürün ekibine debug işlemlerini gerçekleştirebilecekleri bir ortam sağlandı. PoB 387 complete PoB uygulamasının Prod ortamının eski anthos cluster'ından yenisine taşınması için gereken solution reposu ve pipeline'ı oluşturuldu. Smartfarm 388 complete Smartfarm Prod ortamındaki UI servisine www.imeceplatform.com ve imeceplatform.com adreslerinden de ulaşılabilecek düzenlemeler yapıldı. Smartfarm 389 complete Smartfarm UAT ve Prod ortamlarının sertifikaları yenilendi. Smartfarm 390 complete Smartfarm UAT ve Prod ortamları için sürüm çıkışları gerçekleştirildi. Emtia 391 complete Emtia UAT ve Prod ortamları için sürüm çıkışları gerçekleştirildi. Onboarding 392 complete Ünlüco Prod ortamındaki Onboarding sertifikaları yenilendi. Onboarding 376 complete Onboarding beta ortamında kullanılan jibri sunucusunda (192.168.99.132) CPU kaynağı arttırıldı (2 core → 4 core) Gitlab 377 complete gitlab.softtech güncellemeleri yapıldı Emtia 378 complete Emtia prod sürüm çıkışları yapıldı. Onboarding 369 complete Ünlüco ortamındaki onboarding projesinin prod sürüm çıkışlarını gitlab pipeline'ı üzerinden yapabilmek için gitlab-runner kurulumu yapıldı. Emtia 370 complete Emtia prod sürüm çıkışları yapıldı. İşfaktoring 371 complete İşfaktoring prod sürüm çıkışı yapıldı. İmeceplatform 372 complete İmeceplatform prod ortamı için TLS sertifikaları yenilendi. Vektör Veri Tabanları. 364 complete Vektör veri tabanı sistemleri ile ilgili çalışmaya devam edildi. GCP üzerinde softtech-vector-database-test isimli proje açıldı. Sunucu Kurulumları 356 complete Onboarding projesi için ubuntu 22.04 jitsi ve jibri sunucu kurumları yapıldı. Güvenlik 362 complete jenkins.softtech sunucusuna wazuh agent kurulumu yapıldı. Güenlik 357 complete jenkins.softtech upgrade çalışması yapıldı. Bakım 358 complete jenkins.rally.softtech disk temizliği yapıldı. Ünlüco 359 complete Ünlüco ortamında onboarding ekibi prod sürümlerini mevcut hattı kullanarak deploy edememektedir. Çalışır durumda teslim edilen hat, daha önceki ekipler tarafından ihmal edilmiş ve sürümler manuel olarak çıkıldığından atıl kalmıştır. Yeni ekibin bu yapıdan habersiz olduğu farkedilmiş ve hattı kullanmaya çalıştıklarında ise başarılı olamadığı görülmüştür. Bu yüzden prod sürümü çıkışlarını manuel olarak yapmaya devam etmek zorunda kalmışlardır. Bu noktada eski ekibin devops işlerini yürüten ve konuya hakim olanlardan destek alarak prod hattını, repolarını ve hattı işletecek olan gitlab-runner'larını güncellemeleri gerekiyor. Bu çalışma henüz yapılmadığı için Ünlüco sürüm çıkışını gerçekleştiremedi ve desteğimizi talep etti. Biz de gece ekip ile bir toplantı yapıp manuel olarak sürüm çıkışlarını gerçekleştirerek müşterinin problemini çözdük. Anthos Sürüm Çıkışları 360 complete Proemtia preprod ve prod sürüm çıkışları yapıldı. Vektör Veri Tabanları 349 complete Vektör veri tabanı sistemleri ile ilgili çalışmaya devam edildi. Bir sonraki hafta ekiplerle ortak çalışma yapılarak ürün belirlenmesi kararlaştırılacak. Eğitim 350 complete Softtech& GitHub Admin & Co-Pilot Workshop eğitimleri alındı. Anthos Cluster 343 complete Eski anthos cluster'ının dolan iç sertifikalarının yenilenmesi yapıldı. Eğitim 344 complete Softtech& MS GitHub Admin Workshop eğitimlerine katılım sağlandı. Sunucu kurulumları 345 complete Onboarding projesi için 4 adet Jitsi sunucusu ubuntu 22.04 olarak tekrar kuruldu ve ekibe teslim edildi. Anthos Sürüm Çıkışları 346 complete Emtia preprod ve prod, Tekcep Prod sürüm çıkışları gerçekleştirildi Anthos Sürüm Çıkışları 336 complete Emtia preprod ve prod, PoB preprod, Digital Onboarding UAT sürüm çıkışları gerçekleştirildi. Vektör Veri Tabanları 337 complete Milvus, Qdrant ve Weaviate vektör veri tabanı ürünleri üzerinde deneme çalışmaları ( performans, kullanılabilirlik ) yapıldı. Bir sonraki hafta çalışmaya devam edilecek. İşNet Yükle&Gelsin 338 complete Yükle & Gelsin projesi kapsamında Prod PostgreSQL sunucu kurulumu yapıldı Mocha 331 complete Mocha sunucusu için MongoDB kurulumu yapıldı. Vector Database Sunumu 332 complete Vektör veri tabanı sistemleri ile ilgili FriAIDay haftalık toplantısında sunum yapıldı. Problem Giderme Çalışmaları 324 complete Onboarding projesinin notification ile PoB projesinin pobnotification servisindeki sorunların giderilmesi için çalışmalar gerçekleştirildi. Anthos Pipeline 325 complete PoB uygulaması UAT sürüm çıkışları, uygulama sahibi ekip tarafından bir yetkilinin onayı sonrası otomatik olacak hale getirildi. Anthos Pipeline 326 complete Anthos Pipeline üzerinde yapılan iyileştirme ile Emtia projesinin qui üzerine eklediği nginx config dosyasının hat deteği sağlandı. Ünlüco 327 complete Ünlüco sunucularında kurulu olan Microk8s cluster'ında süresi dolan server.crt ve front-proxy-client.crt sertifikaları yenilenerek kubectl erişim problemi düzeltildi. Anthos Sürüm Çıkışları 328 complete Anthos Preprod ve Prod ortamlarına 360Survey, PoB, Emtia, Smartfarm, Onboarding, Tekcep uygulamalarının yeni sürümleri çıkıldı. İşfaktoring 318 complete İsfaktoring Prod Anthos ortamında notification mikroservisinde alınan OTP gönderme sorunu incelendi ve Liveness/Readiness probeların zamanında ayağa kalkmadığı gözlemlendi. Notification ms 'de kaynak artırımı yapılarak çözüm sağlandı. Emtia & Jira 316 complete Emtia ve Jira prod veri tabanı ortamları için yazılım ekiplerinden gelen destek talepleri karşılandı. 360survey 312 complete 360survey uygulaması UAT sürüm çıkışları, uygulama sahibi ekip tarafından bir yetkilinin onayı sonrası otomatik olacak hale getirildi. Anthos Pipeline 313 complete Onprem Anthos ortamına sürüm çıkışı yapılırken kullanılan pipeline üzerinde iyileştirme yapıldı. Manuel yapılan operasyonlar için solution reposu üzerine koyulan patch klasörünün pipeline tarafından çalıştırılması düzenlemesi yapıldı. Anthos Sürüm Çıkışları 314 complete Onprem Anthos üzerinde 360survey ve Smartfarm uygulamalarının Preprod ve Prod ortamlarına sürüm çıkışları gerçekleştirildi İş Faktoring Tedarikçi & Emtia & Smartfarm 306 complete İş faktoring tedarikçi finansmanı, Smartfarm ve Emtia veri tabanı sunucularının 3 saat zaman farkı ( UTC olarak set edilmiş durumda ) bulunmakta. Saat farkının giderilmesi için time-zone ayarlamasının yapılması için ekiplerle toplantı planlandı. Iftedarikçi & Emtia 307 complete Tedarikçi finansmanı ve emtia projesine ait veri tabanı ortamları için kullanıcı yetkilendime ve silme talepleri karşılandı. Vector Databases 308 complete Vektör veri tabanları ile ilgili araştırmalara devam edildi. Konuyla ilgili ekip içerisinde sunum yapıldı. Qdrant, Milvus gibi ürünlerin denemesi yapılarak alternatif ürünlerin seçilmesi ve Cuma günleri yapılan yapay zeka toplantılarında da ekiplere bu sunumun yapılmasına karar verildi. 302 complete Anthos common cluster üzerinde kubernetes dashboard kurulum ve yetkilendirmeler sağlandı. PoB 298 complete PoB uygulaması için solution reposu ve pipeline hazırlndı PoB 299 complete PoB uygulamasının yeni Beta cluster'ına kurulumu gerçekleştirildi. Anthos Sürüm Çıkışları 300 complete İşfaktoring ve Emtia uygulamalarının Prod sürüm çıkışları yapıldı Emtia 291 complete Emtia ortamında müşteri tarafından istenen audit yapısı için Mimari ekiple görüşme sağlandı. Gelen önerilere göre mevcut yapı üzerinde düzenlemeler yapılacak. Emtia 292 complete Yazılım ekibi ile Data Plateau uygulaması üzerinden çalıştırılan raporların performansı konusunda görüşme yapıldı. Raporlama ekibinden de görüş almak için yeni bir planlama yapılacak. Emtia 293 complete Yazılım ekibi tarafında yanlışlıkla güncellenen verilerin kurtarılması konusunda destek sağlandı. Backup restore edilerek, veriler kurtarıldı. Tedarikçi Finansmanı 294 complete Tedarikçi Finansmanı projesi kapsamında cluster yapısında prod PostgreSQL veri tabanı ortamı kuruldu, ekibe erişim konusunda destek sağlandı. İşfaktoring tedarikçi 286 complete İşfaktoring Tedarikçi prod cluster ortamı için prometheus, grafana, alertmanager ve kubedashboard gibi kubernetes bileşenlerinin kurulumları yapıldı. İşfaktoring tedarikçi 284 complete İşfaktoring Tedarikçi prod ortamı için FTP ve SFTP uygulama kurulumları yapıldı. Smartfarm 275 complete Smartfarm Preprod ve Prod sürüm çıkışları yapıldı. 360Survey 276 complete 360Survey Prod ve Preprod sürüm çıkışları yapıldı. Security 277 complete Jenkins.softtech 2.392 versiyonundan 2.418 versiyonuna güncellendi ve meet.onplateau.com sertifika'sı chain sertifika iledeğiştirildi. Iftedarikçi 278 complete Iftedarikçi Prod konfigürasyonu yapıldı ve kurulum gerçekleştirildi, kurulum esnasında oluşan hataların giderilmesi çalışması yapıldı Iftedarikci 279 complete Iftedarikci Prod solution reposu ve deployment pipeline'ı hazırlandı Iftedarikci 280 complete Iftedarikci için Database sunucuları ve FTP sunucusu kuruldu Iftedarikci 281 complete Iftedarikci Prod cluster üzerine; elastic stack, nfs storageclass, ingress nginx kurulumları yapıldı Iftedarikci 282 complete Iftedarikci Prod cluster kurulumu yapıldı Onboarding 259 complete ui.onboardinguat.onplateau.com SSL Sertifikası yenilemesi yapıldı İşfactoring 260 complete İşfactoring için Anthos Prod cluster'ı kuruldu Emtia 261 complete Emtia Prod ve Preprod sürüm çıkışları yapıldı 360survey 262 complete 360survey Preprod ve Prod sürüm çıkışları yapıldı. Emtia 266 complete Audit loglarının analizi için ürün araştırması yapıldı ( ELK vs Loki ) Emtia Dataplateau 263 complete Emtia ekibinden dataplateau aracına ait kullanıcı yetkilerinde düzenleme yapıldı. Bu uygulamanın çalışma yapısı ve oluşabilecek sorunlara ilişkin ekiple toplantı planlandı. Vector DBs 264 complete Vector veri tabanları ile ilgili araştırma yapıldı. Araştırma ağırlıklı olarak PostgreSQL'in extensionları olan pgvector ve pgvecto.rs üzerinde yapıldı. PoB 252 complete Pob servisin servicegatewayvpn.isbank.com.tr adresine giderken yaşanan erişim problemi çözüldü. Emtia DB Prod 249 complete Emtia prod ortamında yazılım ekibine, müşterinin  iletmiş olduğu Jira kayıtları için destek sağlandı. Emtia DB Prod 250 complete DB audit yapısıyla ilgili müşteriye sunum yapıldı. Audit kayıtlarının damgalanması ve barındırılması süreçleri ele alındı. Çalışma fazlanarak devam edecek. 360survey 246 complete 360survey Preprod ve Prod sürüm çıkışları yapıldı. Onboarding 241 complete Onboarding UAT ortamı için UI servisine ssl sertifika tanımı yapıldı Qcloud 242 complete studio.onplateau.com altında bulunan docusaurus doküman servisi için docs.onplateau.com adres tanımı yapıldı Smartfarm 243 complete Smartfarm Prod ve Preprod sürüm çıkışları yapıldı Emtia 244 complete Emtia Prod ve Preprod sürüm çıkışları yapıldı Emtia DB Prod 235 complete Emtia prod dataplateau ortamındaki erişim sorunu düzeltildi 360survey 236 complete Yazılım ekiplerden gelen MySQL ortamları için destek talepleri karşılandı Emtia DB Prod 230 complete Emtia prod veri tabanında data silinme ihtimaline karşılık, yazılım ekibine data kurtarma süreçlerine audit logları ve backuplar restore edilerek destek verildi. Emtia DB Prod 231 complete Emtia veri tabanında müşteri tarafından gelen Jira talebi karşılandı. Emtia DB Prod 232 complete Emtia prod db için audit amacı ile kurulan pg_audit yapısıyla ilgili yazılım ekibine aktarım, sunum yapıldı. PoB 224 complete PoB Beta ve Prod ortamlarında sertifika güncellenmesi yapıldı 360survey 225 complete 360survey Preprod ve Prod sürüm çıkışları yapıldı Smartfarm 226 complete Smartfarm Prerprod ve Prod ortamlarına sürüm çıkışı işlemleri yapıldı Emtia 217 complete Emtia db prod ortamı için belirlenen fafmarket şeması altındaki 12 tablo için audit yapısı oluşturuldu 360Survey 218 complete 360survey Preprod ve Prod sürüm çıkışları yapıldı Emtia 219 complete Emtia Preprod ve Prod ortamlarına sürüm çıkışı yapıldı Smartfarm 220 complete Smartfarm Prerprod ve Prod ortamlarına sürüm çıkışı işlemleri yapıldı Smartfarm 211 complete Smartfarm Preprod ve Prod ortamlarına sürüm çıkış işlemleri yapıldı. Emtia 212 complete Emtia Preprod ve Prod ortamlarına sürüm çıkış işlemleri yapıldı Emtia 206 complete Hafta içinde bir kaç kez preprod/prod sürüm çıkış işlemleri yapıldı İşfaktoring 208 complete Anthos İşfaktoring Tedarikçi Finansmanı uygulamasının eposta atması için işfaktoring ekipleri ile ortak çalışma ile problem giderildi. Emtia 197 complete Emtia veri tabanı ortamı için gelen read / write yetki talepleri karşılandı Veri Tabanı Dökümanı 198 complete PgBouncer ve PostgreSQL minör sürüm güncelleme dökümanı hazırlandı QCloud 203 complete QCloud uygulamaları: entity-designer, plateau-hub, plateau-ui-docs, process-designer, process-designer-diff-viewer, process-manager, provider-designer-provider, qcloud-exporter, qcloud-qui, qcloud-quick, qcloud-stechauth, qcloud-sym, websocketrelayserver yeni anthos üzerinde kuruldu İşfaktoring 204 complete İşFaktoring Tedarikçi uygulaması preprod sürüm çıkışları yapıldı 360survey 199 complete 360survey Preprod sürüm çıkışları yapıldı Proemtia 200 complete Proemtia Preprod ve Prod sürüm çıkışları yapıldı İşfaktoring Tedarikçi Finansmanı 190 complete İşfaktoring Tedarikçi Beta cluster ortamında kubedashboard kurulumu tamamlandı. Qcloud 191 complete Eski anthos üzerinde bulunan Qcloud uygulamasının yeni anthos ortamına taşınması için gerekli MongoDB sunucusunun VCenter üzerinde kurulumu tamamlandı. Smartfarm 192 complete Prod smartfarm cluster üzerinde eski versiyonlu grafana ve kibana, elasticsearch uygulamaları upgrade edildi. Proemtia 186 complete Proemtia Preprod ve Prod sürüm çıkışları yapıldı Ünlüco 183 complete Ünlüco ortamındaki onboarding uygulamasının sorunlarının giderilmesiyle ilgili çalışmalar yapıldı İşfaktoring Tedarikçi Finansmanı 184 complete İşfaktoring Tedarikçi Beta yeni sürüm çıkışları yapıldı, hata gidermeler yapıldı QCloud 179 complete QCloud projesi kapsamında MongoDB veri tabanı sunucu kurulumu yapıldı. Emtia 180 complete Emtia'nın PROD ortamında hem PgBouncer majör , PostgreSQL minör sürüm yüksetlme çalışması yapıldı. Emtia 170 complete Emtia veri tabanı ortamı için gelen destek talepleri karşılandı Tedarikci Finansmanı 171 complete Veri tabanları ve kullanıcıların açılması, yetkilendirilmesi yönünde tedarikçi finansmanı ekibine destek verildi. Smartfarm & Emtia 172 complete Smartfarm UAT ve PROD ortamlarında ve Emtia'nın UAT ortamında hem PgBouncer majör , PostgreSQL minör sürüm yüksetlme çalışması yapıldı. Onboarding 164 complete Ünlüco tarafında kurulu olan onboarding uygulamasının pipeline'ı düzeltildi. Proemtia 165 complete Proemtia Preprod ve Prod sürüm çıkışları ve hata gidermeleri yapıldı İşfaktoring Tedarikçi Beta 166 complete İşfaktoring Tedarikçi Beta deployment'ı gerçekleştirildi. Smartfarm 159 complete SmartFarm PROD için Keycloak DB ui admin kullanıcısı default keycloack chart yerine şifrelenmiş credentials dosyasından okundu. Smartfarm PostgreSQL 160 complete Smartfarm Prod ortamda deployment için kullanılan admin kullanıcısının deployment haricinde kapalı kalabilmesi için çalışma yapıldı. Testler sonrasında kullanıcı kapatılacak. PostgreSQL Log Management System 156 complete PostgreSQL log management system kurulum çalışması kapsamında Loki incelemesi yapıldı. Smartfarm PostgreSQL 154 complete Smartfarm prod veri tabanı sunucusunda data plateau uygulamasında yaşanan problemlere ilişkin destek verildi. İşfaktoring Tedarikçi Beta 151 complete İşFaktoring Tedarikçi finasman beta cluster için grafana, prometheus, alert manager kurulumları tamamlandı. İşfaktoring Tedarikçi Beta 152 complete İşFaktoring Tedarikçi finasman için ftp ve sftp sunucu uygulama kurulumları tamamlandı ve kurulumlar wiki üzerinde dökümante edildi. İşfaktoring Tedarikçi Beta 139 complete İşFaktoring Tedarikçi finasman beta Anthos cluster kurulumu tamamlandı İşfaktoring Tedarikçi Beta 140 complete İşFaktoring Tedarikçi finasman beta NFS sunucusu kuruldu İşfaktoring Tedarikçi Beta 141 complete İşFaktoring Tedarikçi finasman beta Ingress nginx kuruldu ve loadbalancer ip'si için dış ip tanımı yaptırıldı İşfaktoring Tedarikçi Beta 142 complete İşFaktoring Tedarikçi finasman beta Elasticsearch & Kibana kurulumları yapıldı Smartfarm 143 complete SmartFarm için Keycloak DB ui admin kullanıcısı default keycloack chart yerine şifrelenmiş credentials dosyasından okundu. Smartfarm 144 complete Smartfarm Preprod Sürüm çıkışları yapıldı Emtia & Smartfarm 129 complete Emtia ve Smartfarm veri tabanı sunucularına deployment için kullanılan veri tabanı kullanıcısı, keycloack üzerinde de tanımlıydı. Keycloack'taki tanımının düzenlenmesi ile ilgili çalışma yapıldı. Uat ortamındaki testler sonrasında prod ortamlar için de benzer çalışma yapılacak. Smartfarm 130 complete Smartfarm ve diğer veri tabanı ortamlarına yönelik gelen kullanıcı destek talepleri karşılandı. İşFaktoring Tedarikçi Finansmanı 131 complete Proje kapsamında ihtiyaç duyulan PostgreSQL veri tabanı cluster kurulumu yapıldı. Emtia & Smartfarm 132 complete Veri tabanı ortamları için auditing loglarının saklanması için log management system araştırması yapılmakta. Open Source sistem olarak Grafana Loki incelemesi devam etmekte. Smartfarm 121 complete GCP ortamında bulunan smartfarm demo ortamındaki problemler giderildi Anthos Sürüm Çıkışları 122 complete Emtia ve Smartfarm sürüm çıkışları yapıldı Jira UAT 124 complete Jira UAT ortamındaki veri tabanına ilişkin sorunların giderilmesi için destek verildi. Emtia & Smartfarm 118 complete Emtia & SmartFarm veri tabanı ortamları için auditing log yapısının oluşturulması ile ilgili hazırlık çalışması tamamlandı. Hafta içerisinde emtia prep ortamında kurulum çalışması yapılacak, ardından diğer ortamlar için planlama yapılarak denetleme yapısı oluşturulacak. Emtia & Smartfarm 114 complete Emtia & SmartFarm veri tabanı ortamları için auditing log yapısının oluşturulması ile ilgili hazırlık çalışması yapıldı Sunucu Teslimi 115 complete İmeceplatform beta ve prod için dataplateau sunucusu teslim edildi Anthos Sürüm Çıkışları 116 complete Emtia ve Smartfarm sürüm çıkışları yapıldı Anthos Sürüm Çıkışları 110 complete Emtia ve 360Survey uygulamarının PROD & UAT ortamlarına sürüm çıkışları gerçekleştirildi. Pob Database 108 complete Pob database yedekleme betiğinde düzenleme yapıldı, yedeklemeler iyileştirildi. 360Survey 101 complete 360Survey Prod için veritabanı sunucuları hazırlandı 360Survey 102 complete 360Survey Prod için solution reposu ve pipeline oluşturuldu. Prod çıkışı tamamlandı Anthos Cluster 103 complete Emtia Prod Cluster worker node arttırımı yapıldı Emtia 106 complete Emtia uygulamasının performans testleri ile birlikte kaynak request ve limitlerinde güncelleme çalışması yapılarak performansı arttırıldı Emtia 104 complete Preprod ve Prod Sürüm çıkışları yapıldı Common Cluster 96 complete Anthos common cluster üzerine prometheus, alertmanager ve grafana uygulama kurulumu sağlandı. 360survey 94 complete 360survey prod db kurulumu yapıldı Emtia & SmartFarm 92 incomplete Emtia ve Smartfarm DB sunucusunda deployment için kullanılan admin yetkisine sahip kullanıcıların disable edilmesi çalışması yapıldı. Bu kapsamda hat ekibinden ilgili helm chart içerisinde düzeltilme yapılması bekleniyor. Düzenleme sonrasında admin kullanıcısı disable edilecek Anthos Uygulama Sorunları 90 complete POB uygulamasına ait Pobtransaction mikroservisinde yaşanan problemlerin giderilmesi için çalışma gerçekleştirildi. Anthos Sürüm Çıkışları 88 complete Emtia Smartfarm 360Survey AHE uygulamarının PROD ve UAT ortamlarına sürüm çıkışları gerçekleştirildi. 360survey & Emtia 73 complete 360survey ve emtia projeleri kapsamında gelen db destek talepleri karşılandı Smartfarm & Emtia 74 complete Smartfarm ve Emtia db cluster ortamındaki admin kullanıcısının yetkisinin alması çalışması yapıldı. Çalışma henüz devam ediyor. Smartfarm Preprod Prod 81 complete Smartfarm Preprod Prod ortamlarına sürüm çıkışları Emtia Preprod Prod 82 complete Emtia Preprod Prod ortamlarına sürüm çıkışlarıw 360Survey Preprod 83 complete 360Survey Preprod ortamına sürüm çıkışı Smartfarm Dataplateau VM 84 complete Smartfarm projesinde ihtiyaç duyulan dataplateau VM'inin oluşturulması ve teslimi Emtia Jmeter VM 85 complete Emtia projesi için Jmeter kurulu linux sunucu oluşturulması ve teslimi AHE UAT 86 complete Ahe UAT ortamının tekrar aktif edilmesi Emtia Preprod / Prod 69 complete Emtia prep ve prod ortamlarında talep edilen kullanıcı yetkileri tanımlandı ve destek talepleri karşılandı. 360Survey 70 complete 360Survey veri tabanı sunucu servisi olarak MariaDB 10.11.2 versiyonu kuruldu ve beta ortamında hizmete alındı. Quick Editor 62 complete Quick editor Pipeline Prod Deployment Sorununun giderilmesi İşnet Network 63 complete Onboarding network sorununun giderilmesi için destek AHE 64 complete AHE Prod ve Preprod sürüm çıkışları 360Survey 65 complete 360survey DB sunucuları kurulması 360Survey 66 complete 360survey preprod kurulum ve hat çalışmaları Emtia 56 complete Emtia prod ve bet kubernetes cluster üzerinde Zabbix agent kurulumu ve entegrasyonun yapılması Pazarama Alert ve Performans 53 incomplete Pazarama ekibine kendi uygulamalarına ilişkin uyarıların Zabbix üzerinde ulaştırılması 54 complete Pazarama ekibine Prod ortamı Kibana ve Grafana erişim bilgilerinin paylaştırılması Emtia Sürümleri 48 complete Emtia Preprod ve Prod sürüm çıkışları Smartfarm 49 complete Smartfarm Preprod ve Prod sürüm çıkışları AHE Anthos 50 complete AHE Prod Sürüm çıkışı Emtia Preprod & Prod 43 complete Emtia PostgreSQL cluster ortamları için gelen kullanıcı talepleri karşılanarak destek verildi. 44 complete HR360 uygulamasının kurulması için gerekli çalışmaların yapılması. Emtia / SmartFarm 40 incomplete Veri tabanı ortamlarında kullanıcı read ihtiyaçlarını cover edecek uygulama geliştirilecek MongoDB 38 complete MongoDB için continuous archiving yapısı oluşturulacak Emtia Prod 36 incomplete sartuk.palamut kullanıcısına Emtia Prod db sunucusunda geçici süreliğine dml ve select yetkileri verildi. Yetkiler geri alınacak SmartFarm 34 complete Smart Farm PreProd ve Prod ortamları keycloack versiyon 20'ye geçirildi ve hat buna uygun olarak güncellendi. Veritabanı preprod ve prod ordamında mevcut şemalar droplandı ve yeniden ilgili şemalar oluşturuldu ve kullanıcı yetkileri tanımlandı. Sonrasında uygulama kurulumu gerçekleştirildi. Q1 Studio Onplateau 32 complete Studio Onplateau için MongoDB kurulumu yapıldı Q1 Emtia 30 complete Emtia prod ortamı için PostgreSQL cluster kurulumu tamamlandı. Q4 Anthos 28 complete QCloud, onboarding ve pob uygulamaları yeni anthos cluster'a taşındıktan sonra 192.168.104.226 Jenkins sunucusunun kaldırılması ve github.com dan gelen deployment ayağını nasıl kırabiliriz ? Q1 Anthos 17 complete Jenkins Anthos sunucusu üzerinde AHE, Emtia ve Smartfarm uygulamalarının deploymentları için pipeline süreçlerinin oluşturulması Q1 Upgrade 18 complete Anthos gitlab uygulama versiyonun güncellenmesi Q1 AHE 19 complete Parametrik değerlerin environment olarak Azure Devops repoya API üzerinden deploy olması için postman request'inin hazırlanması Q1 AHE 20 complete Uygulamanın Anthos’tan AHE’nin kendi ortamına (Openshift platforma) taşınması için AHE'nin Azure Devops platform üzerinde prod, test ve uat clusterlar için solution reposu oluşturulması Q1 AHE 21 complete AHE için geliştirilen uygulamanın müşteri ortamındaki Openshift cluster üzerinde ayağa kalkması için Softtech Artemis Openshift ortamında kurulumun test edilmesi ve çıkan hataların giderilmesi Q1 Studio onplateau 22 incomplete DB ve minio uygulamalarının connection bilgilerinin secret environment olarak alınması için bu bilgileri kullanan servislerin yeni imageların deploy edilmesi Q1 Studio onplateau 23 complete Yeni anthos ortamı için Mongo DB ve Minio VM Instance sunucularının oluşturulması ve MongoDB, minio uygulama kurulumu Q1 Studio onplateau 24 complete Eski anthos clusterından yeni anthos cluster'ına taşınması için uygulama kubernetes objelerinin deployment'ı için yaml dosyalarının gitlab reposunda oluşturulması Q1 Emtia 25 complete Proemtia prod uygulama kurulumu Q1 Emtia 2 complete Emtia preprod kurulumu ve veri tabanı kontrolleri için Emtia ekibine destek sağlanması Q1 Anthos Alternatifleri 26 complete GlassHouse'a Gereksinimler İletilecek, GlassHouse'tan İşNet'e alternatif teklif istenecek","{'title': 'Anthos Epic', 'id': '84836934', 'source': 'https://wiki.softtech.com.tr/display/SDO/Anthos+Epic'}"
"Quarter Başlık Tasks Detaylar Kazanımlar İşfaktoring 30 complete İşfaktoring worker sunucuların birinde (isfakwrkp1) yaşanan kubelet sorunundan dolayı sunucuya API ile erişim sağlanamıyordu. Sorun İşnet ile beraber incelenerek İşnet'in müdahalesi ile çözüm sağlandı. İşFaktoring 28 complete İşfaktoring'e sağlanan dijital kanallar için müşterinin on-premise prod ortamı için pipeline kurulumu sağlandı. Q2 İşFaktoring 26 complete İşfaktoring uat müşteri ortamı için sürüm çıkışı sağlandı. Q1 AHE 2 incomplete AHE uygulamasının Anthos ortamından Openshift ortamına taşınması sırasında Mart ayı içinde sarf edilen eforun AHE ekibine iletilerek teyit alınması. Sonrasında proforma hazırlanacak. İşFaktoring 4 complete İşfaktoring İşnet Kubernetes ortamlarının Deployment ortamları (Gitla, build image, pipeline vb.) kurulumu İşFaktoring 6 complete İşfaktoring İşnet Kubernetes ortamlarında Docker registry ayarlarını düzeltilmesi için İşnet ile birlikte lcuster config değişikliği yapılması. İşFaktoring 8 complete İşfaktoring faktoring uygulamasının uygulama ayarları ve kontrolleri yapıldı. Ekip tarafından karar verilmesi/değiştirilmesi/satın alınması gereken maddeler belirlenip raporlandı Qpay 10 complete Qpay'in denetim öncesi danışman firma ile birlikte ön denetime girilerek destek verildi. Qpay 12 complete Qpay'in uygulama loglama altyapısı ve performans takibi ile ilgili destek verildi İşFaktoring 14 complete İşfaktoring Dijital Kanallar uygulaması UAT kurulumu ve hat kurulumu müşteri ortamında tamamlandı. Uygulama hataları ve entegrasyonlar ile ilgili destek verilmeye devem ediliyor. İşFaktoring 16 complete İşfaktoring müşteri ortamlarına cluster log ve monitörleme bileşenleri kuruldu İşFaktoring 20 complete İşfaktoring müşteri ortamına bir kaç kez uat sürüm çıkışı yapıldı İşFaktoring 21 complete İşfaktoring müşteri ortamındaki notification başlangıç problemi incelendi ve ekiplerle sonuçlandırıldı. İşFaktoring 22 complete İşfaktoring müşteri ortamına sürüm çıkışlarıo için ürün ekibinin vpn'leri tamamlandı. Gitlab üzerinde ekip üyelerinin yetkilendirmeleri yapıldı ve ekibe sürüm akışı için bilgi verildi. Qpay 24 complete Qpay'e dış denetimler kapsamında destek verildi.","{'title': 'Customers Epic', 'id': '84836936', 'source': 'https://wiki.softtech.com.tr/display/SDO/Customers+Epic'}"
"Quarter Başlık Tasks Detaylar Kazanımlar Wiki UAT Prod Upgrade 79 complete wiki.softtech.com.tr confluence uygulamasında açığa çıkan kritik güvenlik bulgusu nedeni ile UAT ve Prod ortamları 7.19.16 versiyonuna upgrade edildi. Test Yönetim Aracı Mocha 77 complete Test Yönetim aracı mocha'nın kurulum çalışmaları kapsamında aşağıdaki konularda ekiplere destek verildi. 1. side to side vpn oluşturularak işnet sunucularından github.rally.softtech adresine erişim sağlandı. 2. mocha dns'lerinin sertifikaları alındı ve yüklendi. 3. Sunucularda local user olarak çalışacak, oturumu sürekli açık duracak ""mochaAppUser"" user'ı tanımlandı. 4.Testlerin çalışacağı sunucularda Node.js ve gerekli library'lerin kurulumları yapıldı. Wiki UAT Prod Upgrade 75 complete wiki.softtech.com.tr adresinde çıkan güvenlik bulgusunun giderilmesi için uat ve prod confluence uygulaması 7.19.15 versiyonuna upgrade edildi. Test Yönetim Aracı Mocha 73 complete Banka tarafındaki ekiple kurulum çalışmaları gerçekleştirildi. Scoretfs agent'ları kuruldu ve kontroller sağlandı. IIS, postman, edge gibi diğer kurulması gerekenler de diğer sunuculara kuruldu. Çalışmalar Deployment testleri ile devam edecek. Test Yönetim Aracı Mocha 71 complete Banka tarafındaki sunuculara scoretfs agent'ının kurulumu yapılacağı için banka tarafındaki ilgili ekibe bilgi verildi ve kurulum için çalışma planlandı. Test Yönetim Aracı Mocha 69 complete Test yönetim aracı mocha'nın kurulum çalışmaları için banka tarafındaki sunucuların scoretfs sunucusuna erişimleri sağlandı. Bu erişimler için kontroller sağlandı ve bir sorun olmadığı görüldü. Test Yönetim Aracı Mocha 67 complete Test yönetim aracı mocha'nın kurulum çalışmaları için banka tarafındaki sunuculara citrix üzerinden RDP yapabilmek için gerekli yetkiler verildi. Sunuculara RDP ve SSH yapılabildiği kontrol edildi. Test Yönetim Aracı Mocha 65 complete Test yönetim aracı mocha'nın kurulum çalışmaları için banka tarafındaki sunuculara citrix üzerinden RDP yapabilmek için gerekli yetkilerin talebi yapıldı. Q2 Jira-Wiki Prod Upgrade 63 complete Prod jira ve confluence uygulamaları upgrade edildi. Q2 Jira-Wiki Prod Upgrade 61 complete Prod jira ve confluence uygulama upgradeleri için sunucu üzerindeki backup yapısı incelendi ve confluence için script düzenlendi. Ek olarak upgrade çalışması için işnetten disk talebinde bulunuldu ve çalışma 1 Ağustos 2023 tarihi için planlandı. EpayKolay IAM 502 58 complete Banka tarafında EpayKolay kapsamındaki IAM servisinde alınan 502 hatasına destek verildi Jira-Wiki UAT Upgrade 59 complete Jira ve Wiki (Confluence) UAT ortamlarının upgrade'i ekiple birlikte gece operasyonunda yapıldı. Database Support 177 complete Mimari Coe ve diğer yazılım ekiplerinden gelen SQL Server ve PostgreSQL veri tabanı ortamları için erişim sorunları vb. gibi destek talepleri karşılandı. İşfaktoring Tedarikçi Finansmanı 51 complete Tedarikçi Finansmanı ekibinden gelen veri tabanı destek talepleri, problemler çözüldü. Ek olarak UAT ortamında ekipten 4 kişiye geçici süreliğine read yetkilerine ek olarak write yetkileri tanımlandı. Q1 Chatbot 27 complete Chatbot uygulaması için Platform ekibine sıklıkla gelen soru ve cevapların oluşturulması için destek sağlandı. Plateau 14 complete Plateau hizalanma toplantısındaki taskların kontrol edilerek, biz sonraki hizalanma toplantısına kadar taskların tamamlanması. 21.02.2023 - Plateau Hizalanma Toplantısı 1 - Strateji ve Ürün Yönetimi - Confluence (softtech.com.tr) 12 complete gcr.io yazılımcı erişimi ve türis fiziksel sunucuları konularını CAB gündemine getir. 10 complete Backstage Kuberntes entegrasyonu için gerekli tanımlamaların yapılması ve mimari ekiple paylaşılması. Support 8 complete AHE, PoB, Unluco ve Onboarding projelerine destek verildi Q1 Cloud 6 complete Anthos/Gcloud alternatifi olarak Huawei Cloud ile görüşme yapıldı. İş birliği fırsatları değerlendirildi. Q1 Hat Yaygınlaştırma 15 incomplete Plateau, Data Plateau, Conversational AI, Quick (studio.onplateau) ekiplerinin devops ve platform alt yapılarını daha etkin kullanabilmesi için bu ekiplerle haftalık düzenli toplantılar organize edilmesi. Q1 Banka Kodları 4 complete Banka kodlarının Cloud ortamında durması yönünde banka güvenlik endişelerini giderecek çalışma yapılması. Q1 Kod Senkronizasyonu 2 complete Plateau Core kodlarının bankaya senkronize edilmesi ve yapılan çalışmaların dokümante edilmesi. Plateau Core Kod Senkronizasyonu - Softtech DevOps - Confluence Jira UAT Update 20 complete Jira UAT için datacenter lisans geçişinin yapılması UAT'de Xray plugin lisans hatası veriyor güncelleme sonrası, bunu ekip inceliyor. Jira Prod Update 21 complete Jira Prod için datacenter geçişinin yapılması UAT'deki Xray sorunu çözülene kadar beklemedem. Confluence UAT Update 22 complete Confluence (wiki) uat için datacenter geçişinin yapılması 8090 portuna citrix üzerinden erişim sorunu olduğu için beklemede. Confluence Prod Update 23 complete Confluence (wiki.softtech.com.tr) için datacenter lisans geçişinin yapılması Ekibin kararı ile uat yapılmadan prod yapıldı. Q1 Kod Senkronizasyonu 25 incomplete Plateau Core kodlarından ' Plateau IAM Providers ' projesinin banka tarafına aktarılması ve çalışmaların dökümante edilmesi. Plateau Core Kod Senkronizasyonu - Softtech DevOps - Confluence Jira DB Dump Backup 30 complete Sunucu üzerinde son 5 günlük db dump'ının tutulması için script hazırlanması ve devreye alınması Confluence DB Dump Backup 31 complete Sunucu üzerinde son 5 günlük db dump'ının tutulması için script hazırlanması ve devreye alınması Jira Attachment Backup 33 complete Confluence(Wiki) üzerinde olduğu gibi rsync ve lvm snapshot ile diff backup alacak yapı kuruldu. Böylece upgrade çalışmalarında uzun süren file backup süreci kısaltıldı. İşbank NAYS 37 complete NAYS projesi kapsamında iam ürünün iki DC arasında aktif aktif çalıştırılabilmesi için Banka ve Softtech Ekiplerine destek Bu operasyon bir kaç hafta devam edecek. BDDK Genelge 39 complete BDDK Genelgesi kapsamında kimlik doğrulama ile ilgili yapılacak değişiklikler kapsamında tasarım operasyonlarına katıllım sağlandı Bu toplantılar bir süre devam edecek Kod Senkronizasyonu 41 complete Plateau Core kodlarının bankaya senkronize edilmesi projesi kapsamında ortaya çıkan problemlin nedenin tespiti için banka tarafındaki antivirüsün kapatılıp test yapılması. Plateau Core Kod Senkronizasyonu - Softtech DevOps - Confluence Google AI Çalışmaları 43 complete Google üzerinde proje yapısının BT Destek ile tasarlanıp yaratılması ve açılan projeler üzerinde direktörlük çalışanlarına yetkilendirmelerin yapılması BDDK Genelge 45 complete Softtech ürün mimarları ile mTLS nasıl çalışır banka altyapısı üzerinde nasıl entegre edilecek bizim uyuglamalar perspektifinden anlatım yapıldı Github Copilot 47 complete Copilot kullanımları Nilüfer ile incelenerek mayıs ayı bitmeden kullanmayan kişilerin temizleme operasyonu yapıldı. Microsoft ile billing cycle'ın netleşmesi için yazışıyoruz. Şu an kişiyi çıkarsak dahi ay sonuna kadar tam para ödüyoruz. İşbank NAYS 49 complete Keycloak crossdc aktif/aktif çalışma kapsamında Redhat ile birlikte bir planlama toplantısı yapılıp destek verildi İşbank IAM Uygulamaları 53 complete Openshift deployment sonrası yaşanan paket kaybı sorunu için baka ekipleri ile birlikte inceleme yapıldı. Epaykolay IAM 55 complete Banka UAT ortamlarında yaşanan erişim problemlerinde IAM ve banka openshift ekiplerine destek verildi. Bir haftadır ilgileniyorlar, test yöntemleri ve sorunu analiz etmeleri konusunda destek verdik. Henüz çözmediler ama problemin kod tarafında olduğu nu saptayacak noktaya getirdik.","{'title': 'Platform Engineering & Support Epic', 'id': '84836938', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=84836938'}"
"Quarter Başlık Tasks Detaylar Kazanımlar FATURA 42 complete AHE, Smart Farm, Emtia, QPay, Pob Nisan sonuna kadar olan proformalar oluşturuldu ve firma yetkililieri ile paylaşıldı. TEFTİŞ 40 complete GCP yetkilendirme sorgu sonucunu teftiş ekibiyle paylaşılması GCP’deki Rally, CaperaDemo ve Platform projelerine ilişkin 01.2021-10.03.2023 tarihleri arasını kapsayacak şekilde her türlü yetkilendirmelerin, ilgili yetkilendirmelerin detaylarının paylaşılmasını rica ederiz. İşnet üzerinde de yetkilendirmeleriniz mevcut ise, onların da paylaşılmasını rica ederiz. (Taha ve Baransel'den bilgi al. )","{'title': 'Invoice, Agreement & Inspections Epic', 'id': '84836940', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=84836940'}"
"Quarter Başlık Tasks Detaylar Kazanımlar EĞİTİM 100 complete Openshift'in workshop eğitimi yapıldı. EĞİTİM 98 complete OpenShift Platform Workshops eğitimini yapıldı. EĞİTİM 96 complete Redhat ile Openshift üzerine workshop eğitimi yapıldı. EĞİTİM 94 complete RedHat tarafından sağlanan OpenShift Workshop LAB eğitimleri tamamlandı. EĞİTİM 92 complete Akademi tarafından atanan GCP ve Azure Cost Optimization eğitimleri tamamlandı. EĞİTİM 90 complete GCP ve Azure Cost Optimizasyon eğitimlerine devam edildi. EĞİTİM 88 complete Akademi tarafından hazırlanan Devops Gelişim Programındaki zorunlu eğitimlerden Docker for the Absolute Beginner - Hands On - DevOps eğitimi ve Kubernetes for the Absolute Beginners - Hands-on eğitimleri tamamlanarak Devops Gelişim Programı tamamlanmış oldu. EĞİTİM 86 complete Akademi tarafından hazırlanan Devops Gelişim Programındaki zorunlu eğitimlerden Git Complete: The definitive, step-by-step guide to Git tamamlandı. Docker for the Absolute Beginner - Hands On - DevOps eğitimine başlandı EĞİTİM 84 complete Akademi üzerinde atanan GCP ve Azure Cost Optimization eğitimine başlandı . EĞİTİM 82 complete Akademi tarafından hazırlanan Devops Gelişim Programındaki zorunlu eğitimlerden Learn DevOps: CI/CD with Jenkins using Pipelines and Docker tamamlandı. EĞİTİM 79 complete Github Admin Workshop eğitimine katılım sağlandı. (Tüm ekip) EĞİTİM 80 complete Akademi tarafından hazırlanan Devops Gelişim Programındaki zorunlu eğitimlerden Learn Linux in 5 Days tamamlandı. Learn DevOps: CI/CD with Jenkins using Pipelines and Docker eğitimine başlandı. EĞİTİM 76 complete Akademi üzerinde atanan GCP Cost Optimization eğitimine başlandı . EĞİTİM 74 complete Akademi tarafından hazırlanan Devops Gelişim Programındaki zorunlu eğitimlerden Learn Linux in 5 Days eğitimine devam edildi. EĞİTİM 72 complete Yeni atanan zorunlu eğitim Google Cloud Cost Optimization eğitimine Udemy'de başlandı. EĞİTİM 70 complete Udemy'de zorunlu eğitimlerden Learn Linux in 5 Days eğitimine başlandı. AKADEMİ 68 complete Şiddetsiz iletişime giriş eğitimine katılım sağlandı. EĞİTİM 66 complete Udemy'de zorunlu eğitimlerden Big Data with Apache Spark and AWS eğitimi izlendi. AKTARIM 64 complete Sorumluluğumdaki servislerin ve yapıların 5 farklı oturumda anlatımları yapıldı AKADEMI 62 complete Metaverse 101 Eğitimine katılım sağlandı. AKADEMI 60 complete Zorunlu Yapay Zeka Okuryazarlığı eğitimi (5 saat) AKADEMİ 56 complete Desing Thinking eğitimine katılım sağlandı EĞİTİM 54 complete DB2 Eğitimi alındı. EĞİTİM 51 complete Öğrenme İksiri Stratejik Bakış Açısı eğitimine katılım sağlandı. EĞİTİM 52 complete Sosyal Yetkinlik  Eğitimi kapsamında, `Belirsizlikle Baş Edebilme` eğitimine katılım sağlandı. EĞİTİM 48 complete Softtech & Microsoft Azure Admin Workshop eğitimine ekip olarak iki gün katıldık. @Ekip EĞİTİM 46 complete Microsoft AZ-104 eğitimine katılım sağlandı. EĞİTİM 44 complete Openshift platform eğitimine tüm takım olarak katılım sağlandı. Q1 EĞİTİM 42 complete Şiddetsiz İletişime Giriş Eğitimine katılım. Eğitim süresi 2 gün Q1 Microsoft 40 complete Microsoft AZ400 DevOps eğitimine katılım. Eğitim süresi 4 gün. Tüm ekip katıldı. Q1 AKEDEMi 38 complete Akademinin isteği devops eğitim path oluşturulması için eğitimlerin belirlenmesi ve akademi ile paylaşışması Q1 AKADEMİ 36 complete Devops eğitimi için toplantı organize edilecek. Q1 EĞİTİM 9 complete GCP lab kullanımları nasıl olacak. Workshop düzenle Q1 EĞİTİM 10 complete GCP & Azure tarafında yapılacak Eğitim çalışmaları için hedef belirlenmesi gerekir mi? Değerlendirilecek. EĞİTİM 58 complete Çatışma Yönetimi eğitimi.","{'title': 'Eğitimler Epic', 'id': '84836946', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=84836946'}"
,"{'title': 'Kontrol Listeleri', 'id': '84836991', 'source': 'https://wiki.softtech.com.tr/display/SDO/Kontrol+Listeleri'}"
small Github VM backup alındığını kontrol et. Github OS backup alınmasını sağla Mesai saatleri içinde olmadığını teyit et.,"{'title': 'Github Upgrade Checklist', 'id': '84836993', 'source': 'https://wiki.softtech.com.tr/display/SDO/Github+Upgrade+Checklist'}"
,"{'title': 'Github Maintenance Guideline', 'id': '84836997', 'source': 'https://wiki.softtech.com.tr/display/SDO/Github+Maintenance+Guideline'}"
Giriş Kullanıcı - Paket Görüntüleme Yetkileri Softtech Kullanıcısı Success Factory Bizdeki tablo Genom İşbank Kullanıcısı,"{'title': 'Devops Softtech - Checkmarx Development Guideline', 'id': '84837230', 'source': 'https://wiki.softtech.com.tr/display/SDO/Devops+Softtech+-+Checkmarx+Development+Guideline'}"
,"{'title': 'Service Desk Guide', 'id': '86147754', 'source': 'https://wiki.softtech.com.tr/display/SDO/Service+Desk+Guide'}"
Sürüm ekibi örnek pipeline templateleri Java temel template’i: https://scoretfs.isbank/ISBANK/BUILD_TEMPLATES/_git/Yaml_Templates?path=%2Fjava.yml Dotnet build temel template’i: https://scoretfs.isbank/ISBANK/BUILD_TEMPLATES/_git/Yaml_Templates?path=%2Fdotnet-build.yml Nuget-restore temel template’i: https://scoretfs.isbank/ISBANK/BUILD_TEMPLATES/_git/Yaml_Templates?path=%2Fnuget-restore.yml Docker Build Pipeline’ı (Docker Build + push Adımına bakılmalı): https://scoretfs.isbank/ISBANK/BUILD_TEMPLATES/_apps/hub/ms.vss-ciworkflow.build-ci-hub?_a=edit-build-definition&id=3313,"{'title': 'Banka NexusIQ paketlerinin taranması', 'id': '86149835', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=86149835'}"
"Site-2-Site Tanımları Kaynak Netwok/IP Kaynak Port Hedef Network/IP Hedef Port Tedarikçi Finansmanı Sunucu Listesi FTP Sunucuları Int Ortamı FTP Sunucusu GCP üzerinde kurulu sunucu. FTP kullanılıyor, pasif modda","{'title': 'İş Faktoring Tedarikçi Finansmanı', 'id': '89754352', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=89754352'}"
"FTP Yapılandırması Sunucu üzerinde FTP paket kurulumuna başlanmadan önce gerekli yardımcı paket kurulumları tamamlanır. # sudo apt update # sudo apt install acl # sudo apt install net-tools FTP paket kurulumu için aşağıdaki paket yüklenir. # sudo apt install vsftpd FTP servisinin konfigürasyon dosyası /etc/vsftpd.conf aşağıdaki gibi düzenlenir ve eksik olan parametreler eklenir. # cat /etc/vsftpd.conf listen=NO listen_ipv6=YES anonymous_enable=NO local_enable=YES dirmessage_enable=YES use_localtime=YES xferlog_enable=YES connect_from_port_20=YES secure_chroot_dir=/var/run/vsftpd/empty pam_service_name=vsftpd rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem rsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.key ssl_enable=NO write_enable=YES allow_writeable_chroot=YES chroot_local_user=YES userlist_enable=YES userlist_file=/etc/vsftpd.userlist userlist_deny=NO pasv_enable=YES pasv_min_port=50000 pasv_max_port=50999 /etc/vsftpd.userlist dosyası içerisine FTP ile erişmesini istediğimiz user'lar eklenir. Mevcutta bu dosya gelmediğinden oluşturulması gerekir. # cat /etc/vsftpd.userlist tekcep-int musteri-test-1 /etc/pam.d/vsftpd dosyası içerisinde pam_shells.so modülü disable edilir. Pam_shells.so , kullanıcının kabuğu /etc/shells içinde listeleniyorsa sisteme erişime izin veren bir PAM modülüdür, o nedenle ekleyeceğimiz kullanıcının kabuğu ekli olmadığından disable etmemiz gerekir. # cat /etc/pam.d/vsftpd #auth   required pam_shells.so Konfigürasyonların sisteme yansıtılması için servis restart edilip , başarılı açılıp açılmadığı kontrol edilir. # systemctl restart vsftpd.service # systemctl status vsftpd.service # journalctl -x -eu vsftpd.service SFTP Yapılandırması 22.porttan file transfer erişim için aşağıdaki paket kurulur. # sudo apt install ssh Kurulumdan sonra /etc/ssh/sshd_config dosyası içerisinde farklı dizinlere erişmesi gereken userlar ya da userlara dahil gruplar için aşağıdaki parametrelerin eklenmesi gerekir ve ana dizinlerinde farklılık var ise ilgili parametlerin her bir user veya grup için eklenmesi gerekir . # cat /etc/ssh/sshd_config Subsystem sftp internal-sftp Match user tekcep-int ChrootDirectory /data X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp -d /%u PasswordAuthentication yes Match group ftpusers ChrootDirectory /data X11Forwarding no AllowTcpForwarding no ForceCommand internal-sftp -d /tekcep-int/%u PasswordAuthentication yes İlgili değişikliklerin yansıtılması ve kontrol için aşağıdaki komutlar çalıştırılır. # systemctl restart sshd.service # systemctl status sshd.service # netstat -an |grep LISTEN Kullanıcıların erişeceği dizin için ana folder oluşturulur. # mkdir /data # export PWD=/data FTP ve SFTP ile erişim sağlayacak userlar ve grup oluşturulur. # groupadd ftpusers # useradd -s /sbin/nologin -m tekcep-int -d $PWD/tekcep-int # useradd -s /sbin/nologin -m musteri-test-1 -d $PWD/tekcep-int/musteri-test-1 # rm -f ~tekcep-int/.??* ~musteri-test-1/.??* # usermod -a -G ftpusers musteri-test-1 # passwd musteri-test-1 # passwd tekcep-int tekcep-int user'ın $PWD/tekcep-int/ altındaki tüm dosyaları/klasörleri görebilmesi ve musteri-test-1 user'ın sadece $PWD/tekcep-int/musteri-test-1/ altındaki dosyaları/klasörleri görebileceği senaryo için aşağıdaki izinler ve yetkilendirmeler yapılır. # chown tekcep-int:tekcep-int ~tekcep-int; # chmod 701 ~tekcep-int; # chown musteri-test-1:musteri-test-1 ~musteri-test-1; # chmod 700 ~musteri-test-1; Setfacl ile dosya üzerinde tanımlı ACL(Access Control List) girişleri tanımlanır, değiştirilir, silinir. # setfacl -Rb ~tekcep-int; # setfacl -Rdm u:tekcep-int:rwx ~tekcep-int; # setfacl -Rm  u:tekcep-int:rwx ~tekcep-int; Getfacl ile dosya üzerinde tanımlı ACL girişleri görüntülenir. “ls -lrt” komut çıktısına göre imtiyaz alanının sonunda “+” işareti olduğu için dosya ACL girişine sahiptir. # getfacl /$PWD/tekcep-int/ # getfacl /$PWD/tekcep-int/musteri-test-1/ Yeni bir müşteri eklendiğinde aşağıdaki adımların yapılması gerekir. # useradd -s /sbin/nologin -m musteri-test-2 -d $PWD/tekcep-int/musteri-test-2 # usermod -a -G ftpusers musteri-test-2 # chown musteri-test-2:musteri-test-2 ~musteri-test-2; # chmod 700 ~musteri-test-2; # setfacl -Rm  u:tekcep-int:rwx ~tekcep-int; # getfacl /$PWD/tekcep-int/musteri-test-2 # Yeni user'ın FTP ile de erişimi istenirse /etc/vsftpd.userlist dosyasına ilgili user isminin eklenmesi gerekir.image-2023-5-26_23-57-6.pngimage-2023-5-26_23-56-51.png","{'title': 'FTP ve SFTP Kurulumu', 'id': '89754748', 'source': 'https://wiki.softtech.com.tr/display/SDO/FTP+ve+SFTP+Kurulumu'}"
"Bu dökümanda, PgBouncer için majör / minör sürüm güncellemesi ve PostgreSQL için minör sürüm güncellemelerinin nasıl yapıldığı gösterilmektedir. Minör sürüm güncellemeleri; mevcut performans sorunlarına yönelik iyileştirmeleri, bulunan güvenlik açıklarına ve buglara yönelik düzeltmeleri içerdiği için uygulama seviyesinde herhangi bir değişiklik / düzenleme gerektirmemektedir. Dolayısıyla, minör sürüm güncellemeleri aslında en güncel binary dosyaların çekilmesinden ibarettir. PgBouncer İçin Güncelleme Eğer versiyon güncelleme işlemi bir cluster üzerinde yapılacaksa, bu işleme ilk olarak standby nodelardan başlamak daha doğru bir yaklaşım olacaktır. Sürüm güncellemesi en güncel binary dosyalarının kurulmasın ibarettir. Bunun için ilk olarak aşağıdaki komut ile güncel paketler çekilir : sudo apt  update Paket güncellemeleri tamamlandıktan sonra, mevcuttaki sürüm ile kurulacak sürümün ne olduğu kurulum öncesinde aşağıdaki komut ile kontrol edilir : sudo apt policy pgbouncer Bu komutun çıktısı aşağıdaki görsel gibi olacaktır : Bu görsele göre, mevcutta kurulu PgBouncer sürümü 1.18.0 ( 1. adımdaki Installed kısımında yer alır ) , kurulacak yeni sürümün ise 1.19.0 ( 2. adımdaki Candidate kısımında yer alır ) olduğu görülmektedir. Kurulu olan mevcut sürüm ile yükseltilecek yeni sürümden emin olduktan sonra kurulum öncesinde pgbouncer servisi durdurulur : sudo systemctl stop pgbouncer.service Kurulum yapılır : sudo apt install pgbouncer Kurulum yapılırken bu aşamada aşağıdaki görseldeki gibi, mevcuttaki konfigürasyon dosyasını değiştirmek isteyip istemediğimizi soruyor. Bu soruya N ile hayır cevabını vererek, mevcut konfigürasyon dosyasını koruyarak kurulum yapmasını söylüyoruz : Bu aşamada kurulum tamamlandıktan sonra, aşağıdaki komut ile kurulmuş olan yeni sürüm kontrol edilir : pgbouncer -V Pgbouncer'ın son sürüme yükseltildiğini gördükten sonra, servisi start edip durumunu kontrol edebiliriz : sudo systemctl start pgbouncer.service sudo systemctl status pgbouncer.service Servisin Up ve Running durumda olduğunu da gördükten sonra güncelleme süreci son erer. PostgreSQL İçin Güncelleme Eğer versiyon güncelleme işlemi bir cluster üzerinde yapılacaksa, bu işleme ilk olarak standby nodelardan başlamak daha doğru bir yaklaşım olacaktır. PostgreSQL için de sürüm güncelleme işlemi PgBouncer ile aynı bir sürece sahiptir. Sürüm güncellemesi en güncel binary dosyalarının kurulmasın ibarettir. Bunun için ilk olarak aşağıdaki komut ile güncel paketler çekilir ( Daha önceden yapıldıysa bu adım atlanabilir ) : sudo apt  update Paket güncellemeleri tamamlandıktan sonra, mevcuttaki sürüm ile kurulacak sürümün ne olduğu kurulum öncesinde aşağıdaki komut ile kontrol edilir : sudo apt policy postgresql-{majör_versiyon} Bu görsele göre, mevcutta kurulu PostgreSQL sürümü 15.2 ( 1. adımdaki Installed kısımında yer alır ) , kurulacak yeni sürümün ise 15.3 ( 2. adımdaki Candidate kısımında yer alır ) olduğu görülmektedir. Kurulu olan mevcut sürüm ile yükseltilecek yeni sürümden emin olduktan sonra kurulum öncesinde, postgresql servisi durdurulur. PostgreSQL yönetimini eğer Patroni yapıyorsa, Patroni servisi durdurulur : sudo systemctl stop patroni.service Patroni servisi durdurulduktan sonra, kurulum yapılır : sudo apt install postgresql-{majör_sürüm} Kurulum başarılı bir şekilde tamamlandıkta sonra PostgreSQL / Patroni servisi start edilir ve durumu kontrol edilir : sudo systemctl start patroni.service sudo systemctl status patroni.service Patroni servisinin Up ve Running durumda olduğu görüldükten sonra, aşağıdaki komut ile versiony kontrolü yapılır : sudo psql -U postgres -c ""select version();"" Versiyonun istediğimiz sürümde olduğunu gördükten sonra güncelleme süreci son erer.image-2023-6-19_18-55-1.pngimage-2023-6-19_18-53-58.pngimage-2023-6-19_18-52-0.pngimage-2023-6-19_18-45-21.pngimage-2023-6-19_18-44-40.pngimage-2023-6-19_18-35-51.pngimage-2023-6-19_18-18-7.pngimage-2023-6-19_18-16-42.pngimage-2023-6-19_18-10-45.pngimage-2023-6-19_18-5-44.pngimage-2023-6-19_17-45-6.png","{'title': 'PostgreSQL ve PgBouncer Versiyon Güncellemesi', 'id': '89756881', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=89756881'}"
"VM Yaratılması Öncelikle yeni bir VM oluşturmamız gerekiyor. VM oluşturma sayfasında yaptığımız düzenlemeler sırasıyla aşağıdaki gibidir; VM ismi sonarqube-rally-softtech olarak belirlendi ve resource-cost-center: infra label'ı eklendi N2D-standard-8 makinesi seçildi. Boot disk kısmında düzenlemeler yapıldı; Ubuntu 22.04 seçildi. Standard persistent disk seçildi. Size'ı 20 GB belirlendi ve Snapshot schedule 'daily-15' seçildi. Service Account kısmı için 'No Service Account' seçildi. Network interface kısmında default yerine vpc-network seçildi. Network tags olarak sonarqube-rally-softtech verildi. Primary internal IP adres kısmından 10.223.2.105 ip'si reserve edildi ve seçildi. Disks kısmından biri postgresql için diğeri de app için kullanılmak üzere 2 tane disk eklemesi yapıldı. standard persistent disk ve snapshot schedule 'daily-15' seçildi. Ayrıca resource-cost-center: infra label'ı eklendi Management kısmından 'Enable deletion protection' seçildi. Gcloud komutu da aşağıdaki gibidir. Confluence Gcloud Komutu true true Sunucu Konfigürasyonları PostgreSQL Kurulumu Kaynak Link Öncelikle disk kontrollerini yapıp biçimlendiriyoruz. bash Emacs Disk Biçimlendirme true true Daha sonra postgresql kurulumuna geçiyoruz. bash Emacs PostgreSQL Yüklenmesi true true /etc/apt/sources.list.d/pgdg.list  
#postgresql'in private key'ini trusted olarak eklemiş oluyoruz
wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo tee /etc/apt/trusted.gpg.d/pgdg.asc &>/dev/null
apt update
apt install postgresql postgresql-client -y
systemctl stop postgresql]]> Postgresql servisini stop ettik çünkü şimdi disk'e mount edilmesini gerçekleştireceğiz. bash Emacs Mount İşlemi true true SonarQube Kurulumu Kaynak Link Sonarqube kurulumunu yapacağımız dizini yaratıyoruz ve mount işlemini de yine gerçekleştiriyoruz. Daha sonra openjdk kurulumunu da gerçekleştiriyoruz. bash Emacs Sonarqube Mount true true GCP platformu için banka sertifikasını da ekliyoruz. bash Emacs Sertifika Eklenmesi true true /usr/local/share/ca-certificates/ca-platform-softtech-root.crt 
cat <<'EOF' > /usr/local/share/ca-certificates/isbank-root.crt
-----BEGIN CERTIFICATE-----
MIIFvDCCA6SgAwIBAgIQPFvRAaDS6ZFNmIXpSuMsXDANBgkqhkiG9w0BAQsFADBe
MSAwHgYDVQQKExdUVVJLSVlFIElTIEJBTktBU0kgQS5TLjE6MDgGA1UEAxMxVFVS
S0lZRSBJUyBCQU5LQVNJIEEuUy4gS29rIFNlcnRpZmlrYSBTYWdsYXlpY2lzaTAe
Fw0xMDAzMTAwODQ3MzRaFw0zOTAzMTAxNjE3NDVaMF4xIDAeBgNVBAoTF1RVUktJ
WUUgSVMgQkFOS0FTSSBBLlMuMTowOAYDVQQDEzFUVVJLSVlFIElTIEJBTktBU0kg
QS5TLiBLb2sgU2VydGlmaWthIFNhZ2xheWljaXNpMIICIjANBgkqhkiG9w0BAQEF
AAOCAg8AMIICCgKCAgEA1EkZ6rdexmgRBMAbbqi9tPpwdvmyf1ZtSo5FrFZCWUX4
V2I0NwxnyHsRkYYWfApTUD1/Rg8WwToJfmuRhsA0+3G3brm4xSoJPFS6bAzo0IWa
9iyCxD4GvhDgraWUhtD+oQ+2A4BJYCCHfrXOWEjiPCPRITIH/ft9w1UIHNMz4I8G
3n48/aK9X5SsP318jX0CvpI19VCorV+MnRldZ+qH9Q+InFM/dUMquZXbvfgU0105
GdvGRHb5HEnpDg0h+qQAAWxG1ua2u49qDPtxW3JC++GHnF8fA4IcLZfMgj6cRbty
VrAGQWjgjiGw1c99d/J/fneOo23RItw71wRBdz3DUVu6oDg+VlhnJTqkvnk4elQY
seT5KvR8jjnT/lzz2KutthoF9yB6RUv1B9XyTnuIaVqaq2smrJesxJNGiZVQiDAx
t3ZV76Ks5397LG6SGuYLptOiGKx1mdsfXxyZ9a3XB6mIk++DgNYJDJyNys412qc+
2BUHmDMmcoplaR1py4XeHaeXcD7+auHziT8oTBeHTQLm2ZDa7D6mHnAKXu/YkqRc
KwMAMK4qIIKHP5/YdiH724N3jXlnGazZ40VhBsR8MlvroOskiZIpf3NbKpQLkk0t
IUqhlvKQAhhgYIGx7oflEsJZm+g+2jD/xqKvIQ9YRqr0MwbbXSclGVpprshnIy8C
AwEAAaN2MHQwCwYDVR0PBAQDAgGGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYE
FIwiYdZmSVTyqnypZ4HQHriWVHNgMBAGCSsGAQQBgjcVAQQDAgEBMCMGCSsGAQQB
gjcVAgQWBBROFWIdrZk5wQBSTt+ZagGcqgpE2DANBgkqhkiG9w0BAQsFAAOCAgEA
PzsNtYfSb1AhRMsWr+/zgM8KxQUTPsoGNUVFsdT/m9kGF97vdfqPACloyeRnJJJz
oCBn9nbQS0SHNlrHjlE49u/ytjl7/hFL18vZUSyTediinWwNgRkhhS1CxZoGJ3xZ
OQN2xdYyqnUXyj261REQkHc4bcEwmTAB+X9cp8x5It6WAO0sMPV861OQnC3YcLlL
QODAuDnjs7fqL8aDR5NwvS+mrC5sozbY1lyUKrvjqVpQZQA1b+un9AciXyrpmHfj
Z0wZ9xSF19aDKKHqXo5NoYNitTq5XB1zGOHxgRxl80HgWNEXH4+cpX1k0wQXEGnm
YAt0BnYlu/nalEiX1VyDqVulye0dc6juwUWUP+bHktyZPto+Vv6AuWQcU1nCqdKT
IrDK6lgQzYr1UR05oTJzsFVXg/byD4ltsU2IGHMRARCt/Bze9LMr62KfkurWsNWw
E5uvQAAwTk96GGimxUOGxqM20AXd6BshCAj6FEmEyRj0/cXQsOQDbseTeZSty18V
ZNMaa1Ll9e9KPzKbepY39RTM0Ym8RW6/kNJV3B1jiGcyBdA5kunbK/KDSD2N8uYP
I2nd2Cc14K+TG0hHGRu8+PRodK5HuvI+R8Xw1hMN3zuOuOvXIQe/eGvb5cJyk9SW
tpDzhB8RUp1cAINIgAGkDrXkWB2o5LUv204YsXH9kvY=
-----END CERTIFICATE-----
EOF

update-ca-certificates]]> Ardından SonarQube kurulumuna devam ediyoruz. bash Emacs SonarQube Kurulumu true true Postgresql'de SonarQube için DB oluşturuyoruz ve SonarQube'ün erişebildiğini kontrol ediyoruz. bash Emacs SonarQube DB Eklenmesi true true SonarQube'ün konfigürasyon dosyasında (sonar.properties) aşağıdaki düzenlemeleri gerçekleştiriyoruz bash Emacs Konfigürasyon Ayarları true true Sonarqube için systemd dosyasını aşağıdaki gibi hazırlıyoruz. bash Emacs systemd dosyası true true Daha sonra sonarqube'ü çalıştırmayı denediğimizde file-max ve max_map_count değerleriyle ilgili bir hata alınıyordu. Bu hatayı düzeltmek için aşağıdaki düzenlemeyi yapıyoruz. bash Emacs Conf Dosyası true true Nginx Proxy ve Sertifika Ayarları Nginx kurulumunu gerçekleştiriyoruz. bash Emacs Nginx Kurulumu true true /etc/nginx/conf.d/softtech.conf
server_tokens off;
EOF

cat <<'EOF' > /etc/nginx/sites-available/sonarqube.rally.softtech
upstream sonarqube {
  server 127.0.0.1:9000;  #http backend
}

server {
  listen 80;
  server_name sonarqube.rally.softtech;
  location / {
    proxy_set_header Host $host;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header X-Real-IP $remote_addr;
    ##x-forwarded-for ayarı
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_pass http://sonarqube;
  }
}
EOF

ln -s /etc/nginx/sites-available/sonarqube.rally.softtech /etc/nginx/sites-enabled/

systemctl enable --now nginx.service

]]> DNS Kayıtlarının Tanımlanması Softtech-rally projesindeyken; Google Cloud Console'da Network Services > Cloud DNS kısmına giriyoruz. Add standard diyoruz. Domain name sonarqube.rally.softtech yazıyoruz ve IP adresi 10.223.2.105 olarak girerek kaydediyoruz. bash Gcloud Komutu true Openvpn'den gelen kullanılar için dns tanımlamasını yapmak için mgmt-platform projesine geçiyoruz; bash Gcloud Komutu true SSL Tanımı bash SSL Tanımı true SonarQube Login Kurulum tamamlandı. Openvpn'e bağlanarak sonarqube.rally.softtech adresinden erişimi sağlıyoruz. Default username-password bilgisi olarak admin-admin yazarak giriş yapıyoruz ve hemen şifreyi değiştirerek yeni şifreyi belirliyoruz. Ardından bizden lisans key'inin girilmesini istiyor, onu da girerek kaydediyoruz. SAML Entegrasyonu Administration > Authentication > SAML kısmından Saml'ı aktif ediyoruz ve gerekli bilgileri giriyoruz. Application ID: https://sonarqube.rally.softtechimage-2023-7-7_16-45-57.pngimage-2023-7-7_9-48-55.png","{'title': 'SonarQube GCP Kurulumu', 'id': '89757884', 'source': 'https://wiki.softtech.com.tr/display/SDO/SonarQube+GCP+Kurulumu'}"
"Proje Yaratılması infra-platform-softtech projesi BTDESTEK tarafından yaratıldı. Proje yaratılırken ""us-central1"" bölgesi maliyet açısından seçildi. Gerekli Servislerin Aktifleştirilmesi Kullanılacak google servisleri baştan aşağıdaki liste ile aktifleştirilebilir ya da ilerleyen adımlarda hata aldıkça parça parça aktifleştirilebilir. gcloud services enable bigquery.googleapis.com bigquerymigration.googleapis.com bigquerystorage.googleapis.com cloudapis.googleapis.com clouddebugger.googleapis.com cloudtrace.googleapis.com compute.googleapis.com datastore.googleapis.com logging.googleapis.com monitoring.googleapis.com oslogin.googleapis.com servicemanagement.googleapis.com serviceusage.googleapis.com sql-component.googleapis.com storage-api.googleapis.com storage-component.googleapis.com storage.googleapis.com --project=infra-platform-softtech Not: Aktif servislerin listesi şu şekilde çekilebilir: gcloud services list --project=infra-platform-softtech --format=""value(name)"" --filter=""state=( enabled )"" | sed 's#^projects/[0-9]*/services/##' Shared VPC Ayarları mgmt-platform üzerindeki vpc-platform kullanılacak. Varsayılan default network tanımını silelim ve shared vpc'yi buraya attach edelim gcloud compute firewall-rules list --format=""value(NAME)"" --filter=""network=/global/networks/default"" --project=infra-platform-softtech | xargs -i gcloud compute firewall-rules delete {} --project=infra-platform-softtech --quiet

gcloud compute networks delete default --project=infra-platform-softtech --quiet

gcloud compute shared-vpc associated-projects add --host-project=mgmt-platform infra-platform-softtech Servis kurulumlarında ihtiyaç duyulacak subnet tanımları mgmt-platform projesi altında yaratılıp bu projeye paylaşılacak. Snapshot Schedule Yaratılması VM'leri yaratmadan önce günlük, haftalık ve aylık snapshot schedule yaratalım. gcloud compute resource-policies create snapshot-schedule platform-daily-15 --project=infra-platform-softtech --region=us-central1 --max-retention-days=15 --on-source-disk-delete=apply-retention-policy --daily-schedule --start-time=01:00 --storage-location=us-central1 --description=""platform daily snapshot policy, delete after 15 days"" gcloud compute resource-policies create snapshot-schedule platform-weekly-8 --project=infra-platform-softtech --region=us-central1 --max-retention-days=56 --on-source-disk-delete=apply-retention-policy --weekly-schedule=monday --start-time=01:00 --storage-location=us-central1 --description=""platform weekly snapshot policy, delete after 8 weeks"" Proje Ayarları Serial Console Enable Tüm vmler için serial console bağlantısını proje genelinde aktifleştirelim. gcloud compute project-info add-metadata --project=infra-platform-softtech --metadata serial-port-enable=TRUE VM Erişimlerini Hazırlama Öncelikle tüm vm'lerde varsayılan olarak os-login enable edelim. Bu VM'lerin gcloud kullanıcıları kullanılarak login olunmasını sağlıyor. gcloud compute project-info add-metadata --project=infra-platform-softtech --metadata enable-oslogin=TRUE Her kişi kendi ssh key'ini projeye bu şekilde ekleyebilir. Tek tek vm bazlı eklemeye ihtiyaç duymuyoruz. gcloud compute os-login ssh-keys add --project=infra-platform-softtech --ttl 0 --key-file=.ssh/id_rsa.pub","{'title': 'infra-platform-softtech', 'id': '89758377', 'source': 'https://wiki.softtech.com.tr/display/SDO/infra-platform-softtech'}"
"step-ca Private CA Otoritesi Kurulumu Platform Operasyonları Subnet Yaratılması CA sunucusu için kullanılacak subnet: 10.32.1.32/28 : ca-vm-infra-platform : step-ca sunucusunun bulunacağı subnet Öncelikle subnet mgmt-platform projesi altında yaratılır: gcloud compute networks subnets create ca-vm-infra-platform --project=mgmt-platform --range=10.32.1.32/28 --network=vpc-platform --region=us-central1 Bu subnet'in infra-platform-softtech altından kullanılabilmesi için google console üzerinden şu işlem yapılır: shared-vpc sayfasına gidilir: https://console.cloud.google.com/networking/xpn/details?organizationId=413778024800project=mgmt-platform` ATTACHED PROJECTS sekmesi seçilir ve ATTACH PROJECTS seçilir Yukarıdaki listeden infra-platform-softtech projesi seçilir Aradaki ayarlar varsayılanda bırakılır Aşağıdaki listeden ca-vm-infra-platform seçilir ve save edilir VM Yaratılması Internal IP adresini yaratalım: gcloud compute addresses create stepca1-ca-vm-infra-platform --description=""step-ca server private ipv4 address"" --subnet=projects/mgmt-platform/regions/us-central1/subnetworks/ca-vm-infra-platform --project=infra-platform-softtech Internal service dns kaydımızı yaratalım gcloud dns --project=mgmt-platform record-sets create ca.platform.softtech. --rrdatas=10.32.1.34 --type=A --ttl=300 --zone=platform-softtech Softtech-rally projesinden buraya erişilebilmesi için response-policy de girelim gcloud beta dns --project=softtech-rally response-policies rules create ca-platform-softtech --response-policy=""softtech-rally-generic-responce-policy"" --dns-name=""ca.platform.softtech."" --local-data=name=""ca.platform.softtech."",type=""A"",ttl=300,rrdatas=""10.32.1.34"" VM instance için standart bir kural seti belirlendi. Bu standarta göre instance yaratalım: gcloud compute instances create stepca1-ca-vm-infra-platform --project=infra-platform-softtech --zone=us-central1-a --description=""stepca server"" --machine-type=n2d-standard-2 --network-interface=nic-type=GVNIC,private-network-ip=10.32.1.34,subnet=projects/mgmt-platform/regions/us-central1/subnetworks/ca-vm-infra-platform,no-address --metadata=enable-oslogin=true --maintenance-policy=MIGRATE --no-service-account --no-scopes --tags=stepca1-ca-vm-infra-platform,ca-vm-infra-platform --create-disk=auto-delete=yes,boot=yes,device-name=stepca1-ca-vm-infra-platform,disk-resource-policy=projects/infra-platform-softtech/regions/us-central1/resourcePolicies/platform-daily-15,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20220607,mode=rw,size=20,type=projects/infra-platform-softtech/zones/us-central1-a/diskTypes/pd-standard --shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --deletion-protection Firewall İzinleri vpn sunucusundan ingress tcp-22 ssh izni eklenir: gcloud compute --project=mgmt-platform firewall-rules create stepca1-ca-vm-infra-platform-ingress-tcp22 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:22 --source-tags=vpn-vm-mgmt-platform --target-tags=stepca1-ca-vm-infra-platform stepca sunucusu tcp/443 portundan hizmet verecek, yerel networkteki tüm adreslere erişim izni verilir gcloud compute --project=mgmt-platform firewall-rules create stepca1-ca-vm-infra-platform-ingress-tcp443 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:443 --source-ranges=10.0.0.0/8 --target-tags=stepca1-ca-vm-infra-platform stepca http challange ile host doğrulaması yapabilmesi sertifika isteği yapan servisin tcp/80 portuna erişmesi gerekecektir. Bu nedenle yerel networkteki tüm adreslere egress erişim izni verilir gcloud compute --project=mgmt-platform firewall-rules create stepca1-ca-vm-infra-platform-egress-tcp80 --direction=EGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:80 --destination-ranges=10.0.0.0/8 --target-tags=stepca1-ca-vm-infra-platform VM Operasyonları OS Hazırlıkları Host işlemleri yapılır ve güncellenir. Aksi belirtilmedikçe tüm komutlar root yetkisi ile çalıştırılır sudo apt update -y
sudo apt upgrade -y
sudo apt autoremove -y Install step-ca ve step-cli doğrudan deb paketi olarak indirilip kurulacak, bir repo bulunmuyor. Github sayfasından latest sürüm aşağıdaki şekilde indirlebilir. apt update && apt install jq -y
mkdir PAKETLER

latestStepcliDownloadLink=$(curl -s https://api.github.com/repos/smallstep/cli/releases/latest | jq -r '.assets[] | select(.name | endswith(""amd64.deb"")) | .browser_download_url')
wget ""${latestStepcliDownloadLink}"" -O PAKETLER/$(basename ""${latestStepcliDownloadLink}"")

latestStepcaDownloadLink=$(curl -s https://api.github.com/repos/smallstep/certificates/releases/latest | jq -r '.assets[] | select(.name | endswith(""amd64.deb"")) | .browser_download_url')
wget ""${latestStepcaDownloadLink}"" -O PAKETLER/$(basename ""${latestStepcaDownloadLink}"")

dpkg -i PAKETLER/$(basename ""${latestStepcliDownloadLink}"") PAKETLER/$(basename ""${latestStepcaDownloadLink}"") step-ca İlk Yapılandırması step-ca ilk kurulumun başlatalım. Burada parola alanı boş geçilecek ve üretilen parolanın kaydedilmesi gerecektir. export STEPPATH=/etc/step-ca
echo 'export STEPPATH=/etc/step-ca' >> .bashrc
(
  generatedPassword=$(openssl rand -base64 36)
  step ca init --deployment-type=standalone --name=""ca-platform-softtech Root v2022.1"" --dns=ca.platform.softtech --address="":443"" --provisioner=default-provisioner-to-be-deleted --password-file=<(echo ""${generatedPassword}"")
  echo ""${generatedPassword}"" > $(step path)/password.txt
) Intermediate CA yapılandırması Varsayılan intermediate CA yerine domain kısıtı koyarak yeni bir sertifika üreteceğiz. Intermediate CA süresi 5 yıl olacak cat <<'STEOF' > /etc/step-ca/templates/intermediate-ca-v2022.1.1.tpl
{
    ""subject"": {{ toJson .Subject }},
    ""keyUsage"": [""certSign"", ""crlSign""],
    ""basicConstraints"": {
        ""isCA"": true,
        ""maxPathLen"": 0
    },
    ""nameConstraints"": {
        ""critical"": true,
        ""permittedDNSDomains"": [""softtech""]
    }
}
STEOF

step certificate create --template /etc/step-ca/templates/intermediate-ca-v2022.1.1.tpl --password-file=/etc/step-ca/password.txt --ca-password-file=/etc/step-ca/password.txt --ca /etc/step-ca/certs/root_ca.crt --ca-key /etc/step-ca/secrets/root_ca_key ""ca-platform-softtech v2022.1 Intermediate CA v2022.1.1"" /etc/step-ca/certs/intermediate_ca.crt /etc/step-ca/secrets/intermediate_ca_key --not-after $[($(date -d ""now + 5 years"" +%s)-$(date +%s))/(60*60)]h -f Config Değişiklikleri ca.json dosyası üzerinde sunucumuza özel değişiklikleri yazalım jq --tab --arg cn 'ca.platform.softtech' '.  + {commonName: $cn}' /etc/step-ca/config/ca.json > /etc/step-ca/config/ca.json.tmp
mv /etc/step-ca/config/ca.json.tmp /etc/step-ca/config/ca.json ACME yapılandırması init adımında yarattığımız default provisioner silinir ve acme provisioner eklenir. Basılacak sertifikaların maksimum geçerlilik süresi ve varsayılan geçerlilik süresi 30 gün (30*24 saat) olarak belirlenmiştir. step ca provisioner remove default-provisioner-to-be-deleted
step ca provisioner add acme --type ACME --x509-max-dur=$[30*24]h --x509-default-dur=$[30*24]h Log Yapılandırması step-ca servisinin loglarını syslog ile başka bir dosyaya kaydettirelim cat <<'STEOF' > /etc/rsyslog.d/10-step-ca.conf
:programname, isequal, ""step-ca"" /var/log/step-ca.log
& stop
STEOF
systemctl restart rsyslog.service Logrotate yapılandırmasını günlük olacak ve 90 gün saklayacak şekilde tanımlayalım cat <<'STEOF' > /etc/logrotate.d/step-ca
/var/log/step-ca.log
{
  rotate 90
  daily
  create
  missingok
  postrotate
    /usr/lib/rsyslog/rsyslog-rotate
  endscript
}
STEOF Servis systemd Yapılandırması systemd yapılandırmasını yapalım ve servisi başlatalım useradd --system --home /etc/step-ca --shell /bin/false step
setcap CAP_NET_BIND_SERVICE=+eip $(which step-ca)

mkdir -p /etc/step-ca/db
chown -R step:step /etc/step-ca

cat <<'STEOF' > /etc/systemd/system/step-ca.service
[Unit]
Description=step-ca service
Documentation=https://smallstep.com/docs/step-ca
Documentation=https://smallstep.com/docs/step-ca/certificate-authority-server-production
After=network-online.target
Wants=network-online.target
StartLimitIntervalSec=30
StartLimitBurst=3
ConditionFileNotEmpty=/etc/step-ca/config/ca.json
ConditionFileNotEmpty=/etc/step-ca/password.txt

[Service]
Type=simple
User=step
Group=step
Environment=STEPPATH=/etc/step-ca
WorkingDirectory=/etc/step-ca
ExecStart=/usr/bin/step-ca config/ca.json --password-file password.txt
ExecReload=/bin/kill --signal HUP $MAINPID
Restart=on-failure
RestartSec=5
TimeoutStopSec=30
StartLimitInterval=30
StartLimitBurst=3

; Process capabilities & privileges
AmbientCapabilities=CAP_NET_BIND_SERVICE
CapabilityBoundingSet=CAP_NET_BIND_SERVICE
SecureBits=keep-caps
NoNewPrivileges=yes

; Sandboxing
ProtectSystem=full
ProtectHome=true
RestrictNamespaces=true
RestrictAddressFamilies=AF_UNIX AF_INET AF_INET6
PrivateTmp=true
PrivateDevices=true
ProtectClock=true
ProtectControlGroups=true
ProtectKernelTunables=true
ProtectKernelLogs=true
ProtectKernelModules=true
LockPersonality=true
RestrictSUIDSGID=true
RemoveIPC=true
RestrictRealtime=true
SystemCallFilter=@system-service
SystemCallArchitectures=native
MemoryDenyWriteExecute=true
ReadWriteDirectories=/etc/step-ca/db

[Install]
WantedBy=multi-user.target
STEOF

systemctl daemon-reload
systemctl enable --now step-ca Root CA Yüklenmesi Kendi ürettiğimiz Root CA'yı makine üzerinde de trusted olarak ekleyelim curl -sk https://ca.platform.softtech/roots.pem > /usr/local/share/ca-certificates/ca-platform-softtech-Root-v2022.1.crt && update-ca-certificates","{'title': 'stepca1-ca-vm-infra-platform', 'id': '89758381', 'source': 'https://wiki.softtech.com.tr/display/SDO/stepca1-ca-vm-infra-platform'}"
"Forwarding NameServer Kurulumu Platform Operasyonları Subnet Yaratılması DNS sunucusu için kullanılacak subnet: 10.32.1.80/28 : dns-vm-infra-platform : dns sunucularının bulunacağı subnet Öncelikle subnet mgmt-platform projesi altında yaratılır: gcloud compute networks subnets create dns-vm-infra-platform --project=mgmt-platform --range=10.32.1.80/28 --network=vpc-platform --region=us-central1 Bu subnet'in infra-platform-softtech altından kullanılabilmesi için google console üzerinden şu işlem yapılır: shared-vpc sayfasına gidilir: https://console.cloud.google.com/networking/xpn/details?project=mgmt-platform SHARED SUBNETS & PERMISSIONS sekmesi seçilir ve Individual subnet permissions (subnet-level permissions) altındaki Show onlu subnets with defined individual permissions işareti kaldırılır Aşağıdaki listeden dns-vm-infra-platform subnet'i seçilir Sağ taraftaki sekmeden aşağıdaki service account'lar Compute Network User olarak eklenir (bu hesaplar infra-platform-softtech projesindeki default compute engine ve api service account'larıdır) 844597443006-compute@developer.gserviceaccount.com 844597443006@cloudservices.gserviceaccount.com VM Yaratılması Internal IP adreslerini yaratalım: gcloud compute addresses create dns1-dns-vm-infra-platform --description=""dns1 server private ipv4 address"" --subnet=projects/mgmt-platform/regions/us-central1/subnetworks/dns-vm-infra-platform --project=infra-platform-softtech

gcloud compute addresses create dns2-dns-vm-infra-platform --description=""dns2 server private ipv4 address"" --subnet=projects/mgmt-platform/regions/us-central1/subnetworks/dns-vm-infra-platform --project=infra-platform-softtech Internal service dns kaydımızı yaratalım gcloud dns --project=mgmt-platform record-sets create dns1.platform.softtech. --rrdatas=10.32.1.82 --type=A --ttl=300 --zone=platform-softtech

gcloud dns --project=mgmt-platform record-sets create dns2.platform.softtech. --rrdatas=10.32.1.83 --type=A --ttl=300 --zone=platform-softtech Softtech-rally projesinden buraya erişilebilmesi için response-policy de girelim gcloud beta dns --project=softtech-rally response-policies rules create dns1-platform-softtech --response-policy=""softtech-rally-generic-responce-policy"" --dns-name=""dns1.platform.softtech."" --local-data=name=""dns1.platform.softtech."",type=""A"",ttl=300,rrdatas=""10.32.1.82""

gcloud beta dns --project=softtech-rally response-policies rules create dns2-platform-softtech --response-policy=""softtech-rally-generic-responce-policy"" --dns-name=""dns2.platform.softtech."" --local-data=name=""dns2.platform.softtech."",type=""A"",ttl=300,rrdatas=""10.32.1.83"" VM instance için standart bir kural seti belirlendi. Bu standarta göre instance yaratalım: gcloud compute instances create dns1-dns-vm-infra-platform --project=infra-platform-softtech --zone=us-central1-a --description=""dns1 server"" --machine-type=e2-micro --network-interface=nic-type=GVNIC,private-network-ip=10.32.1.82,subnet=projects/mgmt-platform/regions/us-central1/subnetworks/dns-vm-infra-platform,no-address --metadata=enable-oslogin=true --maintenance-policy=MIGRATE --no-service-account --no-scopes --tags=dns1-dns-vm-infra-platform,dns-vm-infra-platform --create-disk=auto-delete=yes,boot=yes,device-name=dns1-dns-vm-infra-platform,disk-resource-policy=projects/infra-platform-softtech/regions/us-central1/resourcePolicies/platform-daily-15,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20220607,mode=rw,size=20,type=projects/infra-platform-softtech/zones/us-central1-a/diskTypes/pd-standard --shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --deletion-protection

gcloud compute instances create dns2-dns-vm-infra-platform --project=infra-platform-softtech --zone=us-central1-a --description=""dns2 server"" --machine-type=e2-micro --network-interface=nic-type=GVNIC,private-network-ip=10.32.1.83,subnet=projects/mgmt-platform/regions/us-central1/subnetworks/dns-vm-infra-platform,no-address --metadata=enable-oslogin=true --maintenance-policy=MIGRATE --no-service-account --no-scopes --tags=dns2-dns-vm-infra-platform,dns-vm-infra-platform --create-disk=auto-delete=yes,boot=yes,device-name=dns2-dns-vm-infra-platform,disk-resource-policy=projects/infra-platform-softtech/regions/us-central1/resourcePolicies/platform-daily-15,image=projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-v20220607,mode=rw,size=20,type=projects/infra-platform-softtech/zones/us-central1-a/diskTypes/pd-standard --shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any --deletion-protection Firewall İzinleri vpn sunucusundan ingress tcp-22 ssh izni eklenir: gcloud compute --project=mgmt-platform firewall-rules create dns-vm-infra-platform-ingress-tcp22 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:22 --source-tags=vpn-vm-mgmt-platform --target-tags=dns-vm-infra-platform dns sunucusu udp/53 tcp/53 portundan hizmet verecek, yerel networkteki tüm adreslere erişim izni verilir gcloud compute --project=mgmt-platform firewall-rules create dns-vm-infra-platform-ingress-udp53 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=udp:53 --source-ranges=10.0.0.0/8 --target-tags=dns-vm-infra-platform
gcloud compute --project=mgmt-platform firewall-rules create dns-vm-infra-platform-ingress-tcp53 --direction=INGRESS --priority=1000 --network=vpc-platform --action=ALLOW --rules=tcp:53 --source-ranges=10.0.0.0/8 --target-tags=dns-vm-infra-platform VM Operasyonları OS Hazırlıkları Host işlemleri yapılır ve güncellenir. Aksi belirtilmedikçe tüm komutlar root yetkisi ile çalıştırılır sudo apt update -y
sudo apt upgrade -y
sudo apt autoremove -y Root CA Yüklenmesi Kendi ürettiğimiz Root CA'yı makine üzerinde de trusted olarak ekleyelim curl -sk https://ca.platform.softtech/roots.pem > /usr/local/share/ca-certificates/ca-platform-softtech-Root-v2022.1.crt
update-ca-certificates systemd-resolved Yapılandırması DNS yönlendirmesi için hali hazırda işletim sisteminde systemd-resolved yapılandırması gelmektedir ve bu google cloud dns'i kullanıyor. Harici bir servis kurmadan aynı servisin diğer interface'ten de hizmet vermesini sağlayacağız. echo ""DNSStubListenerExtra=$(hostname -I)"" >> /etc/systemd/resolved.conf
systemctl restart systemd-resolved.service","{'title': 'dnsX-dns-vm-infra-platform', 'id': '89758384', 'source': 'https://wiki.softtech.com.tr/display/SDO/dnsX-dns-vm-infra-platform'}"
"gcr-archive.rally.softtech GCR imajlarını arşiv amaçlı saklanacağı repository. GCP Operations service account IAM üzerinde gcr-io-archive-harbor service account oluşturuldu. create bucket GCS üzerinde gcr-io-archive bucket oluşturuldu. region: us-central1 Default class: standard Access control: Fine-grained Protection: None Yarattıktan sonra: Bucket içerisinde harborrot dizini oluşturuldu Permissions -> Grant access gcr-io-archive-harbor -> Storage Object Admin Lifecycle Add a rule: Action: Set storage class to Archive Condition: Age : 15 days create vm Name: gcr-io-archive-harbor Type: e2-micro OS disk: 10G standard ubuntu 22.04, snapshot schedule ""daily-15"" Data disk: 20G standard empty, snapshot schedule ""daily-15"" service account: gcr-io-archive-harbor@softtech-rally.iam.gserviceaccount.com Network tag: gcr-archive Network: internal only, reserved an ip, external none os-login enable deletion protection dns records gcr-archive.rally.softtech firewall rules 10.0.0.0/8 -> gcr-archive.rally.softtech:443 ALLOW OS Operations install ca curl -sk https://ca.platform.softtech/roots.pem > /usr/local/share/ca-certificates/ca-platform-softtech-Root-v2022.1.crt
update-ca-certificates data disk mkfs.xfs /dev/sdb
mkdir /data
echo 'UUID=""'""$(blkid -s UUID -o value /dev/sdb)""'"" /data xfs defaults 0 0' >> /etc/fstab
mount -a install docker sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
echo \
  ""deb [arch=""$(dpkg --print-architecture)"" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  ""$(. /etc/os-release && echo ""$VERSION_CODENAME"")"" stable"" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

#configure gcloud helper
gcloud auth configure-docker acme.sh kur apt install socat
wget https://github.com/acmesh-official/acme.sh/archive/refs/tags/3.0.5.tar.gz -O PACKAGES/acme.sh-3.0.5.tar.gz
tar -xf PACKAGES/acme.sh-3.0.5.tar.gz
cd acme.sh-3.0.5/ && bash acme.sh --install --home /etc/acme.sh --no-cron --no-profile && cd .. /etc/acme.sh/acme.sh --home /etc/acme.sh --issue --standalone -d gcr-archive.rally.softtech --server https://ca.platform.softtech/acme/acme/directory cat <<'EOF' > /usr/local/renew-harbor-certificate.sh
#!/bin/bash

/etc/acme.sh/acme.sh --cron --home ""/etc/acme.sh""  #--force  ##uncomment force option to renew before expire
[[ ""${?}"" == 0 ]] || exit 1

#get new certificate modify date in timestamp format
acmeCertModifyTimeStamp=""$(stat -c %Y -L /etc/acme.sh/gcr-archive.rally.softtech/fullchain.cer)""
[[ ""${acmeCertModifyTimeStamp}"" =~ ^[0-9]+$ ]] || exit 1

#get certificate modify date in timestamp format
certModifyTimeStamp=""$(stat -c %Y -L /data/secret/cert/server.crt)""
[[ ""${certModifyTimeStamp}"" =~ ^[0-9]+$ ]] || exit 1

#if certificate changed after service startup
if [[ ${acmeCertModifyTimeStamp} -gt ${certModifyTimeStamp} ]]
then
  #copy certificate and key
  cat /etc/acme.sh/gcr-archive.rally.softtech/fullchain.cer > /data/secret/cert/server.crt || exit 1
  cat /etc/acme.sh/gcr-archive.rally.softtech/gcr-archive.rally.softtech.key > /data/secret/cert/server.key || exit 1

  #restart nginx container to reload cert
  docker container restart nginx
fi
EOF
chmod +x /usr/local/renew-harbor-certificate.sh

echo '30 18 * * * /usr/local/renew-harbor-certificate.sh > /dev/null' | crontab - harbor kur wget https://github.com/goharbor/harbor/releases/download/v2.8.1/harbor-online-installer-v2.8.1.tgz -O PACKAGES/harbor-online-installer-v2.8.1.tgz
tar -xf PACKAGES/harbor-online-installer-v2.8.1.tgz
cp -a harbor/harbor.yml.tmpl harbor/harbor.yml diff -y --suppress-common-lines harbor/harbor.yml.tmpl harbor/harbor.yml
hostname: reg.mydomain.com            | hostname: gcr-archive.rally.softtech
  port: 80                            |   port: 8080
  certificate: /your/certificate/path |   certificate: /etc/acme.sh/gcr-archive.rally.softtech/fullchain.cer
  private_key: /your/private/key/path |   private_key: /etc/acme.sh/gcr-archive.rally.softtech/gcr-archive.rally.softtech.key
harbor_admin_password: Harbor12345    | harbor_admin_password: ***********************
  password: root123                   |   password: *******************
# storage_service:                    | storage_service:
                                      >   gcs:
                                      >     bucket: gcr-io-archive
                                      >     rootdirectory: /harborroot

harbor/install.sh web interface Administration -> Configuration -> System Settings Project Creation : Admin only Administration -> Users -> New user: username: devopshattigelistirme email: devopshattigelistirme@softtech.com.tr name: Devops Hattı Geliştirme Ekibi password: *************** comments: devops readonly hesap Administration -> Robot Accounts -> New Robot Account name: jenkins-rally-softtech expire: Never description: jenkins uploader Projects: softtech-rally-archive Permissions: List repository Pull repository Push repository Read artifact List artifact Create artifact label Create tag List tag Credentials: name: robot$jenkins-rally-softtech secret: **************************** delete project: library create project: softtech-rally-archive Projects -> softtech-rally-archive -> Members -> User+ Name: devopshattigelistirme Role: Limited Guest","{'title': 'gcr-archive.rally.softtech', 'id': '89758386', 'source': 'https://wiki.softtech.com.tr/display/SDO/gcr-archive.rally.softtech'}"
"Gcr.io üzerinde uzun vade saklamak istemediğimiz imajlar var. Bunlar: Snapshot sürüm imajları : 3 aydan eskileri arşive taşıyacağız, ileride arşivden de silinebilir. Kararlı sürüm imajları: 1 yıldan eskileri arşive taşıyacağız, ileride arşivden de silinebilir. LTS sürüm imajları: 5 yıldan eskileri arşive taşıyacağız, ileride arşivden de silinebilir. Tag bulunmayan imajlar: Doğrudan silinebilir. Bunları gerçekleştirebilmek için günlük olarak aşağıdaki iki job'ı sırasıyla çalıştıracağız: Arşive taşıma : Filtrelere göre imajlar arşive kopyalanacak ve kopyalanan imajların tag'leri silinecek. GCR Silme : Tag'i olmayan imajlar silinecek. Arşive Taşıma İlk işlemde toplu olarak yapılacağı için doğrudan gcr-archieve makinesi üzerinde çalışacak şekilde aşağıdaki scriptler çalıştırılabilir. Taşıma işlemi iki aşamadan oluşmaktadır: Imaj listesinin oluşturulması: getGcrImages.sh script'i ile imajlar bir dosyaya yazılabilir. Script içerisindeki filtre kısmı kurallara göre güncellenmelidir. Bu komut yaklaşık 40 dakikada tüm imajları recursive biçimde tarıyor. Listedeki imajların taşınması: Oluşturulmuş olan imaj liste dosyası input verilerek moveGcrImagesToArchiveWithSkopeo.sh script'i çalıştırılabilir. Bu taşınacak imaj sayısı ve boyutuna göre uzun sürebilir. Örnek akış şu şekildedir: bash image-list
bash moveGcrImagesToArchiveWithSkopeo.sh image-list]]> Arşive taşınması yapılan imajlar untag yapılmaktadır ancak silinmemektedir. Diskten silme işlemini tag'i olmayanları silen job yapması gerekmektedir. Çalıştırılan scriptler'in kodları aşağıdaki gibidir: bash getGcrImages.sh true &2
  exit 1
fi

if ! filterDate=$(date -d ""${2}"" '+%Y-%m-%d'); then
  exit 1
fi

# Print header
echo -e ""image${csvSeparator}digest${csvSeparator}creation_timestamp${csvSeparator}tags""

# Stack for repo addresses
repoStack=(""gcr.io/${projectName}"")

# Loop through the repository list
while [[ ${#repoStack[@]} -gt 0 ]]; do
  current=""${repoStack[-1]}""
  repoStack=(""${repoStack[@]:0:${#repoStack[@]}-1}"")

  # Get tag list of the repository
  detailList=$(gcloud container images list-tags ""${current}"" --flatten='tags' --format='csv[separator=""'""${csvSeparator}""'"",no-heading,delimiter="" ""](digest,timestamp,tags)' \
    --filter='
        tags:*
      AND
        timestamp.datetime < '${filterDate}'
      AND
      NOT (
        ( timestamp.datetime > '$(date -d ""${filterDate} 5 years ago"" '+%Y-%m-%d')' AND tags:LTS )
        OR
        ( timestamp.datetime > '$(date -d ""${filterDate} 1 year ago"" '+%Y-%m-%d')' AND -tags:LTS AND -tags:snapshot )
        OR
        ( timestamp.datetime > '$(date -d ""${filterDate} 3 months ago"" '+%Y-%m-%d')' AND tags:snapshot )
      )')

  if [[ ""${detailList}"" != """" ]]; then
    echo ""${detailList}"" | sed 's#^#'""${current}${csvSeparator}""'#'
  else
    # If repository tag list is empty, check if it contains sub repositories
    repoList=$(gcloud container images list --repository ""${current}"" --format='value(name)')

    # Add each sub repository to stack
    for repoItem in ${repoList}; do
      repoStack+=(""${repoItem}"")
    done
  fi
done]]> bash moveGcrImagesToArchiveWithSkopeo.sh true &2
  exit 1
fi

totalCount=$(wc -l < ""${inputFile}"")
currentCount=0
while IFS= read -r line; do
  currentCount=$[${currentCount}+1]

  IFS=""${csvSeparator}"" read -r -a fields <<< ""${line}""

  gcrPath=""${fields[0]}""
  gcrTags=""${fields[3]}""

  #change path from gcr to custom domain and path
  archivePath=$(echo ""${gcrPath}"" | sed 's#^[a-zA-Z0-9\._-]*/[a-zA-Z0-9_-]*/#'""${archiveRepository}""'/#')

  for tag in ${gcrTags}; do

    if [[ $(jobs -p | wc -w) -ge ${batchCount} ]]; then
      wait -n
    fi

    {
      gcrManifest=$(skopeo inspect --raw ""docker://${gcrPath}:${tag}"" 2>&1)
      archiveManifest=$(skopeo inspect --raw ""docker://${archivePath}:${tag}"" 2>&1)

      if [[ ""${gcrManifest}"" == ""${archiveManifest}"" ]]; then
        stdbuf -oL echo ""${currentCount}/${totalCount} Skipping ${gcrPath}:${tag} ----> ${archivePath}:${tag}""
      else
        stdbuf -oL echo ""${currentCount}/${totalCount} Copying ${gcrPath}:${tag} ----> ${archivePath}:${tag}""
        skopeo copy --quiet ""docker://${gcrPath}:${tag}"" ""docker://${archivePath}:${tag}""
      fi

      #untag from gcr ""${gcrPath}:${tag}""
      stdbuf -oL echo ""${currentCount}/${totalCount} Untagging ${gcrPath}:${tag}""
      gcloud container images untag --quiet ""${gcrPath}:${tag}""
    } &
  done
done < ""${inputFile}""
wait]]> GCR Silme Imajları GCR'dan silmek için aşağıdaki script kullanılabilir: bash deleteUntaggedImagesFromGcr.sh true &2
  exit 1
fi

# Stack for repo addresses
repoStack=(""gcr.io/${projectName}"")

# Loop through the repository list
while [[ ${#repoStack[@]} -gt 0 ]]; do
  current=""${repoStack[-1]}""
  repoStack=(""${repoStack[@]:0:${#repoStack[@]}-1}"")

  # Get tag list of the repository
  detailList=$(gcloud container images list-tags ""${current}"" --format='csv[separator=""'""${csvSeparator}""'"",no-heading](digest)' --filter='-tags:*')

  if [[ ""${detailList}"" != """" ]]; then
    for digest in ${detailList}; do
      echo ""Deleting ${current}:${digest}""
      gcloud container images delete --quiet ""${current}:${digest}""
    done
  else
    # If repository tag list is empty, check if it contains sub repositories
    repoList=$(gcloud container images list --repository ""${current}"" --format='value(name)')

    # Add each sub repository to stack
    for repoItem in ${repoList}; do
      repoStack+=(""${repoItem}"")
    done
  fi
done]]>","{'title': 'Imaj Taşıma Silme Süreçleri', 'id': '89758429', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=89758429'}"
"Jenkins sunucusu daha önce utils k8s cluster içerisnde iken performans nedeniyle VM'e taşıdık. Kabaca yapılan işlemler aşağıdaki gibidir: Jenkins-vm hazırlanması Ubuntu 22.04 bir makine kuruldu. 20 GB OS diski, 650 GiB da data diski bağlandı. Bu data diskinde jenkins kurulumları olacak. Data diski mkfs.xfs /dev/sdb komutu ile biçimlendirilir ve fstab'a aşağıdaki şekilde kayıt girilerek /var/lib/jenkins dizinine bağlanması sağlanır bash /etc/fstab Makine üzerine nginx kurulur. Nginx server yapılandırmasına şu dosya eklenir: /etc/nginx/sites-available/jenkins true Makine üzerinde öncelikle nginx ve jenkins kuruldu. Kurulum için jenkins dokümantasyonu referans alınabilir, LTS sürümü olmasına dikkat edilir: https://www.jenkins.io/doc/book/installing/linux/#debianubuntu Eski Jenkins’i kapattık (scale replice 0). Bu deployment’I silmiyoruz çünkü service account’ları hala kullanılıyor. Eski Jenkins diskini snapshot alıp yenisine read-only disk olarak geçici süre ile bağladk : /mnt/Jenkins Jenkins vm’de “githookuser” isminde yerel bir hesap yaratıp buna token ürettik. Bu hesap ve token gitlab üzerinden hook olarak eklerken kullanılacak. Jenkins vm’de servisi durdurup jobları kopyaladık: bash true View’ları elle karşılatırıp kopyaladık: vimdiff bash Eski Jenkins config.xml içerisinde gitlab-service olarak belirtilen adresleri gitlab.rally.softtech olarak değiştirdik: bash Bundan sonra servisi başlatabilriiz. GCP üzerinden dns kaydını da yeni vm'in ip'sine olacak şekilde değiştirmek gerekiyor. Gitlab üzerinden yeni hook’ları toplu olarak ekledik (Kod ile API çağrısı yapıldı, ilgili parça Şaban’da) Kurulacak plugin'ler için Hat Ekibi ile birlikte eski jenkins'tekiler gözle incelendi ve minimum kuruldu. Sonrasında hata oluşunca eksiklikler tamamlandı.","{'title': 'Jenkins VM', 'id': '89758433', 'source': 'https://wiki.softtech.com.tr/display/SDO/Jenkins+VM'}"
"İlgili Ekip : Ender Akgün, Buğra Akdeniz, Sinem Karakurt Bozkurt, Caner Arda (Support) Uat Jira ve Confluence Uygulama/DB Sunucu Bilgisi 10.222.8.76 (sbujruat01) Prod Jira ve Confluence Uygulama/DB Sunucu Bilgisi 10.222.8.82  (sbucfpdb01) - wiki prod db 10.222.18.12 (sbpjrdb01)  - wiki prod (10.222.8.80 sunucu üzerinden erişim sağlanır.) 10.222.8.73 (sbujrs01) - jira prod db 10.222.8.80 (sbpjrs01) - jira prod Upgrade Adımları 1. Upgrade çalışmasına başlamadan önce uygulama sunucuları üzerindeki uygulamalar kapatılır. # systemctl stop jira # systemctl stop confluence.service 2. DB sunucuları üzerindeki postgres db'sinin dump'ı alınır. # su - postgres # cd /home/postgres/postgres-backup/ # pg_dump -d jiradb -f jiradb-$(date +%Y-%m-%d-%H-%M-%S).sql # pg_dump -d confluencedb -f confluencedb-$(date +%Y-%m-%d-%H-%M-%S).sql 3. Update olacak uygulama paketlerinin backup'ı alınır. # cd /opt/atlassian # tar -czf jira-backup-2023-07-20.tgz jira/ & # tar -czf confluence-backup-2023-07-20.tgz confluence/ & 4. Uygulama sunucularındaki root user'ın crontab'ındaki scripti manuel çalıştırarak /var/atlassian/application-data altındaki uygulama datalarının snapshotları alınır. Aşağıdaki komut ile diff dosyasının alınıp alınmadığı kontrol edilir. # lvs Not: Uat için bu adım aşağıdaki şekilde yapılır. # cd /var/atlassian/application-data # tar -czf jira-appdata-backup-2023-07-20.tgz jira/ & # tar -czf confluence-appdata-backup-2023-07-20.tgz confluence/ & 5. Mevcuttaki jira ve confluence klasörü bir dizide backupı alınarak upgrade çalışmasından sonra değişen dosyalar vimdiff ile karşılaştırılır. Softtech özelinde oluşturulmuş eski config ayarları upgrade ile değişen dosyalarla değiştirilir. # mkdir /opt/atlassian/sil # cd /opt/atlassian/sil # cp -pr ../jira . (cp -pr ../confluence .) 6. wget komut ile ilgili binary paketleri ilgili sunuculara indirilir. 7. İndirilen bin paketleri çalıştırılarak upgrade başlatılır. Backup'ını aldığımız ""jira/jre/lib/security/cacerts"" ve ""confluence/jre/lib/security/cacerts"" sertifikası upgrade sonrası eskisi ile ezilir. # ./atlassian-jira-software-8.20.24-x64.bin # ./atlassian-confluence-7.19.11-x64.bin # cp -a jira-changed/cacerts  jira/jre/lib/security/cacerts # cp -a conf-changed/cacerts  confluence/jre/lib/security/cacerts 8. Değişen dosyalar eskisi ile kontrol edildikten sonra servisler başlatılır. systemctl start jira systemctl start confluence.service Uat Jira link : https://jirauat.softtech Uat Wiki Link : https://wikiuat.softtech Prod Jira Link : https://jira.softtech.com.tr Prod Wiki Link : https://wiki.softtech.com.tr 9. Jira ekran üzerinden bazı pluginleri upload edip enable etmek gerekebilir. Jira versiyonu ile pluginlerin versiyonlarının uyumlu olması gerekir. Aksi halde pluginler jirada çalışmaz.","{'title': 'Jira ve Confluence Upgrade Adımları', 'id': '100008235', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=100008235'}"
Confluence sunucusunun SSL LDAP bağlantısı için Domain Controller sunucularına 636 portundan erişimleri olması gerekir. # telnet 10.222.8.33 636 # telnet 10.222.8.34 636 DC Sunucuların üzerindeki sertifika bilgisi .cer dosyası olarak kopyalanarak uygulamanın sertifika dosyasına eklenir. # openssl s_client --verify 5 -connect 10.222.8.33:636  --->   stdc1.cer # openssl s_client --verify 5 -connect 10.222.8.34:636  --->   stdc2.cer #./keytool -import -alias stdc01 -keystore /opt/atlassian/confluence/jre/lib/security/cacerts -file /opt/atlassian/stdc1.cer -storepass changeit #./keytool -import -alias stdc02 -keystore /opt/atlassian/confluence/jre/lib/security/cacerts -file /opt/atlassian/stdc2.cer -storepass changeit Domain Controller sunucu bilgileri Confluence sunucusunun hosts dosyasına eklenir. # cat /etc/hosts 10.222.8.33     stdc01.softtech.local stdc01 10.222.8.34     stdc02.softtech.local stdc02 Ek olarak wiki.softtech.com.tr admin hesabı üzerinden User directories üzerinden SSL LDAP bilgileri host ve port bilgileri girilmesi gerekir.,"{'title': 'Confluence Sunucunun SSL LDAP Bağlantısı İçin Sertifika Yüklemesi', 'id': '100008243', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=100008243'}"
,"{'title': 'DevOps Danışanlık Projeleri', 'id': '100009741', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=100009741'}"
"İş Faktoring Sunucu Listesi No Sunuc Adı IP Kullanıcı adı Açıklama ISFAKBSTP1 10.10.31.16 softtechk8s Atlas veri merkezi bastion sunucusu, Kubernetes altyapısına bağlanmak üzere ISFAKBSTD1 10.10.33.16 softtechk8s Atom veri merkezi bastion sunucusu, Kubernetes altyapısına bağlanmak üzere ISFAKDBP1 10.10.32.10 Atlas, Prod ortamı, Veritabanı adı: isfaktoringdb, Veritabanı kullanıcısı: superadmin, Port: 5432 ISFAKDBP2 10.10.32.11 Atlas, Prod ortamı, Veritabanı adı: isfaktoringdb, Veritabanı kullanıcısı: superadmin, Port: 5432 ISFAKDBT1 10.10.32.12 Atlas, UAT ortamı, Veritabanı adı: isfaktoringdb, Veritabanı kullanıcısı: superadmin, Port: 5432 ISFAKDBD1 10.10.34.10 Atom, ODM ortamı, Veritabanı adı: isfaktoringdb, Veritabanı kullanıcısı: superadmin, Port: 5432 isfakmstp1 10.10.31.10 Atlas, Prod-Uat Ortamı Kubernetes Cluster Master Node 1 isfakmstp2 10.10.31.11 Atlas, Prod-Uat Ortamı Kubernetes Cluster Master Node 2 isfakmstp3 10.10.31.12 Atlas, Prod-Uat Ortamı Kubernetes Cluster Master Node 3 isfakwrkp1 10.10.31.13 Atlas, Prod-Uat Ortamı Kubernetes Cluster Worker Node 1 isfakwrkp2 10.10.31.14 Atlas, Prod-Uat Ortamı Kubernetes Cluster Worker Node 2 isfakwrkp3 10.10.31.15 Atlas, Prod-Uat Ortamı Kubernetes Cluster Worker Node 3 ISFAKMSTD1 10.10.33.10 Atom, ODM Ortamı Kubernetes Cluster Master Node 1 ISFAKMSTD2 10.10.33.11 Atom, ODM Ortamı Kubernetes Cluster Master Node 2 ISFAKMSTD3 10.10.33.12 Atom, ODM Ortamı Kubernetes Cluster Master Node 3 ISFAKWRKD1 10.10.33.13 Atom, ODM Ortamı Kubernetes Cluster Worker Node 1 ISFAKWRKD2 10.10.33.14 Atom, ODM Ortamı Kubernetes Cluster Worker Node 2 ISFAKWRKD3 10.10.33.15 Atom, ODM Ortamı Kubernetes Cluster Worker Node 3 DNS Adları DNS IP *.k8s.atlas.isfaktoring.internal 10.10.31.201 db.prod.isfaktoring.internal 10.10.32.10 db.uat.isfaktoring.internal Yapılan Çalışmalar İş Fakotoring Kubernetes Ortamlarında Insecure Registry Tanımı Yapılması İşfaktoring için İşnet sunucularında kurulan Kubernetes sunucuları için yine Kubernetes içerisine kurulan bir container registry’den http erişimi yapılması gerekiyor. Kurulum için kubespray kurulum dosyalarına aşağıdaki ilgili yerleri ekleyip cluster güncellenmiştir. Dosya: inventory/mycluster/group_vars/all/docker.yml Dosyanın sonuna eklenecek ifade: docker_insecure_registries: - registry.k8s.atlas.isfaktoring.internal - 10.10.31.201","{'title': 'İş Faktoring OnPrem DevOps Danışmanlık Hizmetleri', 'id': '100009743', 'source': 'https://wiki.softtech.com.tr/pages/viewpage.action?pageId=100009743'}"
